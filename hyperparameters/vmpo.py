from __future__ import annotations

from typing import Any


PRESETS: dict[tuple[str, str], dict[str, Any]] = {
    ("cheetah", "run"): {
        "total_steps": 10_000_000,
        "eval_interval": 25_000,
        "save_interval": 50_000,
        "hidden_sizes": (256, 256),
        "rollout_steps": 4096,
        "update_epochs": 20,
        "gamma": 0.99,
        "policy_lr": 1e-4,
        "value_lr": 1e-4,
        "topk_fraction": 1.0,
        "eta": 5.0,
        "eta_lr": 1e-3,
        "epsilon_eta": 0.1,
        "epsilon_mu": 0.01,
        "epsilon_sigma": 1e-4,
        "alpha_lr": 1e-3,
        "kl_mean_coef": 1e-3,
        "kl_std_coef": 1e-3,
        "max_grad_norm": 0.5,
    },
    ("humanoid", "run"): {
        "total_steps": 20_000_000,
        "eval_interval": 50_000,
        "save_interval": 100_000,
        "hidden_sizes": (256, 256),
        "rollout_steps": 8192,
        "update_epochs": 10,
        "gamma": 0.99,
        "policy_lr": 1e-4,
        "value_lr": 1e-4,
        "topk_fraction": 1.0,
        "eta": 5.0,
        "eta_lr": 1e-3,
        "epsilon_eta": 0.1,
        "epsilon_mu": 0.01,
        "epsilon_sigma": 1e-4,
        "alpha_lr": 1e-3,
        "kl_mean_coef": 1e-3,
        "kl_std_coef": 1e-3,
        "max_grad_norm": 0.5,
    },
    ("humanoid", "walk"): {
        "total_steps": 10_000_000,
        "eval_interval": 50_000,
        "save_interval": 100_000,
        "hidden_sizes": (256, 256),
        "rollout_steps": 4096,
        "update_epochs": 10,
        "gamma": 0.99,
        "policy_lr": 1e-4,
        "value_lr": 1e-4,
        "topk_fraction": 1.0,
        "eta": 5.0,
        "eta_lr": 1e-3,
        "epsilon_eta": 0.1,
        "epsilon_mu": 0.01,
        "epsilon_sigma": 1e-4,
        "alpha_lr": 1e-3,
        "kl_mean_coef": 1e-3,
        "kl_std_coef": 1e-3,
        "max_grad_norm": 0.5,
    },
    ("hopper", "hop"): {
        "total_steps": 5_000_000,
        "eval_interval": 25_000,
        "save_interval": 50_000,
        "hidden_sizes": (256, 256),
        "rollout_steps": 2048,
        "update_epochs": 10,
        "gamma": 0.99,
        "policy_lr": 1e-4,
        "value_lr": 1e-4,
        "topk_fraction": 1.0,
        "eta": 5.0,
        "eta_lr": 1e-3,
        "epsilon_eta": 0.1,
        "epsilon_mu": 0.01,
        "epsilon_sigma": 1e-4,
        "alpha_lr": 1e-3,
        "kl_mean_coef": 1e-3,
        "kl_std_coef": 1e-3,
        "max_grad_norm": 0.5,
    },
    ("hopper", "home"): {
        "total_steps": 5_000_000,
        "eval_interval": 25_000,
        "save_interval": 50_000,
        "hidden_sizes": (256, 256),
        "rollout_steps": 2048,
        "update_epochs": 10,
        "gamma": 0.99,
        "policy_lr": 1e-4,
        "value_lr": 1e-4,
        "topk_fraction": 1.0,
        "eta": 5.0,
        "eta_lr": 1e-3,
        "epsilon_eta": 0.1,
        "epsilon_mu": 0.01,
        "epsilon_sigma": 1e-4,
        "alpha_lr": 1e-3,
        "kl_mean_coef": 1e-3,
        "kl_std_coef": 1e-3,
        "max_grad_norm": 0.5,
    },
    ("cartpole", "swingup"): {
        "total_steps": 2_000_000,
        "eval_interval": 10_000,
        "save_interval": 25_000,
        "hidden_sizes": (128, 128),
        "rollout_steps": 2048,
        "update_epochs": 10,
        "gamma": 0.99,
        "policy_lr": 1e-4,
        "value_lr": 1e-4,
        "topk_fraction": 1.0,
        "eta": 5.0,
        "eta_lr": 1e-3,
        "epsilon_eta": 0.1,
        "epsilon_mu": 0.01,
        "epsilon_sigma": 1e-4,
        "alpha_lr": 1e-3,
        "kl_mean_coef": 1e-3,
        "kl_std_coef": 1e-3,
        "max_grad_norm": 0.5,
    },
}


def get(domain: str, task: str) -> dict[str, Any]:
    key = (domain, task)
    if key not in PRESETS:
        available = ", ".join([f"{d}/{t}" for (d, t) in sorted(PRESETS.keys())])
        raise KeyError(f"No VMPO preset for {domain}/{task}. Available: {available}")
    return dict(PRESETS[key])
