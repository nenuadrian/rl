from __future__ import annotations

from typing import Any

from hyperparameters._common import get_preset, merge_preset

_VMPO_DEFAULTS: dict[str, Any] = {
    "max_grad_norm": 0.5,
    "popart_beta": 1e-4,
    "popart_eps": 1e-8,
    "popart_min_sigma": 1e-3,
    "normalize_advantages": True,
}


def _vmpo_preset(**overrides: Any) -> dict[str, Any]:
    return merge_preset(_VMPO_DEFAULTS, **overrides)


_WALKER_BASE = _vmpo_preset(
    num_envs=16,
    total_steps=40_000_000,
    eval_interval=25_000,
    save_interval=50_000,
    policy_layer_sizes=(256, 256),
    value_layer_sizes=(512, 256),
    updates_per_step=2,
    gamma=0.99,
    policy_lr=3e-4,
    value_lr=1e-4,
    epsilon_eta=0.05,
    epsilon_mu=0.01,
    epsilon_sigma=1e-4,
    alpha_lr=5e-4,
    temperature_lr=3e-4,
)

_GYM_MUJOCO_BASE = _vmpo_preset(
    num_envs=16,
    rollout_steps=8192,
    updates_per_step=2,
    total_steps=30_000_000,
    eval_interval=20_000,
    save_interval=200_000,
    policy_layer_sizes=(256, 256),
    value_layer_sizes=(512, 256),
    gamma=0.99,
    policy_lr=1e-4,
    value_lr=3e-4,
    topk_fraction=0.2,
    temperature_init=2.0,
    temperature_lr=1e-3,
    alpha_lr=1e-4,
    epsilon_eta=0.7,
    epsilon_mu=0.05,
    epsilon_sigma=1e-3,
)


PRESETS: dict[str, dict[str, Any]] = {
    "dm_control/cheetah/run": _vmpo_preset(
        num_envs=16,
        total_steps=10_000_000,
        eval_interval=25_000,
        save_interval=500_000,
        policy_layer_sizes=(256, 256),
        value_layer_sizes=(512, 256),
        rollout_steps=8192,
        updates_per_step=2,
        gamma=0.99,
        policy_lr=2e-4,
        value_lr=5e-5,
        topk_fraction=0.25,
        temperature_init=2.0,
        temperature_lr=3e-4,
        epsilon_eta=0.1,
        epsilon_mu=0.05,
        epsilon_sigma=6e-4,
        alpha_lr=1e-4,
        max_grad_norm=2.0,
    ),
    "dm_control/humanoid/run": _vmpo_preset(
        num_envs=1,
        total_steps=30_000_000,
        eval_interval=50_000,
        save_interval=100_000,
        policy_layer_sizes=(512, 256, 256),
        value_layer_sizes=(256, 256),
        rollout_steps=16_384 / 2,
        updates_per_step=1,
        gamma=0.995,
        policy_lr=5e-5,
        value_lr=1e-4,
        topk_fraction=0.4,
        temperature_init=4.0,
        temperature_lr=5e-4,
        epsilon_eta=0.3,
        epsilon_mu=0.05,
        epsilon_sigma=3e-4,
        alpha_lr=1e-4,
    ),
    "dm_control/humanoid/walk": _vmpo_preset(
        num_envs=16,
        rollout_steps=8192,
        updates_per_step=6,
        total_steps=30_000_000,
        eval_interval=20_000,
        save_interval=200_000,
        policy_layer_sizes=[512, 512],
        value_layer_sizes=[1024, 512],
        gamma=0.995,
        lambda_gae=0.97,
        policy_lr=1e-4,
        value_lr=3e-4,
        topk_fraction=0.5,
        temperature_init=1.0,
        temperature_lr=1e-4,
        alpha_lr=5e-5,
        epsilon_eta=0.1,
        epsilon_mu=0.01,
        epsilon_sigma=0.02,
    ),
    "dm_control/cartpole/swingup": _vmpo_preset(
        num_envs=1,
        total_steps=2_000_000,
        eval_interval=10_000,
        save_interval=25_000,
        policy_layer_sizes=(256, 256, 256),
        value_layer_sizes=(256, 256),
        rollout_steps=2048,
        updates_per_step=10,
        gamma=0.99,
        policy_lr=1e-4,
        value_lr=1e-4,
        topk_fraction=1.0,
        temperature_init=5.0,
        temperature_lr=1e-3,
        epsilon_eta=0.1,
        epsilon_mu=0.01,
        epsilon_sigma=1e-4,
        alpha_lr=1e-3,
        kl_mean_coef=1e-3,
        kl_std_coef=1e-3,
    ),
    "dm_control/walker/walk": merge_preset(
        _WALKER_BASE,
        rollout_steps=4096,
        topk_fraction=0.25,
        temperature_init=3.0,
    ),
    "dm_control/walker/run": merge_preset(
        _WALKER_BASE,
        rollout_steps=8192,
        topk_fraction=0.2,
        temperature_init=2.0,
    ),
    "Humanoid-v5": _vmpo_preset(
        num_envs=16,
        total_steps=30_000_000,
        eval_interval=10_000,
        save_interval=100_000,
        policy_layer_sizes=(256, 256),
        value_layer_sizes=(512, 256),
        rollout_steps=8192,
        updates_per_step=2,
        gamma=0.995,
        policy_lr=5e-5,
        value_lr=1e-4,
        topk_fraction=0.1,
        temperature_init=2.0,
        temperature_lr=5e-4,
        epsilon_eta=0.25,
        epsilon_mu=0.05,
        epsilon_sigma=3e-4,
        alpha_lr=1e-4,
    ),
    "HalfCheetah-v5": merge_preset(_GYM_MUJOCO_BASE),
    "Ant-v5": merge_preset(_GYM_MUJOCO_BASE),
    "Walker2d-v5": merge_preset(_GYM_MUJOCO_BASE),
}


def get(env_id: str) -> dict[str, Any]:
    return get_preset(
        env_id=env_id,
        presets=PRESETS,
        algorithm_name="VMPO",
        defaults={"optimizer_type": "adam"},
    )
