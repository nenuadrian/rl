from __future__ import annotations

from typing import Any


PRESETS: dict[tuple[str, str], dict[str, Any]] = {
    ("cheetah", "run"): {
        "total_steps": 10_000_000,
        "eval_interval": 25_000,
        "save_interval": 50_000,
        "policy_layer_sizes": (256, 256, 256),
        "rollout_steps": 16384,
        "updates_per_step": 3,
        "gamma": 0.99,
        "policy_lr": 1e-4,
        "value_lr": 2e-4,
        "topk_fraction": 0.5,
        "eta_init": 1.0,
        "eta_lr": 5e-4,
        "epsilon_eta": 0.1,
        "epsilon_mu": 0.05,
        "epsilon_sigma": 5e-4,
        "alpha_lr": 3e-4,
        "max_grad_norm": 5.0,
    },
    ("humanoid", "run"): {
        "total_steps": 30_000_000,
        "eval_interval": 50_000,
        "save_interval": 100_000,
        "policy_layer_sizes": (512, 256),
        "rollout_steps": 16384,
        "updates_per_step": 20,
        "gamma": 0.995,
        "policy_lr": 5e-5,
        "value_lr": 1e-4,
        "topk_fraction": 0.6,
        "eta_init": 10.0,
        "eta_lr": 5e-4,
        "epsilon_eta": 0.3,
        "epsilon_mu": 0.05,
        "epsilon_sigma": 3e-4,
        "alpha_lr": 1e-4,
        "max_grad_norm": 0.5,
    },
    ("humanoid", "walk"): {
        "total_steps": 30_000_000,
        "eval_interval": 50_000,
        "save_interval": 100_000,
        "policy_layer_sizes": (1024, 512),
        "rollout_steps": 16384,
        "updates_per_step": 1,
        "gamma": 0.995,
        "policy_lr": 5e-5,
        "value_lr": 1e-4,
        "topk_fraction": 0.6,
        "eta_init": 10.0,
        "eta_lr": 5e-4,
        "epsilon_eta": 0.3,
        "epsilon_mu": 0.05,
        "epsilon_sigma": 3e-4,
        "alpha_lr": 1e-4,
        "max_grad_norm": 0.5,
    },
    ("hopper", "stand"): {
        "total_steps": 5_000_000,
        "eval_interval": 25_000,
        "save_interval": 50_000,
        "policy_layer_sizes": (256, 256),
        "rollout_steps": 2048,
        "updates_per_step": 10,
        "gamma": 0.99,
        "policy_lr": 1e-4,
        "value_lr": 1e-4,
        "topk_fraction": 1.0,
        "eta_init": 5.0,
        "eta_lr": 1e-3,
        "epsilon_eta": 0.1,
        "epsilon_mu": 0.01,
        "epsilon_sigma": 1e-4,
        "alpha_lr": 1e-3,
        "max_grad_norm": 0.5,
    },
    ("cartpole", "swingup"): {
        "total_steps": 2_000_000,
        "eval_interval": 10_000,
        "save_interval": 25_000,
        "policy_layer_sizes": (256, 256, 256),
        "rollout_steps": 2048,
        "updates_per_step": 10,
        "gamma": 0.99,
        "policy_lr": 1e-4,
        "value_lr": 1e-4,
        "topk_fraction": 1.0,
        "eta_init": 5.0,
        "eta_lr": 1e-3,
        "epsilon_eta": 0.1,
        "epsilon_mu": 0.01,
        "epsilon_sigma": 1e-4,
        "alpha_lr": 1e-3,
        "kl_mean_coef": 1e-3,
        "kl_std_coef": 1e-3,
        "max_grad_norm": 0.5,
    },
    ("walker", "walk"): {
        "num_envs": 1,
        "total_steps": 3_000_000,
        "eval_interval": 25_000,
        "save_interval": 50_000,
        "policy_layer_sizes": (512, 256),
        "rollout_steps": 16_384,
        "updates_per_step": 2,
        "gamma": 0.99,
        "policy_lr": 3e-4,
        "value_lr": 1e-4,
        "topk_fraction": 0.4,
        "eta_init": 5.0,
        "eta_lr": 5e-4,
        "epsilon_eta": 0.1,
        "epsilon_mu": 0.01,
        "epsilon_sigma": 1e-4,
        "alpha_lr": 5e-4,
        "max_grad_norm": 0.5,
    },
    ("walker", "run"): {
        "num_envs": 1,
        "total_steps": 5_000_000,
        "eval_interval": 25_000,
        "save_interval": 50_000,
        "policy_layer_sizes": (256, 256),
        "rollout_steps": 24_576,
        "updates_per_step": 1,
        "gamma": 0.995,
        "policy_lr": 2e-4,
        "value_lr": 1e-4,
        "topk_fraction": 0.5,
        "eta_init": 8.0,
        "eta_lr": 3e-4,
        "epsilon_eta": 0.2,
        "epsilon_mu": 0.02,
        "epsilon_sigma": 2e-4,
        "alpha_lr": 3e-4,
        "max_grad_norm": 0.5,
    },
}


def get(domain: str, task: str) -> dict[str, Any]:
    key = (domain, task)
    if key not in PRESETS:
        available = ", ".join([f"{d}/{t}" for (d, t) in sorted(PRESETS.keys())])
        raise KeyError(f"No VMPO preset for {domain}/{task}. Available: {available}")
    return dict(PRESETS[key])
