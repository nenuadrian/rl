from __future__ import annotations

from typing import Any

from hyperparameters._common import get_preset


PRESETS: dict[str, dict[str, Any]] = {
    "dm_control/cheetah/run": {
        "num_envs": 1,
        "rollout_steps": 2048,
        "updates_per_step": 1,
        "total_steps": 2_000_000,
        "policy_layer_sizes": (256, 256, 256),
        "value_layer_sizes": (512, 512, 256),
        "gamma": 0.99,
        "policy_lr": 1e-4,
        "value_lr": 1e-4,
        "topk_fraction": 0.5,
        "temperature_init": 1.0,
        "temperature_lr": 1e-4,
        "alpha_lr": 1e-4,
        "epsilon_eta": 0.15,
        "epsilon_mu": 0.05,
        "epsilon_sigma": 1e-5,
        "max_grad_norm": 1.0,
        "normalize_advantages": True,
        "optimizer_type": "adam",
    },
    "dm_control/humanoid/run": {
        "num_envs": 1,
        "rollout_steps": 2048,
        "updates_per_step": 1,
        "total_steps": 2_000_000,
        "policy_layer_sizes": (256, 256, 256),
        "value_layer_sizes": (512, 512, 256),
        "gamma": 0.99,
        "policy_lr": 1e-4,
        "value_lr": 1e-4,
        "topk_fraction": 0.5,
        "temperature_init": 1.0,
        "temperature_lr": 1e-4,
        "alpha_lr": 1e-4,
        "epsilon_eta": 0.15,
        "epsilon_mu": 0.05,
        "epsilon_sigma": 1e-5,
        "max_grad_norm": 1.0,
        "normalize_advantages": True,
        "optimizer_type": "adam",
    },
    "dm_control/humanoid/walk": {
        "num_envs": 1,
        "rollout_steps": 2048,
        "updates_per_step": 1,
        "total_steps": 2_000_000,
        "policy_layer_sizes": (256, 256, 256),
        "value_layer_sizes": (512, 512, 256),
        "gamma": 0.99,
        "policy_lr": 1e-4,
        "value_lr": 1e-4,
        "topk_fraction": 0.5,
        "temperature_init": 1.0,
        "temperature_lr": 1e-4,
        "alpha_lr": 1e-4,
        "epsilon_eta": 0.15,
        "epsilon_mu": 0.05,
        "epsilon_sigma": 1e-5,
        "max_grad_norm": 1.0,
        "normalize_advantages": True,
        "optimizer_type": "adam",
    },
    "dm_control/walker/walk": {
        "num_envs": 1,
        "rollout_steps": 2048,
        "updates_per_step": 1,
        "total_steps": 2_000_000,
        "policy_layer_sizes": (256, 256, 256),
        "value_layer_sizes": (512, 512, 256),
        "gamma": 0.99,
        "policy_lr": 1e-4,
        "value_lr": 1e-4,
        "topk_fraction": 0.5,
        "temperature_init": 1.0,
        "temperature_lr": 1e-4,
        "alpha_lr": 1e-4,
        "epsilon_eta": 0.15,
        "epsilon_mu": 0.05,
        "epsilon_sigma": 1e-5,
        "max_grad_norm": 1.0,
        "normalize_advantages": True,
        "optimizer_type": "adam",
    },
    "dm_control/walker/run": {
        "num_envs": 1,
        "rollout_steps": 2048,
        "updates_per_step": 1,
        "total_steps": 2_000_000,
        "policy_layer_sizes": (256, 256, 256),
        "value_layer_sizes": (512, 512, 256),
        "gamma": 0.99,
        "policy_lr": 1e-4,
        "value_lr": 1e-4,
        "topk_fraction": 0.5,
        "temperature_init": 1.0,
        "temperature_lr": 1e-4,
        "alpha_lr": 1e-4,
        "epsilon_eta": 0.15,
        "epsilon_mu": 0.05,
        "epsilon_sigma": 1e-5,
        "max_grad_norm": 1.0,
        "normalize_advantages": True,
        "optimizer_type": "adam",
    },
    "Humanoid-v5": {
        "num_envs": 1,
        "rollout_steps": 2048,
        "updates_per_step": 1,
        "total_steps": 1_500_000,
        "policy_layer_sizes": (256, 256, 256),
        "value_layer_sizes": (512, 512, 256),
        "gamma": 0.99,
        "policy_lr": 1e-4,
        "value_lr": 1e-4,
        "topk_fraction": 0.5,
        "temperature_init": 1.0,
        "temperature_lr": 1e-4,
        "alpha_lr": 1e-4,
        "epsilon_eta": 0.15,
        "epsilon_mu": 0.05,
        "epsilon_sigma": 1e-5,
        "max_grad_norm": 1.0,
        "normalize_advantages": True,
        "optimizer_type": "adam",
    },
    "HalfCheetah-v5": {
        "num_envs": 1,
        "rollout_steps": 2048,
        "updates_per_step": 1,
        "total_steps": 1_000_000,
        "policy_layer_sizes": (256, 256, 256),
        "value_layer_sizes": (512, 512, 256),
        "gamma": 0.99,
        "policy_lr": 1e-4,
        "value_lr": 1e-4,
        "topk_fraction": 0.5,
        "temperature_init": 1.0,
        "temperature_lr": 1e-4,
        "alpha_lr": 1e-4,
        "epsilon_eta": 0.15,
        "epsilon_mu": 0.05,
        "epsilon_sigma": 1e-5,
        "max_grad_norm": 1.0,
        "normalize_advantages": True,
        "optimizer_type": "adam",
    },
    "Ant-v5": {
        "num_envs": 1,
        "rollout_steps": 2048,
        "updates_per_step": 1,
        "total_steps": 1_500_000,
        "policy_layer_sizes": (256, 256, 256),
        "value_layer_sizes": (512, 512, 256),
        "gamma": 0.99,
        "policy_lr": 1e-4,
        "value_lr": 1e-4,
        "topk_fraction": 0.5,
        "temperature_init": 1.0,
        "temperature_lr": 1e-4,
        "alpha_lr": 1e-4,
        "epsilon_eta": 0.15,
        "epsilon_mu": 0.05,
        "epsilon_sigma": 1e-5,
        "max_grad_norm": 1.0,
        "normalize_advantages": True,
        "optimizer_type": "adam",
    },
    "Walker2d-v5": {
        "num_envs": 1,
        "rollout_steps": 2048,
        "updates_per_step": 1,
        "total_steps": 2_500_000,
        "policy_layer_sizes": (256, 256, 256),
        "value_layer_sizes": (512, 512, 256),
        "gamma": 0.99,
        "policy_lr": 1e-4,
        "value_lr": 1e-4,
        "topk_fraction": 0.5,
        "temperature_init": 1.0,
        "temperature_lr": 1e-4,
        "alpha_lr": 1e-4,
        "epsilon_eta": 0.15,
        "epsilon_mu": 0.05,
        "epsilon_sigma": 1e-5,
        "max_grad_norm": 1.0,
        "normalize_advantages": True,
        "optimizer_type": "adam",
    },
}


def get(env_id: str) -> dict[str, Any]:
    return get_preset(
        env_id=env_id,
        presets=PRESETS,
        algorithm_name="VMPO",
        defaults={
            "optimizer_type": "adam",
            "sgd_momentum": 0.9,
            "advantage_estimator": "returns",
            "gae_lambda": 0.95,
        },
    )
