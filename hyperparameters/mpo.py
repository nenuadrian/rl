from __future__ import annotations

from typing import Any


PRESETS: dict[tuple[str, str], dict[str, Any]] = {
    ("cheetah", "run"): {
        "updates_per_step": 1,
        "total_steps": 20_000_000,
        "update_after": 1_000,
        "batch_size": 256,
        "replay_size": 1_000_000,
        "eval_interval": 10_000,
        "save_interval": 50_000,
        "policy_layer_sizes": (256, 256, 256),
        "critic_layer_sizes": (256, 256, 256),
        "gamma": 0.995,
        "tau": 0.005,
        "policy_lr": 3e-4,
        "q_lr": 3e-4,
        "temperature_init": 1.0,
        "temperature_lr": 3e-4,
        "kl_epsilon": 0.2,
        "mstep_kl_epsilon": 0.1,
        "epsilon_mean": None,
        "epsilon_stddev": None,
        "per_dim_constraining": True,
        "lambda_init": 1.0,
        "lambda_lr": 3e-4,
        "action_penalization": False,
        "epsilon_penalty": 0.001,
        "max_grad_norm": 1.0,
        "action_samples": 128,
        "use_retrace": True,
        "retrace_steps": 2,
        "retrace_mc_actions": 8,
        "retrace_lambda": 0.95,
    },
    ("humanoid", "run"): {
        "updates_per_step": 1,
        "total_steps": 50_000_000,
        "update_after": 5_000,
        "batch_size": 512,
        "replay_size": 2_000_000,
        "eval_interval": 2_000,
        "save_interval": 50_000,
        "policy_layer_sizes": (256, 256, 256),
        "critic_layer_sizes": (512, 256, 256),
        "gamma": 0.995,
        "tau": 0.005,
        "policy_lr": 3e-4,
        "q_lr": 3e-4,
        "temperature_init": 3.0,
        "temperature_lr": 3e-4,
        "kl_epsilon": 0.1,
        "mstep_kl_epsilon": 0.1,
        "epsilon_mean": None,
        "epsilon_stddev": None,
        "per_dim_constraining": False,
        "lambda_init": 1.0,
        "lambda_lr": 3e-4,
        "action_penalization": False,
        "epsilon_penalty": 0.001,
        "max_grad_norm": 1.0,
        "action_samples": 256,
        "use_retrace": True,
        "retrace_steps": 2,
        "retrace_mc_actions": 8,
        "retrace_lambda": 0.95,
    },
    ("humanoid", "walk"): {
        "updates_per_step": 1,
        "total_steps": 50_000_000,
        "update_after": 500_000,
        "batch_size": 512,
        "replay_size": 1_000_000,
        "eval_interval": 20_000,
        "save_interval": 50_000,
        "policy_layer_sizes": (256, 256, 256),
        "critic_layer_sizes": (512, 256, 256),
        "gamma": 0.995,
        "tau": 0.005,
        "policy_lr": 3e-4,
        "q_lr": 3e-4,
        "temperature_init": 5.0,
        "temperature_lr": 3e-4,
        "kl_epsilon": 0.1,
        "mstep_kl_epsilon": 0.1,
        "epsilon_mean": None,
        "epsilon_stddev": None,
        "per_dim_constraining": False,
        "lambda_init": 1.0,
        "lambda_lr": 3e-4,
        "action_penalization": False,
        "epsilon_penalty": 0.001,
        "max_grad_norm": 1.0,
        "action_samples": 256,
        "use_retrace": True,
        "retrace_steps": 2,
        "retrace_mc_actions": 8,
        "retrace_lambda": 0.95,
    },
    ("walker", "walk"): {
        "updates_per_step": 1,
        "total_steps": 40_000_000,
        "update_after": 500_000,
        "batch_size": 256,
        "replay_size": 1_000_000,
        "eval_interval": 3_000,
        "save_interval": 50_000,
        "policy_layer_sizes": (256, 256, 256),
        "critic_layer_sizes": (512, 256, 256),
        "gamma": 0.995,
        "tau": 0.005,
        "policy_lr": 3e-4,
        "q_lr": 3e-4,
        "temperature_init": 3.0,
        "temperature_lr": 3e-4,
        "kl_epsilon": 0.1,
        "mstep_kl_epsilon": 0.1,
        "epsilon_mean": None,
        "epsilon_stddev": None,
        "per_dim_constraining": True,
        "lambda_init": 1.0,
        "lambda_lr": 3e-4,
        "action_penalization": True,
        "epsilon_penalty": 0.001,
        "max_grad_norm": 1.0,
        "action_samples": 256,
        "use_retrace": True,
        "retrace_steps": 2,
        "retrace_mc_actions": 8,
        "retrace_lambda": 0.95,
    },
    ("walker", "run"): {
        "updates_per_step": 1,
        "total_steps": 40_000_000,
        "update_after": 2_000,
        "batch_size": 512,
        "replay_size": 1_000_000,
        "eval_interval": 3_000,
        "save_interval": 10_000,
        "policy_layer_sizes": (256, 256, 256),
        "critic_layer_sizes": (512, 512, 256),
        "gamma": 0.995,
        "tau": 0.005,
        "policy_lr": 3e-4,
        "q_lr": 3e-4,
        "temperature_init": 3.0,
        "temperature_lr": 3e-4,
        "kl_epsilon": 0.1,
        "mstep_kl_epsilon": 0.1,
        "epsilon_mean": None,
        "epsilon_stddev": None,
        "per_dim_constraining": False,
        "lambda_init": 1.0,
        "lambda_lr": 3e-4,
        "action_penalization": False,
        "epsilon_penalty": 0.001,
        "max_grad_norm": 1.0,
        "action_samples": 512,
        "use_retrace": True,
        "retrace_steps": 5,
        "retrace_mc_actions": 8,
        "retrace_lambda": 0.95,
    },
    ("hopper", "stand"): {
        "updates_per_step": 1,
        "total_steps": 2_000_000,
        "update_after": 1_000,
        "batch_size": 256,
        "replay_size": 1_000_000,
        "eval_interval": 10_000,
        "save_interval": 50_000,
        "policy_layer_sizes": (256, 256, 256),
        "critic_layer_sizes": (512, 512, 256),
        "gamma": 0.995,
        "tau": 0.005,
        "policy_lr": 3e-4,
        "q_lr": 3e-4,
        "temperature_init": 5.0,
        "temperature_lr": 3e-4,
        "kl_epsilon": 0.1,
        "mstep_kl_epsilon": 0.1,
        "epsilon_mean": None,
        "epsilon_stddev": None,
        "per_dim_constraining": True,
        "lambda_init": 1.0,
        "lambda_lr": 3e-4,
        "action_penalization": False,
        "epsilon_penalty": 0.001,
        "max_grad_norm": 1.0,
        "action_samples": 64,
        "use_retrace": True,
        "retrace_steps": 2,
        "retrace_mc_actions": 8,
        "retrace_lambda": 0.95,
    },
    ("cartpole", "swingup"): {
        "updates_per_step": 1,
        "total_steps": 500_000,
        "update_after": 1_000,
        "batch_size": 256,
        "replay_size": 200_000,
        "eval_interval": 5_000,
        "save_interval": 50_000,
        "policy_layer_sizes": (256, 256, 256),
        "critic_layer_sizes": (512, 512, 256),
        "gamma": 0.995,
        "tau": 0.005,
        "policy_lr": 3e-4,
        "q_lr": 3e-4,
        "temperature_init": 1.0,
        "temperature_lr": 3e-4,
        "kl_epsilon": 0.1,
        "mstep_kl_epsilon": 0.1,
        "epsilon_mean": None,
        "epsilon_stddev": None,
        "per_dim_constraining": True,
        "lambda_init": 1.0,
        "lambda_lr": 1e-3,
        "action_penalization": False,
        "epsilon_penalty": 0.001,
        "max_grad_norm": 1.0,
        "action_samples": 64,
        "use_retrace": False,
        "retrace_steps": 2,
        "retrace_mc_actions": 8,
        "retrace_lambda": 0.95,
    },
}


def get(domain: str, task: str) -> dict[str, Any]:
    key = (domain, task)
    if key not in PRESETS:
        available = ", ".join([f"{d}/{t}" for (d, t) in sorted(PRESETS.keys())])
        raise KeyError(f"No MPO preset for {domain}/{task}. Available: {available}")
    return dict(PRESETS[key])
