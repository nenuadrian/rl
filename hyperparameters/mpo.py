from __future__ import annotations

from typing import Any

from hyperparameters._common import get_preset


PRESETS: dict[str, dict[str, Any]] = {
    "dm_control/cheetah/run": {
        "replay_size": 1_000_000,
        "save_interval": 1_000_000,
        "total_steps": 400_000,
        "update_after": 1_000,
        "batch_size": 128,
        "eval_interval": 1_000,
        "updates_per_step": 1,
        "policy_layer_sizes": (256, 256, 256),
        "critic_layer_sizes": (512, 512, 256),
        "gamma": 0.995,
        "target_policy_update_period": 50,
        "target_critic_update_period": 100,
        "policy_lr": 3e-4,
        "q_lr": 3e-4,
        "temperature_lr": 3e-4,
        "kl_epsilon": 0.2,
        "mstep_kl_epsilon": 0.3,
        "per_dim_constraining": False,
        "lambda_init": 1.0,
        "lambda_lr": 3e-4,
        "action_penalization": False,
        "epsilon_penalty": 0.001,
        "max_grad_norm": 1.0,
        "action_samples": 32,
        "temperature_init": 1.0,
        "use_retrace": False,
        "retrace_steps": 2,
        "retrace_mc_actions": 8,
        "retrace_lambda": 0.95,
    },
    "dm_control/humanoid/run": {
        "replay_size": 1_000_000,
        "save_interval": 50_000,
        "total_steps": 400_000,
        "update_after": 1_000,
        "batch_size": 512,
        "eval_interval": 2_000,
        "updates_per_step": 1,
        "policy_layer_sizes": (256, 256, 256),
        "critic_layer_sizes": (512, 512, 256),
        "gamma": 0.995,
        "target_policy_update_period": 100,
        "target_critic_update_period": 100,
        "policy_lr": 3e-4,
        "q_lr": 3e-4,
        "temperature_lr": 3e-4,
        "kl_epsilon": 0.1,
        "mstep_kl_epsilon": 0.1,
        "per_dim_constraining": True,
        "lambda_init": 1.0,
        "lambda_lr": 3e-4,
        "action_penalization": False,
        "epsilon_penalty": 0.001,
        "max_grad_norm": 1.0,
        "action_samples": 256,
        "temperature_init": 1.0,
        "use_retrace": True,
        "retrace_steps": 2,
        "retrace_mc_actions": 8,
        "retrace_lambda": 0.95,
    },
    "dm_control/humanoid/walk": {
        "replay_size": 1_000_000,
        "save_interval": 50_000,
        "total_steps": 400_000,
        "update_after": 1_000,
        "batch_size": 512,
        "eval_interval": 100,
        "updates_per_step": 2,
        "policy_layer_sizes": (256, 256, 256),
        "critic_layer_sizes": (512, 512, 256),
        "gamma": 0.995,
        "target_policy_update_period": 100,
        "target_critic_update_period": 100,
        "policy_lr": 3e-4,
        "q_lr": 3e-4,
        "temperature_lr": 3e-4,
        "kl_epsilon": 0.1,
        "mstep_kl_epsilon": 0.1,
        "per_dim_constraining": True,
        "lambda_init": 1.0,
        "lambda_lr": 3e-4,
        "action_penalization": False,
        "epsilon_penalty": 0.001,
        "max_grad_norm": 1.0,
        "action_samples": 256,
        "temperature_init": 1.0,
        "use_retrace": True,
        "retrace_steps": 2,
        "retrace_mc_actions": 8,
        "retrace_lambda": 0.95,
    },
    "dm_control/walker/walk": {
        "replay_size": 1_000_000,
        "save_interval": 50_000,
        "total_steps": 400_000,
        "update_after": 1_000,
        "batch_size": 256,
        "eval_interval": 100,
        "updates_per_step": 2,
        "policy_layer_sizes": (256, 256, 256),
        "critic_layer_sizes": (512, 512, 256),
        "gamma": 0.995,
        "target_policy_update_period": 100,
        "target_critic_update_period": 100,
        "policy_lr": 3e-4,
        "q_lr": 3e-4,
        "temperature_lr": 3e-4,
        "kl_epsilon": 0.1,
        "mstep_kl_epsilon": 0.1,
        "per_dim_constraining": True,
        "lambda_init": 1.0,
        "lambda_lr": 3e-4,
        "action_penalization": False,
        "epsilon_penalty": 0.001,
        "max_grad_norm": 1.0,
        "action_samples": 256,
        "temperature_init": 1.0,
        "use_retrace": True,
        "retrace_steps": 2,
        "retrace_mc_actions": 8,
        "retrace_lambda": 0.95,
    },
    "dm_control/walker/run": {
        "replay_size": 1_000_000,
        "save_interval": 50_000,
        "total_steps": 400_000,
        "update_after": 1_000,
        "batch_size": 256,
        "eval_interval": 100,
        "updates_per_step": 2,
        "policy_layer_sizes": (256, 256, 256),
        "critic_layer_sizes": (512, 512, 256),
        "gamma": 0.995,
        "target_policy_update_period": 100,
        "target_critic_update_period": 100,
        "policy_lr": 3e-4,
        "q_lr": 3e-4,
        "temperature_lr": 3e-4,
        "kl_epsilon": 0.1,
        "mstep_kl_epsilon": 0.1,
        "per_dim_constraining": True,
        "lambda_init": 1.0,
        "lambda_lr": 3e-4,
        "action_penalization": False,
        "epsilon_penalty": 0.001,
        "max_grad_norm": 1.0,
        "action_samples": 256,
        "temperature_init": 3.0,
        "use_retrace": True,
        "retrace_steps": 2,
        "retrace_mc_actions": 8,
        "retrace_lambda": 0.95,
    },
    "dm_control/cartpole/swingup": {
        "replay_size": 200_000,
        "save_interval": 50_000,
        "total_steps": 100_000,
        "update_after": 1_000,
        "batch_size": 256,
        "eval_interval": 100,
        "updates_per_step": 1,
        "policy_layer_sizes": (256, 256, 256),
        "critic_layer_sizes": (512, 512, 256),
        "gamma": 0.995,
        "target_policy_update_period": 100,
        "target_critic_update_period": 100,
        "policy_lr": 3e-4,
        "q_lr": 3e-4,
        "temperature_lr": 3e-4,
        "kl_epsilon": 0.1,
        "mstep_kl_epsilon": 0.1,
        "per_dim_constraining": True,
        "lambda_init": 1.0,
        "lambda_lr": 1e-3,
        "action_penalization": False,
        "epsilon_penalty": 0.001,
        "max_grad_norm": 1.0,
        "action_samples": 64,
        "temperature_init": 1.0,
        "use_retrace": False,
        "retrace_steps": 2,
        "retrace_mc_actions": 8,
        "retrace_lambda": 0.95,
    },
    "Humanoid-v5": {
        "replay_size": 1_000_000,
        "save_interval": 50_000,
        "total_steps": 400_000,
        "update_after": 50_000,
        "batch_size": 512,
        "eval_interval": 100,
        "updates_per_step": 1,
        "policy_layer_sizes": (256, 256, 256),
        "critic_layer_sizes": (512, 512, 256),
        "gamma": 0.995,
        "target_policy_update_period": 100,
        "target_critic_update_period": 100,
        "policy_lr": 3e-4,
        "q_lr": 3e-4,
        "temperature_lr": 3e-4,
        "kl_epsilon": 0.1,
        "mstep_kl_epsilon": 0.1,
        "per_dim_constraining": True,
        "lambda_init": 1.0,
        "lambda_lr": 3e-4,
        "action_penalization": False,
        "epsilon_penalty": 0.001,
        "max_grad_norm": 1.0,
        "action_samples": 256,
        "temperature_init": 3.0,
        "use_retrace": True,
        "retrace_steps": 2,
        "retrace_mc_actions": 8,
        "retrace_lambda": 0.95,
    },
    "HalfCheetah-v5": {
        "replay_size": 1_000_000,
        "save_interval": 100_000,
        "total_steps": 100_000,
        "update_after": 10_000,
        "batch_size": 512,
        "eval_interval": 100,
        "updates_per_step": 1,
        "policy_layer_sizes": (256, 256, 256),
        "critic_layer_sizes": (512, 512, 256),
        "gamma": 0.995,
        "target_policy_update_period": 100,
        "target_critic_update_period": 100,
        "policy_lr": 3e-4,
        "q_lr": 3e-4,
        "temperature_lr": 3e-4,
        "kl_epsilon": 0.1,
        "mstep_kl_epsilon": 0.1,
        "per_dim_constraining": True,
        "lambda_init": 1.0,
        "lambda_lr": 3e-4,
        "action_penalization": False,
        "epsilon_penalty": 0.001,
        "max_grad_norm": 1.0,
        "action_samples": 256,
        "temperature_init": 3.0,
        "use_retrace": True,
        "retrace_steps": 2,
        "retrace_mc_actions": 8,
        "retrace_lambda": 0.95,
    },
    "Walker2d-v5": {
        "replay_size": 1_000_000,
        "save_interval": 50_000,
        "total_steps": 150_000,
        "update_after": 10_000,
        "batch_size": 512,
        "eval_interval": 100,
        "updates_per_step": 1,
        "policy_layer_sizes": (256, 256, 256),
        "critic_layer_sizes": (512, 512, 256),
        "gamma": 0.995,
        "target_policy_update_period": 100,
        "target_critic_update_period": 100,
        "policy_lr": 3e-4,
        "q_lr": 3e-4,
        "temperature_lr": 3e-4,
        "kl_epsilon": 0.1,
        "mstep_kl_epsilon": 0.1,
        "per_dim_constraining": True,
        "lambda_init": 1.0,
        "lambda_lr": 3e-4,
        "action_penalization": False,
        "epsilon_penalty": 0.001,
        "max_grad_norm": 1.0,
        "action_samples": 256,
        "temperature_init": 3.0,
        "use_retrace": True,
        "retrace_steps": 2,
        "retrace_mc_actions": 8,
        "retrace_lambda": 0.95,
    },
    "Ant-v5": {
        "replay_size": 1_000_000,
        "save_interval": 50_000,
        "total_steps": 400_000,
        "update_after": 10_000,
        "batch_size": 512,
        "eval_interval": 100,
        "updates_per_step": 1,
        "policy_layer_sizes": (256, 256, 256),
        "critic_layer_sizes": (512, 512, 256),
        "gamma": 0.995,
        "target_policy_update_period": 100,
        "target_critic_update_period": 100,
        "policy_lr": 3e-4,
        "q_lr": 3e-4,
        "temperature_lr": 3e-4,
        "kl_epsilon": 0.1,
        "mstep_kl_epsilon": 0.1,
        "per_dim_constraining": True,
        "lambda_init": 1.0,
        "lambda_lr": 3e-4,
        "action_penalization": False,
        "epsilon_penalty": 0.001,
        "max_grad_norm": 1.0,
        "action_samples": 256,
        "temperature_init": 3.0,
        "use_retrace": True,
        "retrace_steps": 2,
        "retrace_mc_actions": 8,
        "retrace_lambda": 0.95,
    },
}


def get(env_id: str) -> dict[str, Any]:
    return get_preset(
        env_id=env_id,
        presets=PRESETS,
        algorithm_name="MPO",
        defaults={"optimizer_type": "adam", "sgd_momentum": 0.9},
    )
