from __future__ import annotations

from typing import Any


PRESETS: dict[tuple[str, str], dict[str, Any]] = {
    ("cheetah", "run"): {
        "total_steps": 10_000_000,
        "start_steps": 5_000,
        "update_after": 1_000,
        "batch_size": 256,
        "replay_size": 1_000_000,
        "eval_interval": 10_000,
        "save_interval": 1_000_000,
        "hidden_sizes": (256, 256),
        "gamma": 0.995,
        "tau": 0.005,
        "policy_lr": 3e-4,
        "q_lr": 3e-4,
        "eta_init": 1.0,
        "eta_lr": 3e-4,
        "kl_epsilon": 0.2,
        "mstep_kl_epsilon": 0.1,
        "lambda_init": 1.0,
        "lambda_lr": 3e-4,
        "max_grad_norm": 1.0,
        "action_samples": 64,
        "retrace_steps": 15,
        "retrace_mc_actions": 32,
    },
    ("humanoid", "run"): {
        "total_steps": 30_000_000,
        "start_steps": 10_000,
        "update_after": 5_000,
        "batch_size": 512,
        "replay_size": 2_000_000,
        "eval_interval": 25_000,
        "save_interval": 1_000_000,
        "hidden_sizes": (512, 256),
        "gamma": 0.995,
        "tau": 0.005,
        "policy_lr": 3e-4,
        "q_lr": 3e-4,
        "eta_init": 5.0,
        "eta_lr": 3e-4,
        "kl_epsilon": 0.1,
        "mstep_kl_epsilon": 0.1,
        "lambda_init": 1.0,
        "lambda_lr": 3e-4,
        "max_grad_norm": 1.0,
        "action_samples": 128,
        "retrace_steps": 30,
        "retrace_mc_actions": 32,
    },
    ("humanoid", "walk"): {
        "total_steps": 30_000_000,
        "start_steps": 10_000,
        "update_after": 2_000,
        "batch_size": 512,
        "replay_size": 1_000_000,
        "eval_interval": 20_000,
        "save_interval": 1_000_000,
        "hidden_sizes": (512, 256),
        "gamma": 0.995,
        "tau": 0.005,
        "policy_lr": 3e-4,
        "q_lr": 3e-4,
        "eta_init": 5.0,
        "eta_lr": 3e-4,
        "kl_epsilon": 0.1,
        "mstep_kl_epsilon": 0.1,
        "lambda_init": 1.0,
        "lambda_lr": 3e-4,
        "max_grad_norm": 1.0,
        "action_samples": 128,
        "retrace_steps": 30,
        "retrace_mc_actions": 32,
    },
    ("hopper", "stand"): {
        "total_steps": 2_000_000,
        "start_steps": 5_000,
        "update_after": 1_000,
        "batch_size": 256,
        "replay_size": 1_000_000,
        "eval_interval": 10_000,
        "save_interval": 1_000_000,
        "hidden_sizes": (256, 256),
        "gamma": 0.995,
        "tau": 0.005,
        "policy_lr": 3e-4,
        "q_lr": 3e-4,
        "eta_init": 5.0,
        "eta_lr": 3e-4,
        "kl_epsilon": 0.1,
        "mstep_kl_epsilon": 0.1,
        "lambda_init": 1.0,
        "lambda_lr": 3e-4,
        "max_grad_norm": 1.0,
        "action_samples": 64,
        "retrace_steps": 30,
        "retrace_mc_actions": 32,
    },
    ("cartpole", "swingup"): {
        "total_steps": 500_000,
        "start_steps": 2_000,
        "update_after": 500,
        "batch_size": 256,
        "replay_size": 200_000,
        "eval_interval": 5_000,
        "save_interval": 1_000_000,
        "hidden_sizes": (128, 128),
        "gamma": 0.995,
        "tau": 0.005,
        "policy_lr": 3e-4,
        "q_lr": 3e-4,
        "eta_init": 1.0,
        "eta_lr": 3e-4,
        "kl_epsilon": 0.1,
        "mstep_kl_epsilon": 0.1,
        "lambda_init": 1.0,
        "lambda_lr": 1e-3,
        "max_grad_norm": 1.0,
        "action_samples": 64,
        "retrace_steps": 30,
        "retrace_mc_actions": 32,
    },
}


def get(domain: str, task: str) -> dict[str, Any]:
    key = (domain, task)
    if key not in PRESETS:
        available = ", ".join([f"{d}/{t}" for (d, t) in sorted(PRESETS.keys())])
        raise KeyError(f"No MPO preset for {domain}/{task}. Available: {available}")
    return dict(PRESETS[key])
