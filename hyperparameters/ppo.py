from __future__ import annotations

from typing import Any


PRESETS: dict[tuple[str, str], dict[str, Any]] = {
    ("cheetah", "run"): {
        "num_envs": 4,
        "total_steps": 3_000_000,
        "eval_interval": 10_000,
        "save_interval": 1_000_000,
        "policy_layer_sizes": (256, 256, 256),
        "critic_layer_sizes": (256, 256, 256),
        "rollout_steps": 2048,
        "update_epochs": 4,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "minibatch_size": 256,
        "policy_lr": 2e-4,
        "value_lr": 3e-4,
        "clip_ratio": 0.25,
        "ent_coef": 1e-4,
        "vf_coef": 1.0,
        "max_grad_norm": 0.5,
        "target_kl": 0.02,
        "normalize_obs": True,
    },
    ("humanoid", "run"): {
        "total_steps": 10_000_000,
        "eval_interval": 25_000,
        "save_interval": 1_000_000,
        "policy_layer_sizes": (256, 256, 256),
        "critic_layer_sizes": (512, 256, 256),
        "rollout_steps": 8192,
        "update_epochs": 12,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "minibatch_size": 512,
        "policy_lr": 1e-4,
        "value_lr": 2e-5,
        "clip_ratio": 0.15,
        "ent_coef": 3e-4,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5,
        "target_kl": 0.02,
        "normalize_obs": True,
    },
    ("humanoid", "walk"): {
        "total_steps": 5_000_000,
        "eval_interval": 100_000,
        "save_interval": 1_000_000,
        "policy_layer_sizes": (256, 256, 256),
        "critic_layer_sizes": (512, 256, 256),
        "rollout_steps": 8192 * 2,
        "update_epochs": 8,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "minibatch_size": 256,
        "policy_lr": 2e-4,
        "value_lr": 5e-5,
        "clip_ratio": 0.2,
        "ent_coef": 5e-4,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5,
        "target_kl": 0.02,
        "normalize_obs": True,
    },
    ("walker", "walk"): {
        "num_envs": 4,
        "total_steps": 10_000_000,
        "eval_interval": 15_000,
        "save_interval": 50_000,
        "policy_layer_sizes": (256, 256, 256),
        "critic_layer_sizes": (512, 256, 256),
        "rollout_steps": 2048,
        "update_epochs": 4,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "minibatch_size": 128,
        "policy_lr": 2e-4,
        "value_lr": 1e-4,
        "clip_ratio": 0.2,
        "ent_coef": 1e-4,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5,
        "target_kl": 0.02,
        "normalize_obs": True,
    },
    ("walker", "run"): {
        "num_envs": 4,
        "total_steps": 5_000_000,
        "eval_interval": 15_000,
        "save_interval": 50_000,
        "policy_layer_sizes": (256, 256, 256),
        "critic_layer_sizes": (512, 256, 256),
        "rollout_steps": 4096,
        "update_epochs": 6,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "minibatch_size": 256,
        "policy_lr": 2e-4,
        "value_lr": 1e-4,
        "clip_ratio": 0.2,
        "ent_coef": 1e-4,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5,
        "target_kl": 0.02,
        "normalize_obs": True,
    },
    ("hopper", "stand"): {
        "total_steps": 2_000_000,
        "eval_interval": 10_000,
        "save_interval": 1_000_000,
        "policy_layer_sizes": (256, 256, 256),
        "critic_layer_sizes": (256, 256, 256),
        "rollout_steps": 2048,
        "update_epochs": 10,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "minibatch_size": 128,
        "policy_lr": 3e-4,
        "value_lr": 1e-4,
        "clip_ratio": 0.2,
        "ent_coef": 2e-4,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5,
        "target_kl": 0.02,
        "normalize_obs": True,
    },
    ("cartpole", "swingup"): {
        "total_steps": 1_000_000,
        "eval_interval": 5_000,
        "save_interval": 1_000_000,
        "policy_layer_sizes": (256, 256, 256),
        "critic_layer_sizes": (256, 256, 256),
        "rollout_steps": 2048,
        "update_epochs": 10,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "minibatch_size": 128,
        "policy_lr": 3e-4,
        "value_lr": 1e-4,
        "clip_ratio": 0.2,
        "ent_coef": 1e-3,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5,
        "target_kl": 0.02,
        "normalize_obs": False,
    },
}


def get(domain: str, task: str) -> dict[str, Any]:
    key = (domain, task)
    if key not in PRESETS:
        available = ", ".join([f"{d}/{t}" for (d, t) in sorted(PRESETS.keys())])
        raise KeyError(f"No PPO preset for {domain}/{task}. Available: {available}")
    return dict(PRESETS[key])
