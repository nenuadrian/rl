\documentclass[10pt]{article}

\usepackage[margin=1in]{geometry}

\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{url}
% For \coloneqq and related symbols
\usepackage{mathtools}
\usepackage{breqn}
\usepackage[style=authoryear, backend=biber]{biblatex}

\addbibresource{references.bib}

\begin{document}

\setcounter{secnumdepth}{1}

\title{Expectation--Maximization Frameworks for LLM RL Fine-Tuning}

\maketitle
\tableofcontents
\clearpage

\section{Research Questions}
Transformer language models are commonly adapted to downstream tasks via supervised fine-tuning (SFT), and further improved via RL fine-tuning against a learned or human preference reward. While these procedures are usually presented as distinct (cross-entropy training versus policy optimization), both can be interpreted as alternating between constructing a training target distribution and then fitting the model to that target.

\begin{enumerate}
  \item Can supervised fine-tuning and KL-regularized RL fine-tuning be expressed under a common EM/MAP formulation with a shared E-step/M-step interpretation?
  \item In what precise sense does V-MPO correspond to regularized policy iteration, and how does that differ from direct policy-gradient optimization (e.g.\ PPO)?
  \item Does adaptive temperature optimization in the E-step provide a practical stability advantage over fixed-temperature weighting at LLM scale?
  \item How does the DAR closed-form target relate to the V-MPO target, and under which limits are they equivalent?
  \item Do EM-style weighted-MLE updates provide practical benefits for transformer fine-tuning, including compatibility with dropout-style regularization?
\end{enumerate}

\section{Preliminaries}

There are several methods for optimizing a policy $\pi_\theta$. They differ in how they move from a current parameter $\theta_k$ to a better one $\theta_{k+1}$. The four estimation paradigms that recur throughout are MLE, MAP, policy gradients, and EM-style policy iteration.

\paragraph{Maximum likelihood estimation (MLE).}
Given observed data $X = \{x_i\}_{i=1}^N$, MLE finds the parameters that maximize the data likelihood:
\[
  \theta_{\mathrm{MLE}}
  =
  \arg\max_\theta
  \sum_{i=1}^{N}
  \log p_\theta(x_i).
\]

\paragraph{Maximum a posteriori (MAP).}
MAP adds a prior $p(\theta)$, acting as a regularizer on the objective:
\[
  \theta_{\mathrm{MAP}}
  =
  \arg\max_\theta
  \bigl(
    \log p_\theta(X)
    +
    \log p(\theta)
  \bigr).
\]
MAP reduces to MLE with an uniform prior $\log p(\theta) = \log c$.

\paragraph{Policy gradients.}
Given a return objective $J(\theta)=\mathbb{E}_{\tau\sim\pi_\theta}[R(\tau)]$, policy-gradient methods take a direct ascent step in parameter space:
\[
  \theta_{k+1}
  =
  \theta_k
  +
  \lambda\,
  \widehat{\nabla_\theta J(\theta_k)},
\]
optionally with clipping (PPO) or explicit KL penalties to limit step size. The gradient estimate is noisy and acts directly on $\theta$.

\paragraph{EM policy iteration.}
Introduction an auxiliary distribution $q$ for decoupling of improvement and fitting. The E-step finds an improved non-parametric target by solving a regularized advantage-maximization:
\[
  q_{k+1}
  =
  \arg\max_q
  \Big(
    \mathbb{E}_{a\sim q}
    [A^{\pi_k}(s,a)]
    -
    \eta\,
    \mathrm{KL}(q(\cdot\mid s)\,\|\,\pi_k(\cdot\mid s))
  \Big)
  \;\;\Longrightarrow\;\;
  q_{k+1}(a\mid s)
  \propto
  \pi_k(a\mid s)\,
  \exp\!\Big(\frac{A^{\pi_k}(s,a)}{\eta}\Big).
\]
The M-step then projects this target back onto the parametric policy via weighted maximum likelihood:
\[
  \theta_{k+1}
  =
  \arg\max_\theta\;
  \mathbb{E}_{a\sim q_{k+1}}
  [\log \pi_\theta(a\mid s)],
\]
often with an additional trust-region term on $\mathrm{KL}(\pi_k\|\pi_\theta)$. Connection to classical EM: the E-step constructs a target distribution (analogous to computing the posterior $q(z) = p_{\theta_k}(z\mid x)$), and the M-step fits the model to that target (analogous to maximizing the expected complete-data log-likelihood).

\paragraph{Key distinction.}
Policy gradients optimize parameters directly via noisy first-order steps. EM-style methods perform policy improvement in distribution space first, then do weighted maximum-likelihood fitting. The M-step is structurally identical to supervised learning, which has practical consequences for stability, dropout compatibility, and integration with standard training pipelines.

\paragraph{LLM alignment.}
Supervised fine-tuning (SFT) is MLE: maximise $\sum_t \log \pi_\theta(y_t \mid y_{<t}, x)$ over demonstration data, with no reward signal or policy comparison. It is the starting point for most alignment pipelines, producing the initial policy that subsequent RL stages improve. PPO is the canonical policy-gradient method for RLHF: it collects on-policy rollouts scored by a reward model and updates $\theta$ via clipped importance-sampling ratios. V-MPO, AWR, and DAR are EM-style alternatives that replace the ratio-based gradient with the E-step/M-step decomposition above---the M-step is weighted cross-entropy over the same rollouts, differing only in how the weights are computed. DPO collapses the EM loop into a single preference-MLE objective by analytically eliminating the E-step, avoiding explicit reward modelling or rollout generation entirely.



\paragraph{RL Fine-tuning}

Given a language model $\pi_\theta$ to be aligned, a prompt dataset $\mathcal{D}(x)$ and a reward model $r$, online RL fine-tuning aims to optimise:

\begin{equation*}
  J_{\mathrm{RLHF}}(\pi_\theta; \pi_{\mathrm{ref}})
  =
  \max_{\pi_\theta}
  \mathbb{E}_{x \sim \mathcal{D}(x), y \sim \pi_\theta(y|x)}
  \left[
    r(x,y)
  \right]
  -
  \alpha
  D_{\mathrm{KL}}
  \left(
    \pi_\theta(y|x)
    \;\|\;
    \pi_{\mathrm{ref}}(y|x)
  \right).
\end{equation*}

Here $\alpha > 0$ controls KL regularization toward a fixed reference policy $\pi_{\mathrm{ref}}$.


\section{V-MPO: Maximum a Posteriori Policy Optimization}

V-MPO \parencite{song2019vmpoonpolicymaximumposteriori} decomposes optimisation into EM phases. Its pairing with the Gated Transformer-XL architecture for RL is described in Appendix~\ref{sec:gtrxl}.

The total objective is
\begin{equation*}
  \mathcal{L}(\phi,\theta,\eta,\alpha)
  =
  \mathcal{L}_V(\phi)
  +
  \mathcal{L}_{\mathrm{V\text{-}MPO}}(\theta,\eta,\alpha),
\end{equation*}
where
\begin{equation*}
  \mathcal{L}_{\mathrm{V\text{-}MPO}}(\theta,\eta,\alpha)
  =
  \mathcal{L}_\pi(\theta)
  +
  \mathcal{L}_\eta(\eta)
  +
  \mathcal{L}_\alpha(\theta,\alpha).
\end{equation*}
% ============================================================
\paragraph{Policy Evaluation (Critic Update).}

The value function is fitted via n-step bootstrapped regression:
\begin{equation*}
  \mathcal{L}_V(\phi)
  =
  \frac{1}{2|\mathcal{D}|}
  \sum_{s_t \sim \mathcal{D}}
  \left(
    V^\pi_\phi(s_t)
    -
    G_t^{(n)}
  \right)^2,
\end{equation*}
with
\[
  G_t^{(n)}
  =
  \sum_{k=t}^{t+n-1}
  \gamma^{k-t} r_k
  +
  \gamma^n V^\pi_\phi(s_{t+n}).
\]

Advantages are defined as
\[
  A^\pi(s_t,a_t)
  =
  G_t^{(n)} - V^\pi_\phi(s_t).
\]

% ============================================================
\paragraph{Policy Improvement via EM}

MAP estimation, where $I$ denotes the improvement event.:
\begin{equation*}
  \theta^\star
  =
  \arg\max_\theta
  \log p_\theta(I=1)
  +
  \log p(\theta),
\end{equation*}


Introduce a variational distribution $\psi(s,a)$:
\begin{equation*}
  \log p_\theta(I=1)
  =
  \sum_{s,a}
  \psi(s,a)
  \log
  \frac{p_\theta(I=1,s,a)}{\psi(s,a)}
  +
  \mathrm{KL}
  \big(
    \psi(s,a)
    \;\|\;
    p_\theta(s,a \mid I=1)
  \big).
\end{equation*}


% ============================================================
\paragraph{E-Step: Non-Parametric Policy Construction}

The E-step solves
\begin{equation*}
  \begin{aligned}
    \psi^\star
    =
    \arg\max_{\psi}
    &\;
    \sum_{s,a}
    \psi(s,a)
    A^{\pi_{\theta_{\mathrm{old}}}}(s,a) \\
    \text{s.t.}
    &\;
    \sum_{s,a}
    \psi(s,a)
    \log
    \frac{\psi(s,a)}{p_{\theta_{\mathrm{old}}}(s,a)}
    <
    \epsilon_\eta,
    \\
    &\;
    \sum_{s,a} \psi(s,a) = 1.
  \end{aligned}
\end{equation*}

The Lagrangian is
\begin{equation*}
  \begin{aligned}
    J(\psi,\eta,\lambda)
    =
    &\sum_{s,a}
    \psi(s,a)
    A^{\pi_{\theta_{\mathrm{old}}}}(s,a)
    \\
    &+
    \eta
    \Big(
      \epsilon_\eta
      -
      \sum_{s,a}
      \psi(s,a)
      \log
      \frac{\psi(s,a)}{p_{\theta_{\mathrm{old}}}(s,a)}
    \Big)
    \\
    &+
    \lambda
    \Big(
      1 - \sum_{s,a} \psi(s,a)
    \Big).
  \end{aligned}
\end{equation*}

Here, stationarity refers to the KKT first-order optimality condition with respect to the variational distribution $\psi$:
\begin{equation*}
  \frac{\partial J}{\partial \psi(s,a)}
  =
  A^{\pi_{\theta_{\mathrm{old}}}}(s,a)
  -
  \eta
  \left(
    \log
    \frac{\psi(s,a)}{p_{\theta_{\mathrm{old}}}(s,a)}
    + 1
  \right)
  -
  \lambda
  =
  0,
  \quad \forall (s,a).
\end{equation*}

Solving this stationarity condition for $\psi$ gives
\begin{equation*}
  \psi(s,a)
  =
  \frac{
    p_{\theta_{\mathrm{old}}}(s,a)
    \exp\!\big(
      A^{\pi_{\theta_{\mathrm{old}}}}(s,a)/\eta
    \big)
  }{
    \sum_{s',a'}
    p_{\theta_{\mathrm{old}}}(s',a')
    \exp\!\big(
      A^{\pi_{\theta_{\mathrm{old}}}}(s',a')/\eta
    \big)
  }.
\end{equation*}

The temperature dual is
\begin{equation*}
  \mathcal{L}_\eta(\eta)
  =
  \eta \epsilon_\eta
  +
  \eta
  \log
  \Big(
    \sum_{s,a}
    p_{\theta_{\mathrm{old}}}(s,a)
    \exp
    \big(
      A^{\pi_{\theta_{\mathrm{old}}}}(s,a)/\eta
    \big)
  \Big).
\end{equation*}

% ============================================================
\paragraph{M-Step: Parametric Projection with KL}

The M-step minimises the negative lower bound:

\begin{equation*}
  \mathcal{L}_\pi(\theta)
  =
  -
  \sum_{s,a}
  \psi(s,a)
  \log
  \pi_\theta(a|s).
\end{equation*}

Subject to a KL trust-region constraint:
\begin{equation*}
  \mathbb{E}_{s \sim p(s)}
  \big[
    \mathrm{KL}
    (
      \pi_{\theta_{\mathrm{old}}}(\cdot|s)
      \;\|\;
      \pi_\theta(\cdot|s)
    )
  \big]
  <
  \epsilon_\alpha.
\end{equation*}

The Lagrangian form is
\begin{equation}
  J(\theta,\alpha)
  =
  \mathcal{L}_\pi(\theta)
  +
  \alpha
  \Big(
    \epsilon_\alpha
    -
    \mathbb{E}_{s}
    \mathrm{KL}
    (
      \pi_{\theta_{\mathrm{old}}}
      \|\,
      \pi_\theta
    )
  \Big).
  \label{eq:vmpo_mstep_lag}
\end{equation}

In implementation, the loss becomes
\begin{equation}
  \begin{aligned}
    \mathcal{L}_\alpha(\theta,\alpha)
    =
    \alpha
    \Big(
      \epsilon_\alpha
      -
      \mathrm{sg}
      [
        \mathrm{KL}
        (
          \pi_{\theta_{\mathrm{old}}}
          \|\,
          \pi_\theta
        )
      ]
    \Big)
    +
    \mathrm{sg}[\alpha]
    \,
    \mathrm{KL}
    (
      \pi_{\theta_{\mathrm{old}}}
      \|\,
      \pi_\theta
    ).
  \end{aligned}
\end{equation}

\section{PPO: Proximal Policy Optimization}

PPO is a PG method as it directly estimates and follows the gradient of expected return in parameter space with an on-policy actor-critic algorithm that constrains each policy update to stay close to the behaviour policy via a clipped surrogate objective, avoiding the instability of unconstrained policy gradient steps.

A trajectory $\tau = (s_0,a_0,\dots,s_{T-1},a_{T-1})$ is collected under the current policy $\pi_{\theta_{\mathrm{old}}}$. Per-step log-probabilities and their sum are
\[
  \ell_{\theta,t} = \log \pi_\theta(a_t \mid s_t),
  \qquad
  \ell_\theta(\tau) = \sum_{t=0}^{T-1} \ell_{\theta,t}.
\]

The discounted reward-to-go from step $t$ is
\[
  R_t = \sum_{k=t}^{T-1} \gamma^{k-t} r_k.
\]

The policy gradient theorem gives the direction of steepest ascent for the expected return:
\[
  \nabla_\theta J(\theta)
  =
  \mathbb{E}_{\tau \sim \pi_\theta}
  \left[
    \sum_{t=0}^{T-1}
    \nabla_\theta \log \pi_\theta(a_t \mid s_t)\, A_t
  \right].
\]

To reuse data collected under $\pi_{\theta_{\mathrm{old}}}$, importance sampling introduces the per-step probability ratio
\[
  r_t(\theta)
  =
  \frac{\pi_\theta(a_t \mid s_t)}
  {\pi_{\theta_{\mathrm{old}}}(a_t \mid s_t)}
  =
  \exp\!\left(\ell_{\theta,t}-\ell_{\mathrm{old},t}\right).
\]

The unclipped surrogate objective is then $L^{\mathrm{PG}}(\theta) = \mathbb{E}_t\!\left[r_t(\theta)\, A_t\right]$, but without further constraint this can lead to destructively large updates.

\[
  L^{\mathrm{CLIP}}(\theta)
  =
  \mathbb{E}_t
  \left[
    \min\!\left(
      r_t(\theta)\, A_t,\;
      \operatorname{clip}\!\left(
        r_t(\theta),\, 1-\epsilon,\, 1+\epsilon
      \right) A_t
    \right)
  \right].
\]

The critic is fitted by minimising a squared regression loss to the empirical returns:
\[
  L^{\mathrm{VF}}(\phi)
  =
  \mathbb{E}_t
  \left[
    \left(V_\phi(s_t) - R_t\right)^2
  \right].
\]

An entropy bonus encourages exploration by penalising premature policy collapse:
\[
  S(\pi_\theta(\cdot \mid s_t))
  =
  -\sum_a \pi_\theta(a \mid s_t) \log \pi_\theta(a \mid s_t).
\]

Rather than using raw Monte Carlo returns to estimate $A_t$, PPO typically uses GAE, which trades off bias and variance via a decay parameter $\lambda \in [0,1]$.

\[
  \delta_t = r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t).
\]

GAE accumulates these residuals with exponentially decaying weights:
\[
  A_t^{\mathrm{GAE}(\gamma,\lambda)}
  =
  \sum_{l=0}^{\infty} (\gamma\lambda)^l\, \delta_{t+l},
\]

\paragraph{Full Objective}

The three terms are combined into a single objective (to minimise):
\[
  \mathcal{L}(\theta,\phi)
  =
  -L^{\mathrm{CLIP}}(\theta)
  +
  c_1\, L^{\mathrm{VF}}(\phi)
  -
  c_2\, \mathbb{E}_t\!\left[S(\pi_\theta(\cdot \mid s_t))\right],
\]
where $c_1$ and $c_2$ are scalar coefficients balancing the three losses.

\paragraph{Sequence-Level PPO (LLM Case)}

When the ``action'' is an entire generated sequence (as in LLM fine-tuning), the per-step ratios multiply into a sequence-level ratio:
\[
  r_{\mathrm{seq}}(\theta)
  =
  \exp\!\left(
    \sum_{t=0}^{T-1}
    \bigl(\ell_{\theta,t} - \ell_{\mathrm{old},t}\bigr)
  \right).
\]

A KL penalty between the updated and old policy can be estimated cheaply as
$
  \widehat{\mathrm{KL}}
  =
  \mathbb{E}_t\!\left[\ell_{\mathrm{old},t} - \ell_{\theta,t}\right]
$.


\section{DPO: Direct Preference Optimization}
\label{sec:dpo}

DPO \parencite{rafailov2023dpo} is a preference MLE method: it bypasses reward modelling and policy-gradient estimation entirely, reducing alignment to a single binary cross-entropy objective over preference pairs.

Unlike policy-gradient methods, DPO needs no reward model (the reward is implicit), no value function, no advantage estimation, no importance-sampling ratios, and no online rollouts. The entire training signal comes from offline preference pairs.



\paragraph{Closed-form optimal policy.}
The optimisation over $\pi$ for each $x$ is a KL-regularised linear problem
\begin{equation}\label{eq:dpo_optpolicy}
  \pi^\star(y\mid x)
  =
  \frac{1}{Z(x)}\,
  \pi_{\mathrm{ref}}(y\mid x)\,
  \exp\!\Big(\frac{r(x,y)}{\beta}\Big),
\end{equation}
where $Z(x) = \sum_y \pi_{\mathrm{ref}}(y\mid x)\exp(r(x,y)/\beta)$ is the partition function.

\paragraph{Reward reparameterisation.}
Rearranging \eqref{eq:dpo_optpolicy} expresses the reward as a function of the optimal policy:
\begin{equation}\label{eq:dpo_reward_reparam}
  r(x,y)
  =
  \beta\log\frac{\pi^\star(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}
  +
  \beta\log Z(x).
\end{equation}

\paragraph{Bradley--Terry preference model.}
The probability of preferring response $y_w$ over $y_l$ under the Bradley--Terry model is $p(y_w \succ y_l \mid x) = \sigma\big(r(x,y_w) - r(x,y_l)\big)$, where $\sigma$ is the logistic function. Substituting \eqref{eq:dpo_reward_reparam}:
\begin{equation}\label{eq:dpo_bt_sub}
  p(y_w \succ y_l \mid x)
  =
  \sigma\!\bigg(
    \beta\log\frac{\pi^\star(y_w\mid x)}{\pi_{\mathrm{ref}}(y_w\mid x)}
    -
    \beta\log\frac{\pi^\star(y_l\mid x)}{\pi_{\mathrm{ref}}(y_l\mid x)}
  \bigg).
\end{equation}
The partition function $Z(x)$ cancels in the reward difference, algebraic step making DPO tractable.

\paragraph{The DPO loss.}
Replacing the unknown $\pi^\star$ with a parametric policy $\pi_\theta$ and maximising the log-likelihood of observed preferences yields the DPO objective:
\begin{equation}\label{eq:dpo_loss}
  \mathcal{L}_{\mathrm{DPO}}(\theta)
  =
  -\mathbb{E}_{(x,y_w,y_l)\sim\mathcal{D}}
  \left[
    \log\sigma\!\bigg(
      \beta\log\frac{\pi_\theta(y_w\mid x)}{\pi_{\mathrm{ref}}(y_w\mid x)}
      -
      \beta\log\frac{\pi_\theta(y_l\mid x)}{\pi_{\mathrm{ref}}(y_l\mid x)}
    \bigg)
  \right].
\end{equation}

\paragraph{DPO as Preference MLE}

The DPO loss \eqref{eq:dpo_loss} is binary cross-entropy over preference pairs---structurally, it is MLE on a classification task. The gradient has the form
\[
  \nabla_\theta \mathcal{L}_{\mathrm{DPO}}
  =
  -\beta\,\mathbb{E}\!\Big[
    \sigma\!\big(\hat{r}_\theta(y_l) - \hat{r}_\theta(y_w)\big)
    \big(
      \nabla_\theta\log\pi_\theta(y_w\mid x)
      -
      \nabla_\theta\log\pi_\theta(y_l\mid x)
    \big)
  \Big],
\]
where $\hat{r}_\theta(y) \coloneqq \beta\log(\pi_\theta(y\mid x)/\pi_{\mathrm{ref}}(y\mid x))$ is the implicit reward. The sigmoid weight measures how wrong the current model is on this pair: it is large when the model incorrectly prefers $y_l$, and vanishes as the model learns the correct ranking. This is a weighted MLE update that increases the log-probability of the preferred response and decreases that of the dispreferred one.

\paragraph{Dropout compatibility.}
DPO's gradient involves only single forward passes through $\pi_\theta$ (computing $\log\pi_\theta(y_w\mid x)$ and $\log\pi_\theta(y_l\mid x)$), with no ratio between two forward passes under different modes. Dropout is therefore compatible, for the same structural reasons of SFT and EM-style M-steps.

\section{AWR: Advantage-Weighted Regression}

Consider an MDP with states \(s\in\mathcal{S}\), actions \(a\in\mathcal{A}\), reward \(r(s,a)\) and discount
\( \gamma\in[0,1)\). For a policy \(\pi\) define the return of a state--action pair under a sampling policy
\(\mu\) as
\[
  R^\mu_{s,a} \;=\; \sum_{t=0}^{\infty} \gamma^{t} r_t,
   \qquad
  V^\mu(s) \;=\; \mathbb{E}_{a\sim\mu(\cdot|s)}\big[ R^\mu_{s,a} \big],
  \qquad
  A^\mu(s,a) \;=\; R^\mu_{s,a} - V^\mu(s).
\]

\paragraph{Expected improvement objective.}
Define the expected improvement of a candidate policy \(\pi\) over \(\mu\) as
\[
  \eta(\pi) \;=\; J(\pi) - J(\mu)
  \;=\;
  \mathbb{E}_{s\sim d_\pi}\mathbb{E}_{a\sim\pi(\cdot|s)}\big[ A^\mu(s,a)\big],
\]
where \(d_\pi(s)=\sum_{t=0}^\infty \gamma^{t} p(s_t=s\mid\pi)\) is the unnormalised discounted state distribution.
To avoid sampling from \(\pi\) during the optimisation we use the common first-order surrogate that uses the
sampling state distribution \(d_\mu\):
\[
  \widehat{\eta}(\pi)
  \;=\;
  \mathbb{E}_{s\sim d_\mu}\mathbb{E}_{a\sim\pi(\cdot|s)}\big[ A^\mu(s,a)\big].
\]

\paragraph{Constrained policy search (primal).}
We formulate a constrained optimisation that maximises the surrogate expected improvement while
keeping \(\pi\) close to \(\mu\) in KL averaged under \(d_\mu\):
\[
  \begin{aligned}
    &\max_{\pi} \quad
    \mathbb{E}_{s\sim d_\mu}\mathbb{E}_{a\sim\pi(\cdot|s)}\big[ R^\mu_{s,a} - V^\mu(s) \big],\\[4pt]
    &\text{s.t.}\quad
    \mathbb{E}_{s\sim d_\mu}\big[ D_{\mathrm{KL}}\!\big(\pi(\cdot|s)\,\|\,\mu(\cdot|s)\big)\big] \le \varepsilon,
    \qquad \forall s:\ \int_a \pi(a|s)\,da = 1.
  \end{aligned}
\]

\paragraph{Lagrangian and stationarity.}
Introduce Lagrange multiplier \(\beta>0\) for the KL constraint and pointwise normaliser \(\alpha_s\).
The (softened) Lagrangian is
\[
  \mathcal{L}(\pi,\beta,\alpha)
  =
  \mathbb{E}_{s\sim d_\mu}\!\Big[
    \mathbb{E}_{a\sim\pi(\cdot|s)}\big[ R^\mu_{s,a}-V^\mu(s) \big]
  \Big]
  + \beta\Big(\varepsilon - \mathbb{E}_{s\sim d_\mu}\!\big[ D_{\mathrm{KL}}(\pi(\cdot|s)\|\mu(\cdot|s))\big]\Big)
  + \mathbb{E}_{s\sim d_\mu}\!\big[ \alpha_s(1-\!\!\int_a\pi(a|s)\,da)\big].
\]
Differentiate \(\mathcal{L}\) w.r.t. \(\pi(a|s)\), set derivative to zero and solve for \(\pi\). Rearranging yields the
Boltzmann-tilted closed form for the optimal (per-state) conditional distribution:
\[
  \pi^\star(a|s)
  \;=\;
  \frac{1}{Z(s)}\;
  \mu(a|s)\;
  \exp\!\Big(\frac{1}{\beta}\big(R^\mu_{s,a}-V^\mu(s)\big)\Big),
  \qquad
  Z(s)\;=\;\int \mu(a'|s)\exp\!\Big(\tfrac{1}{\beta}\big(R^\mu_{s,a'}-V^\mu(s)\big)\Big)\,da'.
  \label{eq:awr_opt_policy}
\]

This formula is obtained by solving the stationarity condition
\(\partial\mathcal{L}/\partial\pi(a|s)=0\) and enforcing normalisation; it is the soft, advantage-weighted
reweighting of the behaviour distribution \(\mu(a|s)\).

\paragraph{Projection to parameterised policy (regression step).}
If \(\pi\) is parameterised (e.g. neural network) we cannot set it equal to \(\pi^\star\) pointwise. Instead
project \(\pi^\star\) onto the parameterised family by minimising expected KL:
\[
  \pi_{k+1} \;=\; \arg\min_{\pi\in\Pi}\;
  \mathbb{E}_{s\sim d_\mu}\big[ D_{\mathrm{KL}}\big(\pi^\star(\cdot|s)\,\|\,\pi(\cdot|s)\big)\big].
\]
Expanding the KL and substituting \(\pi^\star\) yields an equivalent supervised regression objective:
\[
  \pi_{k+1}
  \;=\;
  \arg\max_{\pi\in\Pi}\;
  \mathbb{E}_{s\sim d_\mu}\mathbb{E}_{a\sim\mu(\cdot|s)}
  \Big[
    \log\pi(a|s)\;
    \exp\!\Big(\frac{1}{\beta}\big(R^\mu_{s,a}-V^\mu(s)\big)\Big)
  \Big].
  \label{eq:awr_regression}
\]
Fit \(\pi\) by maximum likelihood on behaviour data weighted by exponentiated advantages.

\paragraph{Value update.}
AWR alternates the policy update with a value regression that fits \(V\) to returns in \(\mathcal{D}\),
typically by minimising a squared TD($\lambda$) or Monte-Carlo return loss:
\[
  V^{\mathcal{D}} \;=\; \arg\min_{V}\;
  \mathbb{E}_{(s,a)\sim\mathcal{D}}\big[\,\big(R^{\mathcal{D}}_{s,a} - V(s)\big)^2\big].
  \label{eq:awr_value_update}
\]

\section{DAR: Direct Advantage Regression}

DAR \parencite{he2025directadvantageregressionaligning} studies online alignment with AI-generated reward signals. The method preserves the regularised online RLHF objective while replacing iterative policy-gradient updates with a closed-form advantage-weighted target followed by supervised projection.

%-------------------------------------------------------------

\paragraph{Advantage Weighted Regression}

\begin{equation*}
  J_{\mathrm{AWR}}(\pi_\theta)
  =
  \max_{\pi_\theta}
  \mathbb{E}_{x \sim d_{\pi_\theta}(x), y \sim \pi_\theta(y|x)}
  \left[
    A(x,y)
  \right], A(x,y) = r(x,y) - V^{\pi_t}(x).
\end{equation*}

To remove dependence on $d_{\pi_\theta}$, we approximate using $d_{\pi_t}$ and impose KL trust-region regularization:

\begin{equation*}
  J_{\mathrm{AWR}}(\pi_\theta; \pi_t)
  =
  \max_{\pi_\theta}
  \mathbb{E}_{x \sim d_{\pi_t}(x), y \sim \pi_\theta(y|x)}
  \left[
    A(x,y)
  \right]
  -
  \beta
  D_{\mathrm{KL}}
  \left(
    \pi_\theta(y|x)
    \;\|\;
    \pi_t(y|x)
  \right).
\end{equation*}

%-------------------------------------------------------------

\paragraph{Dual-Constrained Objective}

DAR incorporates reference regularization:

\begin{equation*}
  J_{\mathrm{DAR}}(\pi_\theta; \pi_{\mathrm{ref}}, \pi_t)
  =
  \max_{\pi_\theta}
  \mathbb{E}_{x \sim d_{\pi_t}(x), y \sim \pi_\theta(y|x)}
  \left[
    A(x,y)
  \right]
  -
  \alpha D_{\mathrm{KL}}(\pi_\theta \| \pi_{\mathrm{ref}})
  -
  \beta D_{\mathrm{KL}}(\pi_\theta \| \pi_t).
\end{equation*}

\paragraph{Theorem.}
Under mild assumptions, for the dual-constrained advantage (or reward)
maximization objective above with strictly positive KL coefficients, the
optimal policy is:

\begin{equation*}
  \phi(x,y)
  \;\coloneqq\;
  \pi_{\mathrm{ref}}(y \mid x)^{\frac{\alpha}{\alpha+\beta}}
  \;
  \pi_t(y \mid x)^{\frac{\beta}{\alpha+\beta}}
  \;
  \exp\!\left(
    \frac{A(x,y)}{\alpha+\beta}
  \right),
  \pi^\star(y\mid x)
  =
  \frac{1}{Z(x)}\,
  \phi(x,y),
\end{equation*}
where $Z(x) = \sum_{y}\phi(x,y)$, the partition function.

\paragraph{Proof.}

\begingroup
\allowdisplaybreaks
\setlength{\jot}{2pt}
\begin{dmath*}
  \max_{\pi}\mathbb{E}_{x,y\sim\pi}\big[ A(x,y)\big]
  \;-\;\alpha D_{\mathrm{KL}}\big[\pi(y|x)\,\|\,\pi_{\mathrm{ref}}(y|x)\big]
  \;-\;\beta D_{\mathrm{KL}}\big[\pi(y|x)\,\|\,\pi_t(y|x)\big] \\
  = \min_{\pi}\mathbb{E}_{x,y\sim\pi}\Big[\alpha\log\frac{\pi(y|x)}{\pi_{\mathrm{ref}}(y|x)}
  +\beta\log\frac{\pi(y|x)}{\pi_t(y|x)}-A(x,y)\Big] \\
  = \min_{\pi}\mathbb{E}_{x,y\sim\pi}\Big[(\alpha+\beta)\log\pi(y|x)
  -\alpha\log\pi_{\mathrm{ref}}(y|x)-\beta\log\pi_t(y|x)-A(x,y)\Big] \\
  = \min_{\pi}\mathbb{E}_{x,y\sim\pi}\Big[\log\pi(y|x)
    -\log\pi_{\mathrm{ref}}(y|x)^{\frac{\alpha}{\alpha+\beta}}
    -\log\pi_t(y|x)^{\frac{\beta}{\alpha+\beta}}
  -\frac{1}{\alpha+\beta}A(x,y)\Big] \\
  = \min_{\pi}\mathbb{E}_{x,y\sim\pi}\Big[\log\frac{\pi(y|x)}
    {\pi_{\mathrm{ref}}(y|x)^{\frac{\alpha}{\alpha+\beta}}\,
    \pi_t(y|x)^{\frac{\beta}{\alpha+\beta}}}
  -\frac{1}{\alpha+\beta}A(x,y)\Big] \\
  = \min_{\pi}\mathbb{E}_{x,y\sim\pi}\Big[\log\frac{\pi(y|x)}
    {\frac{1}{Z(x)}\pi_{\mathrm{ref}}(y|x)^{\frac{\alpha}{\alpha+\beta}}
      \pi_t(y|x)^{\frac{\beta}{\alpha+\beta}}
  \exp\!\big(\frac{1}{\alpha+\beta}A(x,y)\big)}-\log Z(x)\Big].
\end{dmath*}

Because the partition function does not depend on $\pi$, $\log Z(x)$ is constant with respect to the optimisation variable and can be dropped:
\begin{dmath*}
  \min_{\pi}\mathbb{E}_{x,y\sim\pi}\Big[\log\frac{\pi(y|x)}
    {\frac{1}{Z(x)}\pi_{\mathrm{ref}}(y|x)^{\frac{\alpha}{\alpha+\beta}}
      \pi_t(y|x)^{\frac{\beta}{\alpha+\beta}}
  \exp\!\big(\frac{1}{\alpha+\beta}A(x,y)\big)}\Big] \\
  = \min_{\pi}\mathbb{E}_x\;D_{\mathrm{KL}}\!\Bigg[
    \pi(y|x)\;\Bigg\|\;
    \frac{1}{Z(x)}\pi_{\mathrm{ref}}(y|x)^{\frac{\alpha}{\alpha+\beta}}
    \pi_t(y|x)^{\frac{\beta}{\alpha+\beta}}
  \exp\!\big(\tfrac{1}{\alpha+\beta}A(x,y)\big)\Bigg].
\end{dmath*}
\endgroup

By Gibbs' inequality, the KL term is minimised when the two distributions are identical:
\begin{equation}
  \pi^\star(y|x)
  \;=\;
  \frac{1}{Z(x)}\,
  \pi_{\mathrm{ref}}(y|x)^{\frac{\alpha}{\alpha+\beta}}
  \pi_t(y|x)^{\frac{\beta}{\alpha+\beta}}
  \exp\!\Big(\frac{1}{\alpha+\beta}A(x,y)\Big).
\end{equation}

The improved parametric policy is then obtained by minimising the KL divergence to the target:

\[
  \min_{\pi_\theta}\mathbb{E}_{x\sim d_{\pi_t}(x)} D_{\mathrm{KL}}\!\big[\pi^\star(\cdot\mid x)\;\|\;\pi_\theta(\cdot\mid x)\big],
\]
Substituting $\pi^\star$ gives:
\[
  \min_{\pi_\theta}\mathbb{E}_{x\sim d_{\pi_t}(x)} D_{\mathrm{KL}}\!\Bigg[
    \frac{1}{Z(x)}\pi_{\mathrm{ref}}(y\mid x)^{\frac{\alpha}{\alpha+\beta}}
    \pi_t(y\mid x)^{\frac{\beta}{\alpha+\beta}}
    \exp\!\Big(\frac{1}{\alpha+\beta}A(x,y)\Big)
    \;\Big\|\;
  \pi_\theta(\cdot\mid x)\Bigg],
\]
Expanding the KL divergence and dropping terms independent of $\pi_\theta$ yields:
\[
  \min_{\pi_\theta}\mathbb{E}_{x\sim d_{\pi_t}(x)}\Bigg[-\sum_y
    \frac{1}{Z(x)}\,
    \pi_{\mathrm{ref}}(y\mid x)^{\frac{\alpha}{\alpha+\beta}}
    \pi_t(y\mid x)^{\frac{\beta}{\alpha+\beta}}
    \exp\!\Big(\frac{1}{\alpha+\beta}A(x,y)\Big)\,
  \log\pi_\theta(y\mid x)\Bigg],
\]
Since $Z(x)$ is a positive constant with respect to $\pi_\theta$, it can be removed without changing the optimum:
\[
  \min_{\pi_\theta}\mathbb{E}_{x\sim d_{\pi_t}(x)}\Big[-\sum_y
    \pi_{\mathrm{ref}}(y\mid x)^{\frac{\alpha}{\alpha+\beta}}
    \pi_t(y\mid x)^{\frac{\beta}{\alpha+\beta}}
    \exp\!\Big(\frac{1}{\alpha+\beta}A(x,y)\Big)\,
  \log\pi_\theta(y\mid x)\Big],
\]
Using $\pi_t$ as the sampling policy gives the final practical objective:
\[
  \max_{\pi_\theta}\mathbb{E}_{x\sim d_{\pi_t}(x)}\mathbb{E}_{y\sim\pi_t(y\mid x)}
  \left[\left(\frac{\pi_{\mathrm{ref}}(y\mid x)}{\pi_t(y\mid x)}\right)^{\frac{\alpha}{\alpha+\beta}}
    \exp\!\Big(\frac{1}{\alpha+\beta}A(x,y)\Big)\,
  \log\pi_\theta(y\mid x)\right].
\]


\section{MaxMin-RLHF}
\label{sec:maxmin_method}

MaxMin-RLHF \parencite{chakraborty2024maxminrlhfalignmentdiversehuman} can be interpreted as a nested Expectation--Maximisation procedure for alignment under heterogeneous preferences. An EM algorithm learns a mixture of subgroup-specific reward models from pairwise preference data by treating subgroup identity as a latent variable and alternating between estimating subgroup responsibilities (E-step) and maximising subgroup-specific reward likelihoods (M-step). Policy optimisation proceeds via a KL-regularised EM update, but with the reward model selected adversarially: at each iteration the subgroup whose expected utility is currently minimal under the policy is identified, and the policy is updated through an exponential-tilted target distribution derived from that subgroup's reward, followed by a projection back to the parametric policy family.

\paragraph{Method.}

Let $\mathcal{V}$ be the token vocabulary and $\mathcal{X}$ the space of prompts. A
language model (policy) is $\pi_\theta(\cdot\mid x)$ parameterised by $\theta$ and, given a prompt
$x\in\mathcal{X}$, produces a response sequence $y\sim\pi_\theta(\cdot\mid x)$.

Preference data are pairwise comparisons $(x,y_1,y_2)$ where a human annotator prefers $y_1$ to $y_2$.
Under the Bradley--Terry (BT) parametrisation the preference probability induced by a latent reward
$r^\star(y,x)$ is
\begin{equation}
  p^\star(y_1 \succ y_2 \mid x)
  \;=\;
  \sigma\!\big(r^\star(y_1,x)-r^\star(y_2,x)\big)
  \;=\;
  \frac{\exp(r^\star(y_1,x))}{\exp(r^\star(y_1,x))+\exp(r^\star(y_2,x))},
  \label{eq:bt}
\end{equation}
where $\sigma$ is the logistic function. A parametric reward model is $r_\phi(y,x)$, trained by
binary cross-entropy (negative log-likelihood)
\[
  \mathcal{L}_R(\phi; \mathcal{D})
  =
  -\mathbb{E}_{(x,y_1,y_2)\sim\mathcal{D}}
  \Big[
    \log\sigma\big(r_\phi(y_1,x)-r_\phi(y_2,x)\big)
  \Big].
\]

The KL-regularised reinforcement objective used for policy fine-tuning is
\begin{equation}
  \mathcal{J}(\pi; r)
  =
  \mathbb{E}_{x\sim\mathcal{P},\,y\sim\pi(\cdot\mid x)}
  \big[ r(y,x) \big]
  - \beta\, \mathbb{E}_{x\sim\mathcal{P}}
  \big[ D_{\mathrm{KL}}(\pi(\cdot\mid x)\,\|\,\pi_{\mathrm{ref}}(\cdot\mid x))\big],
  \label{eq:kl_reg_obj}
\end{equation}
with regularisation weight $\beta>0$ and reference policy $\pi_{\mathrm{ref}}$ (SFT checkpoint). 

\paragraph{Diversity and an impossibility bound. }
Define a population composed of $|U|$ sub-populations $H=\bigcup_{u=1}^{|U|} H_u$.
Let $p_u^\star$ denote the preference distribution for sub-population $H_u$.
Diversity between two sub-populations is measured by total variation
\[
  \mathrm{Diversity}(i,j)\coloneqq \mathrm{TV}(p_i^\star,p_j^\star).
\]
The paper proves (Theorem 1) that, under reasonable assumptions (linear reward parametrisation,
bounded features, Lipschitz mapping from reward to policy) the alignment gap for single-reward RLHF
admits a lower bound proportional to the population diversity and inversely proportional to model
regularity constants. In one scalar form (adapted notation) the lower bound reads
\begin{equation}
  \mathrm{Align\!-\!Gap} \ \gtrsim\ \frac{\lambda_\Psi}{\beta^2 L_\pi}\; \frac{\epsilon(1-\eta(u))}{D^2},
  \label{eq:align_gap}
\end{equation}
where $\lambda_\Psi$ is the minimum eigenvalue of the empirical feature matrix, $L_\pi$ a Lipschitz
constant relating policy and reward, $\beta$ the KL weight, $D$ a feature bound, $\eta(u)$ the mixing
weight of subgroup $u$, and $\epsilon$ a gap derived from pairwise total-variation distances. The
consequence is that high subpopulation diversity or vanishing subgroup mass $\eta(u)$ make accurate
alignment via a single reward improbable. See the original proof and the supporting lemma for the
precise constants and derivation.

To address this failure mode, we learn a mixture of $|U|$ reward models
$\{r_{\phi_u}\}_{u=1}^{|U|}$ and simultaneously clusters annotators (or annotator-derived signals)
so that each component models a coherent sub-population preference. Algorithmically this is solved
with a standard hard/soft EM procedure. We reproduce the key steps and the small algebraic derivation
that yields the E-step responsibilities.

\paragraph{Generative model for pairwise labels.}
For a fixed subpopulation index $u$, assume the pairwise preference likelihood under reward
$r_{\phi_u}$ follows \eqref{eq:bt}. For an observed labelled pair $(x,y_1,y_2)$ where $y_1$ is preferred,
the likelihood is
\[
  p\big(z=(y_1\succ y_2)\mid u,\phi_u,x\big)
  \;=\;
  \frac{\exp(r_{\phi_u}(y_1,x))}{\exp(r_{\phi_u}(y_1,x))+\exp(r_{\phi_u}(y_2,x))}.
\]
This can be re-written as a normalised exponential,
\[
  p(z\mid u,\phi_u,x)
  =
  \frac{\exp(r_{\phi_u}(y_1,x))}{\exp(r_{\phi_u}(y_1,x))+\exp(r_{\phi_u}(y_2,x))}
  \;=\;
  \frac{\exp(r_{\phi_u}(y_1,x))}{\sum_{j\in\{1,2\}}\exp(r_{\phi_u}(y_j,x))}.
\]

\paragraph{E-step (responsibilities).}
Given a current parameter set $\{\phi_u\}$ and prior cluster weights, the posterior responsibility
(or soft assignment) of cluster $u$ for datum $(x,y_1,y_2)$ is proportional to the product of the
cluster prior and the pairwise likelihood. Under equal priors (or when priors are absorbed in normalisation)
the local responsibility used in hard-assignment E-step reduces to the BT soft-likelihood:
\begin{equation}
  w(\phi_u; x,y_1,y_2)
  \;=\;
  \frac{\exp(r_{\phi_u}(y_1,x))}
  {\exp(r_{\phi_u}(y_1,x))+\exp(r_{\phi_u}(y_2,x))}.
  \label{eq:em_weight}
\end{equation}
This expression follows directly from the BT model and is the quantity used to assign users to clusters
(Algorithm~\ref{alg:em_reward}).

\paragraph{M-step (reward update).}
Given cluster assignments (hard or soft), update each reward parameter $\phi_u$ by minimising the
negative log-likelihood restricted to the data assigned to cluster $u$, i.e.
\[
  \phi_u \leftarrow \arg\min_\phi\;
  -\sum_{(x,y_1,y_2)\in \mathcal{D}_u}
  \log \sigma\big(r_\phi(y_1,x) - r_\phi(y_2,x)\big),
\]
where $\mathcal{D}_u$ are the pairs assigned to cluster $u$. In practice this is SGD on
the binary cross-entropy loss. Convergence of the EM loop is monitored by cluster-stability or
likelihood improvement. 

\paragraph{Objective and policy iteration. }
Once a set of reward models $\{r_{\phi_u}\}$ is obtained, the alignment objective pursued is an
Egalitarian (max--min) social-utility objective:
\begin{equation}
  \pi^\star_{\mathrm{MM}}
  \;=\;
  \arg\max_{\pi}\ \min_{u\in U}\;
  \Big\{ F_{r_{\phi_u}}(\pi) - \beta\,\mathbb{E}_x\big[ D_{\mathrm{KL}}(\pi(\cdot\mid x)\,\|\,\pi_{\mathrm{ref}}(\cdot\mid x))\big] \Big\},
  \label{eq:maxmin_obj}
\end{equation}
with $F_{r}(\pi) := \mathbb{E}_{x\sim\mathcal{P},\,y\sim\pi(\cdot\mid x)}[r(y,x)]$ the expected return.
Optimisation is performed by iterated policy improvement that (1) selects the worst-performing
sub-population under the current policy and (2) updates the policy using a standard RL fine-tuner
(e.g. PPO) toward improving that subgroup's objective. Concretely the paper proposes:

\begin{enumerate}
  \item Compute $u_{\min} \leftarrow \arg\min_{u\in U} F_{r_{\phi_u}}(\pi_t)$.
  \item Perform a policy update (PPO) to maximise $F_{r_{\phi_{u_{\min}}}}(\pi) - \beta\,\mathrm{KL}(\pi_t\|\pi_{\mathrm{ref}})$,
    producing $\pi_{t+1}$.
  \item Repeat until convergence.
\end{enumerate}

This iterated max--min policy iteration is summarised in Algorithm~\ref{alg:maxmin} below. 

\paragraph{Algorithm 1: MaxMin-RLHF (policy-level)}\label{alg:maxmin}
\begin{enumerate}
  \item \textbf{Input:} preference dataset $\mathcal{D}$, initial reward parameters $\{\phi_{u}^{(0)}\}_{u=1}^{|U|}$ (or pretraining),
    initial policy $\pi_0$, KL weight $\beta$, PPO (or other RL) trainer.
  \item \textbf{Reward learning:} run Algorithm~\ref{alg:em_reward} to estimate $\{\phi_u\}$ (mixture learning via EM).
  \item \textbf{For} $t=0,\dots,T-1$:
    \begin{enumerate}
      \item Evaluate subgroup utilities $F_{r_{\phi_u}}(\pi_t)$ for all $u\in U$ (via rollouts or importance-weighted evaluation).
      \item Select $u_{\min} \leftarrow \arg\min_u F_{r_{\phi_u}}(\pi_t)$.
      \item Update policy using PPO (or another KL-regularised optimiser) towards improving subgroup $u_{\min}$:
        \[
          \pi_{t+1} \leftarrow \text{PPO\_step}\Big(\pi_t,\; r_{\phi_{u_{\min}}},\; \beta,\; \pi_{\mathrm{ref}}\Big).
        \]
    \end{enumerate}
  \item \textbf{Output:} $\pi_T$ aligned to the max-min objective.
\end{enumerate}

\paragraph{Algorithm 2: Learning rewards with EM (clustered reward models)}\label{alg:em_reward}
\begin{enumerate}
  \item \textbf{Input:} pairwise preference dataset $\mathcal{D}$, number of clusters $|U|$, initial $\{\phi_u\}$.
  \item \textbf{Repeat until convergence:}
    \begin{enumerate}
      \item \textbf{E-step (hard or soft):} for each annotator (or datum) compute responsibilities
        $w(\phi_u; x,y_1,y_2)$ as in \eqref{eq:em_weight} and assign datum to the cluster with largest responsibility (hard assignment) or keep soft weights.
      \item \textbf{M-step:} for each $u$ update $\phi_u$ by minimising the binary cross-entropy over data assigned to $u$:
        \[
          \phi_u \leftarrow \arg\min_\phi -\sum_{(x,y_1,y_2)\in\mathcal{D}_u}
          \log\sigma\big(r_\phi(y_1,x)-r_\phi(y_2,x)\big).
        \]
    \end{enumerate}
  \item \textbf{Return:} $\{\phi_u\}$ and cluster assignments.
\end{enumerate}

\section{VIREL: A Variational Inference Framework for RL}
\label{sec:virel}

VIREL \parencite{fellows2020virel} formulates reinforcement learning as variational inference with an explicit EM decomposition. The paper defines state-action pairs as $h=(s,a)$, a critic $\hat Q_\omega(h)$, and a residual temperature $\varepsilon_\omega$:
\begin{equation}
  \varepsilon_\omega
  \coloneqq
  c\left\lVert T_\omega \hat Q_\omega(h)-\hat Q_\omega(h)\right\rVert_p^p,
  \qquad
  T_\omega = T^{\pi_\omega}\!\cdot
  \coloneqq
  r(h)+\gamma\mathbb{E}_{h'\sim p(s'|h)\pi_\omega(a'|s')}[\cdot].
  \label{eq:virel_eps}
\end{equation}
The induced Boltzmann policy (their Eq.~(3)) is
\begin{equation}
  \pi_\omega(a|s)
  \coloneqq
  \frac{\exp\!\big(\hat Q_\omega(h)/\varepsilon_\omega\big)}
  {\int_{\mathcal{A}}\exp\!\big(\hat Q_\omega(h')/\varepsilon_\omega\big)\,da'},
  \label{eq:virel_boltzmann}
\end{equation}
where $h'=(s,a')$.

\paragraph{ELBO objective.}
VIREL maximises the ELBO (their Eq.~(4)):
\begin{equation}
  L(\omega,\theta)
  \coloneqq
  \mathbb{E}_{s\sim d(s)}\mathbb{E}_{a\sim\pi_\theta(a|s)}
  \left[
    \frac{\hat Q_\omega(h)}{\varepsilon_\omega}
  \right]
  +
  \mathcal{H}\!\big(\pi_\theta(a|s)\big),
  \label{eq:virel_elbo}
\end{equation}
with $q_\theta(h)\coloneqq d(s)\pi_\theta(a|s)$. They rewrite (Eq.~(5)):
\begin{equation}
  L(\omega,\theta)
  =
  \ell(\omega)
  -
  \mathrm{KL}\!\big(q_\theta(h)\,\|\,p_\omega(h)\big)
  -
  \mathcal{H}(d(s)),
  \label{eq:virel_decomp}
\end{equation}
where
\begin{equation}
  \ell(\omega)
  \coloneqq
  \log\!\int_{\mathcal{H}}
  \exp\!\left(\frac{\hat Q_\omega(h)}{\varepsilon_\omega}\right)\,dh,
  \qquad
  p_\omega(h)
  \coloneqq
  \frac{\exp\!\big(\hat Q_\omega(h)/\varepsilon_\omega\big)}
  {\int_{\mathcal{H}}\exp\!\big(\hat Q_\omega(h)/\varepsilon_\omega\big)\,dh}.
  \label{eq:virel_l_p}
\end{equation}

\paragraph{Variational EM updates (actor-critic as EM).}
The E-step updates actor parameters by maximising $L(\omega_k,\theta)$:
\begin{equation}
  \theta_{k+1}
  \leftarrow
  \arg\max_\theta L(\omega_k,\theta),
  \quad
  \theta_{i+1}
  \leftarrow
  \theta_i + \alpha_{\mathrm{actor}}
  \big(\varepsilon_{\omega_k}\nabla_\theta L(\omega_k,\theta)\big)\big|_{\theta=\theta_i},
  \label{eq:virel_estep_update}
\end{equation}
with gradient (their Eq.~(6))
\begin{equation}
  \varepsilon_{\omega_k}\nabla_\theta L(\omega_k,\theta)
  =
  \mathbb{E}_{s\sim d(s),\,a\sim\pi_\theta(a|s)}
  \Big[
    \hat Q_{\omega_k}(h)\nabla_\theta\log\pi_\theta(a|s)
    +
    \varepsilon_{\omega_k}\nabla_\theta\mathcal{H}\!\big(\pi_\theta(a|s)\big)
  \Big].
  \label{eq:virel_estep_grad}
\end{equation}

The M-step updates critic parameters by maximising $L(\omega,\theta_{k+1})$:
\begin{equation}
  \omega_{k+1}
  \leftarrow
  \arg\max_\omega L(\omega,\theta_{k+1}),
  \quad
  \omega_{i+1}
  \leftarrow
  \omega_i
  +
  \alpha_{\mathrm{critic}}(\varepsilon_{\omega_i})^2
  \nabla_\omega L(\omega,\theta_{k+1})\big|_{\omega=\omega_i},
  \label{eq:virel_mstep_update}
\end{equation}
with gradient (their Eq.~(7))
\begin{equation}
  (\varepsilon_{\omega_i})^2\nabla_\omega L(\omega,\theta_{k+1})
  =
  \varepsilon_{\omega_i}
  \mathbb{E}_{d(s)\pi_{\theta_{k+1}}(a|s)}
  \big[\nabla_\omega \hat Q_\omega(h)\big]
  -
  \mathbb{E}_{d(s)\pi_{\theta_{k+1}}(a|s)}
  \big[\hat Q_{\omega_i}(h)\nabla_\omega \varepsilon_\omega\big].
  \label{eq:virel_mstep_grad}
\end{equation}


\section{Generalized EM Policy Improvement (GEMPI)}
This is an attempt to describe a generalised EM policy improvement method that encompasses a wide range of algorithms as special cases. The key idea is to allow multiple anchor policies and multiple divergence functionals in the E-step, as well as flexible temperature and filtering mechanisms.

\paragraph{The GEMPI tuple.}
A Generalized EM Policy Improvement method is specified by
\[
  \mathcal{G}
  =
  \big(
    \{D_j\}_{j=1}^m,\;
    \{\pi_j\}_{j=1}^m,\;
    \{\lambda_j\}_{j=1}^m,\;
    \mathcal{T},\;
    \mathcal{F},\;
    D_M,\;
    \varepsilon_M
  \big),
\]
where $\{D_j\}$ are divergence functionals used in the E-step, $\{\pi_j\}$ are anchor policies, $\{\lambda_j > 0\}$ are regularization coefficients, $\mathcal{T}$ is the temperature mechanism (fixed, dual-adaptive, or closed-form), $\mathcal{F}$ is a filtering mechanism (identity or top-$k$), and $D_M$ with budget $\varepsilon_M$ is an optional M-step trust-region divergence.

\subsection{Relation to VIREL}

\paragraph{Overlap (E-step structure).}
At the policy-improvement level, GEMPI and VIREL share the same exponential-tilting form.
VIREL uses a Boltzmann target \eqref{eq:virel_boltzmann} / \eqref{eq:virel_l_p}:
\[
  p_\omega(y\mid x)
  \propto
  \exp\!\Big(\frac{\hat Q_\omega(x,y)}{\varepsilon_\omega}\Big).
\]
GEMPI's KL E-step gives
\[
  q^\star(y\mid x)
  \propto
  \pi_1(y\mid x)\exp\!\Big(\frac{A(x,y)}{\lambda_1}\Big)
  \quad (m=1).
\]
Choosing a uniform anchor $\pi_1$ (or absorbing $\log\pi_1$ into $A$), setting
$A(x,y)=\hat Q_\omega(x,y)$, and $\lambda_1=\varepsilon_\omega$ recovers the same Boltzmann family.
So GEMPI subsumes VIREL's \emph{policy-improvement form} as a special case.

\paragraph{What GEMPI can express that VIREL (as originally formulated) does not.}
\begin{itemize}
  \item \textbf{Multi-anchor improvement:} GEMPI supports $m>1$ anchors (e.g.\ $\pi_{\mathrm{ref}}$ and $\pi_t$ simultaneously), yielding geometric mixtures such as DAR's dual-anchor target.
  \item \textbf{Divergence/filtering modularity:} GEMPI makes divergence choice and filtering ($\mathcal{F}$, including top-$k$) first-class knobs in one template.
  \item \textbf{Collapsed single-stage variants:} GEMPI naturally includes collapsed E-step constructions (e.g.\ DPO-style elimination), while VIREL is explicitly a two-stage variational EM loop.
\end{itemize}

\paragraph{What VIREL can do that base GEMPI does not encode.}
\begin{itemize}
  \item \textbf{Critic-learning M-step:} VIREL's M-step updates value parameters $\omega$ via \eqref{eq:virel_mstep_update}--\eqref{eq:virel_mstep_grad}. GEMPI's M-step \eqref{eq:gempi_mstep} is a policy projection and does not by itself specify Bellman-style critic learning.
  \item \textbf{Residual-coupled adaptive exploration:} VIREL ties temperature to Bellman residual $\varepsilon_\omega$ in \eqref{eq:virel_eps}, linking exploration strength to critic error. GEMPI temperatures/regularizers are generic coefficients or duals, without this specific coupling unless added explicitly.
  \item \textbf{VIREL-specific optimality guarantees:} VIREL provides ELBO-to-policy optimality and deterministic-limit results. GEMPI is a unifying design template and does not, on its own, imply those guarantees.
\end{itemize}

GEMPI is strictly broader as a \emph{policy-improvement parameterisation}; VIREL is richer as a \emph{full actor-critic variational inference framework}. They are therefore complementary: GEMPI generalises the E-step design space, while VIREL contributes a specific critic-coupled semantics and theory.

% ============================================================
\subsection{General Regularized E-Step}

The E-step constructs a non-parametric target distribution $q^\star(\cdot\mid x)$ by solving a regularized advantage maximization over $m$ anchor policies:
\begin{equation}\label{eq:gempi_estep}
  q^\star
  =
  \arg\max_{q}
  \bigg\{
    \mathbb{E}_{y\sim q}\!\big[A(x,y)\big]
    -
    \sum_{j=1}^{m} \lambda_j\, D_j\!\big(q(\cdot\mid x)\,\big\|\,\pi_j(\cdot\mid x)\big)
  \bigg\},
\end{equation}
subject to $q$ being a valid distribution. The advantage $A(x,y)$ may be sequence-level (reward minus baseline) or token-level, and the divergences $D_j$ penalize deviation from each anchor $\pi_j$.

\paragraph{Theorem (Multi-KL Closed Form).}
When all divergences are KL, i.e.\ $D_j = \mathrm{KL}$ for $j=1,\dots,m$, the solution to \eqref{eq:gempi_estep} is
\begin{equation}\label{eq:gempi_closedform}
  q^\star(y\mid x)
  =
  \frac{1}{Z(x)}
  \prod_{j=1}^{m}
  \pi_j(y\mid x)^{\lambda_j/\Lambda}
  \;\exp\!\Big(\frac{A(x,y)}{\Lambda}\Big),
\end{equation}
where $\Lambda \coloneqq \sum_{j=1}^{m}\lambda_j$ is the effective temperature and $Z(x)$ is the partition function ensuring normalization.

\paragraph{Proof.}
Write the Lagrangian with multiplier $\mu$ for the normalization constraint:
\[
  \mathcal{J}(q,\mu)
  =
  \sum_y q(y\mid x)\, A(x,y)
  -
  \sum_{j=1}^{m} \lambda_j
  \sum_y q(y\mid x)\log\frac{q(y\mid x)}{\pi_j(y\mid x)}
  +
  \mu\Big(1 - \sum_y q(y\mid x)\Big).
\]
Expanding the KL terms and grouping:
\[
  \mathcal{J}
  =
  \sum_y q(y\mid x)
  \bigg[
    A(x,y)
    -
    \Lambda\log q(y\mid x)
    +
    \sum_{j=1}^{m}\lambda_j \log\pi_j(y\mid x)
  \bigg]
  +
  \mu\Big(1-\sum_y q(y\mid x)\Big).
\]
Taking the functional derivative with respect to $q(y\mid x)$ and setting to zero:
\[
  A(x,y)
  -
  \Lambda\big(\log q(y\mid x) + 1\big)
  +
  \sum_{j=1}^{m}\lambda_j\log\pi_j(y\mid x)
  -
  \mu
  =
  0.
\]
Solving for $\log q(y\mid x)$:
\[
  \log q(y\mid x)
  =
  \frac{1}{\Lambda}A(x,y)
  +
  \sum_{j=1}^{m}\frac{\lambda_j}{\Lambda}\log\pi_j(y\mid x)
  +
  \mathrm{const}.
\]
Exponentiating and normalizing yields \eqref{eq:gempi_closedform}.

\paragraph{Log-space form.}
The unnormalized log-weight for sample $(x,y)$ is
\begin{equation}\label{eq:gempi_logweight}
  \ell(x,y)
  =
  \sum_{j=1}^{m}
  \frac{\lambda_j}{\Lambda}\log\pi_j(y\mid x)
  +
  \frac{1}{\Lambda}A(x,y).
\end{equation}
The coefficients $\lambda_j/\Lambda$ sum to one, so the first term is a convex combination of the anchor log-policies---a geometric mixture---shifted by the scaled advantage. This reveals the E-step target as a geometry-aware interpolation between the anchors, tilted toward high-advantage sequences.

\paragraph{Multi-temperature dual.}
When the E-step is posed as a constrained problem---maximize expected advantage subject to $\mathrm{KL}(q\|\pi_j) < \varepsilon_j$ for each anchor---the Lagrangian yields the same functional form with each $\lambda_j$ replaced by an optimal dual variable $\lambda_j^\star$. The dual objective generalizes V-MPO's scalar temperature dual:
\begin{equation}\label{eq:gempi_dual}
  L_{\mathrm{dual}}(\lambda_1,\dots,\lambda_m)
  =
  \sum_{j=1}^{m}\lambda_j\,\varepsilon_j
  +
  \Lambda\,\log Z(x;\lambda_1,\dots,\lambda_m).
\end{equation}
This can be minimized by gradient descent, adapting all temperatures simultaneously.

% ============================================================
\subsection{General M-Step}

The M-step projects the non-parametric target $q^\star$ back to the parametric policy family:
\begin{equation}\label{eq:gempi_mstep}
  \theta_{k+1}
  =
  \arg\min_{\theta}
  \bigg\{
    -\mathbb{E}_{q^\star}\!\big[\log\pi_\theta(y\mid x)\big]
    +
    \gamma\,
    D_M\!\big(\pi_{\theta_k}(\cdot\mid x)\,\big\|\,\pi_\theta(\cdot\mid x)\big)
  \bigg\},
\end{equation}
where $\gamma \ge 0$ controls the M-step trust region. The three main variants are:

\begin{itemize}
  \item \textbf{Unconstrained} ($\gamma=0$): pure weighted maximum likelihood. Used by DAR and AWR.
  \item \textbf{Penalized} ($\gamma > 0$, $D_M = \mathrm{KL}$): soft KL penalty constraining the parametric policy.  V-MPO M-step uses this with $\gamma$ treated as a Lagrangian dual variable $\alpha$ optimized against a budget $\varepsilon_\alpha$.
  \item \textbf{Hard-constrained}: replace the penalty with a constraint $D_M(\pi_{\theta_k}\|\pi_\theta) < \varepsilon_M$ and solve via the Lagrangian, yielding the stop-gradient decomposition used in V-MPO's implementation.
\end{itemize}

\paragraph{Importance-weighted practical form.}
When samples are drawn from the sampling policy $\pi_t$ but $q^\star \neq \pi_t$, the M-step loss uses importance weights:
\[
  \mathcal{L}_\pi(\theta)
  =
  -\sum_{i\in S} w_i
  \sum_{t=1}^{T_i}
  \log\pi_\theta\!\big(y_t^{(i)}\mid y_{<t}^{(i)}, x^{(i)}\big),
\]
where $w_i \propto q^\star(y^{(i)}\mid x^{(i)})/\pi_t(y^{(i)}\mid x^{(i)})$. When $\pi_t$ is one of the anchors, the corresponding factor in the geometric mixture cancels partially, simplifying the weights.

% \subsection{Finite-Sample Approximation in the M-Step}

The closed-form target \eqref{eq:gempi_closedform} is a population-level object: the
partition function
\[
  Z(x) = \sum_y \prod_{j=1}^m \pi_j(y\mid x)^{\lambda_j/\Lambda}
  \exp\!\Big(\frac{A(x,y)}{\Lambda}\Big)
\]
sums over the entire output space, which is intractable for large vocabulary sequence
models. In practice, $Z(x)$ is estimated from a finite batch
$\mathcal{B} = \{y^{(i)}\}_{i=1}^N$ drawn from the sampling policy $\pi_t$, giving
\[
  \hat Z(x) = \frac{1}{N}\sum_{i=1}^N
  \frac{\prod_j \pi_j(y^{(i)}\mid x)^{\lambda_j/\Lambda}
  \exp\!\big(\frac{1}{\Lambda}A(x,y^{(i)})\big)}
  {\pi_t(y^{(i)}\mid x)},
\]
a self-normalised importance-weighted estimate. Three distinct approximation errors arise.

\paragraph{1. Partition function bias.}

$\hat Z(x)$ is a ratio estimator and is therefore biased. By the delta method,
\[
  \mathbb{E}[\hat Z(x)] = Z(x)
  \Big(1 + O\!\big(N^{-1}\big)\big),
\]
so the bias is $O(N^{-1})$ and vanishes as the batch grows. The self-normalised
importance weights
\[
  \hat w_i
  = \frac{
    \prod_j \pi_j(y^{(i)}\mid x)^{\lambda_j/\Lambda}
    \exp\!\big(\frac{1}{\Lambda}A(x,y^{(i)})\big)
    /\,\pi_t(y^{(i)}\mid x)
  }{
    \sum_{i'} \prod_j \pi_j(y^{(i')}\mid x)^{\lambda_j/\Lambda}
    \exp\!\big(\frac{1}{\Lambda}A(x,y^{(i')})\big)
    /\,\pi_t(y^{(i')}\mid x)
  }
\]
converge to $q^\star(y^{(i)}\mid x)/\pi_t(y^{(i)}\mid x)$ almost surely as $N\to\infty$
by the strong law, provided $q^\star \ll \pi_t$ (absolute continuity).

\paragraph{2. Effective sample size and weight degeneracy.}

The quality of the finite-sample approximation is governed by the effective sample size
\[
  \mathrm{ESS} = \frac{\big(\sum_i \hat w_i\big)^2}{\sum_i \hat w_i^2},
\]
which equals $N$ when all weights are equal and degrades toward $1$ when a single
sample dominates. The ESS is controlled by the mismatch between $q^\star$ and $\pi_t$,
quantified by their $\chi^2$-divergence:
\[
  \mathrm{ESS} \approx \frac{N}{1 + \chi^2(q^\star \| \pi_t)}.
\]
Within the GEMPI parameterisation, increasing $\Lambda$ (i.e.\ raising regularisation
coefficients or lowering the effective temperature) shrinks the advantage tilt, bringing
$q^\star$ closer to the geometric anchor mixture and hence to $\pi_t$. This directly
improves ESS, providing a formal justification for the practical observation that lower
temperature produces more stable weighted updates.

\paragraph{3. M-step gradient bias under finite ESS.}

The M-step objective \eqref{eq:gempi_mstep} evaluated with self-normalised weights is
\[
  \hat{\mathcal{L}}_\pi(\theta)
  =
  -\sum_{i\in S} \hat w_i
  \sum_{t=1}^{T_i}
  \log\pi_\theta\!\big(y_t^{(i)}\mid y_{<t}^{(i)},x^{(i)}\big).
\]
The gradient $\nabla_\theta \hat{\mathcal{L}}_\pi$ is a biased estimator of
$\nabla_\theta \mathbb{E}_{q^\star}[-\log\pi_\theta]$ because the self-normalised
weights themselves depend on the sample. The bias is of order $O(N^{-1})$ and has been
analysed in the self-normalised IS literature \parencite{Hesterberg1995,
mcbook}; it does not affect consistency but does affect the finite-sample
variance of the gradient.

\paragraph{Connection to GEMPI stabilisers.}


\begin{itemize}
  \item \textbf{Adaptive temperature} (dual optimisation of $\lambda_j$) minimises
    $\chi^2(q^\star \| \pi_t)$, maximising ESS and reducing partition function bias.
  \item \textbf{Top-$k$ filtering} replaces soft self-normalised weighting over all $N$
    samples with hard selection of the $k$ samples where $q^\star / \pi_t$ is largest.
    This acts as a variance-reduction strategy: by concentrating mass on the samples
    most likely under $q^\star$, it reduces the second moment of the importance weights
    at the cost of introducing a support truncation bias of order
    $O\big(\mathbb{E}_{q^\star}[q^\star/\pi_t \cdot \mathbf{1}_{i \notin S}]\big)$,
    which is small when the top-$k$ fraction $\rho$ is chosen so that the omitted tail
    of $q^\star$ is negligible.
  \item \textbf{Log-sum-exp normalisation} addresses floating-point
    representation of $\hat Z$ rather than its statistical properties, but is a
    necessary precondition for the estimates above to be numerically meaningful at
    large $|\!A|/\Lambda$.
\end{itemize}


\subsection{Recovering Existing Methods}


\medskip
\begin{center}
  \small
  \begin{tabular}{lccccccc}
    \toprule
    \textbf{Method}
    & $m$
    & \textbf{Anchors}
    & \textbf{Coefficients}
    & $\Lambda$
    & \textbf{Temperature}
    & \textbf{M-step $D_M$}
    & \textbf{Filtering} \\
    \midrule
    SFT
    & 0
    & ---
    & ---
    & ---
    & ---
    & None
    & --- \\
    AWR
    & 1
    & $\pi_t$
    & $\beta$
    & $\beta$
    & Fixed
    & None
    & None \\
    V-MPO
    & 1
    & $\pi_t$
    & $\eta$
    & $\eta$
    & Adaptive dual
    & KL ($\alpha$ dual)
    & Top-$k$ \\
    DAR
    & 2
    & $\pi_{\mathrm{ref}},\pi_t$
    & $\alpha,\beta$
    & $\alpha{+}\beta$
    & Fixed
    & None
    & None \\
    RL-EM
    & 1
    & $\pi_{\mathrm{ref}}$
    & $\beta$
    & $\beta$
    & Fixed
    & Optional KL
    & None \\
    \midrule
    DPO (collapsed)
    & 1
    & $\pi_{\mathrm{ref}}$
    & $\beta$
    & $\beta$
    & Fixed
    & None
    & --- \\
    \bottomrule
  \end{tabular}
\end{center}
\medskip

\paragraph{V-MPO.}
Set $m=1$, $D_1=\mathrm{KL}$, $\pi_1=\pi_t$. The closed form \eqref{eq:gempi_closedform} becomes
\[
  q^\star(y\mid x) \propto \pi_t(y\mid x)\,\exp\!\Big(\frac{A(x,y)}{\eta}\Big),
\]
with $\Lambda = \eta$. The temperature is treated as a dual variable optimised via the scalar dual \eqref{eq:gempi_dual} (which reduces to $\mathcal{L}_\eta = \eta\varepsilon_\eta + \eta\log(Z/k)$). Top-$k$ filtering restricts the support. The M-step adds a KL trust region with dual $\alpha$. This recovers the V-MPO formulation.

\paragraph{DAR.}
Set $m=2$, $D_1=D_2=\mathrm{KL}$, $\pi_1=\pi_{\mathrm{ref}}$ with coefficient $\alpha$, $\pi_2=\pi_t$ with coefficient $\beta$. The closed form \eqref{eq:gempi_closedform} becomes
\[
  q^\star(y\mid x)
  \propto
  \pi_{\mathrm{ref}}(y\mid x)^{\alpha/(\alpha+\beta)}\;
  \pi_t(y\mid x)^{\beta/(\alpha+\beta)}\;
  \exp\!\Big(\frac{A(x,y)}{\alpha+\beta}\Big),
\]
with $\Lambda = \alpha+\beta$. This is precisely the DAR optimal policy.

\paragraph{AWR.}
Set $m=1$, $D_1=\mathrm{KL}$, $\pi_1=\pi_t$, $\lambda_1=\beta$ fixed. No dual optimization, no filtering, unconstrained M-step. The target is $q^\star \propto \pi_t\exp(A/\beta)$, recovering standard AWR \parencite{peng2019advantageweightedregression}.

\paragraph{SFT.}
The degenerate case with no E-step optimization: $q(\tau\mid x) = \delta(\tau=y)$ places all mass on demonstrated responses, and the M-step is pure ML. This corresponds to $m=0$ in the GEMPI tuple.

\paragraph{DPO (collapsed).}
The KL-regularised RLHF objective \eqref{eq:dpo_rlhf} yields the same E-step closed form as GEMPI with $m=1$, $\pi_1=\pi_{\mathrm{ref}}$, $\Lambda=\beta$: namely $q^\star \propto \pi_{\mathrm{ref}}\exp(r/\beta)$. DPO does not construct this target explicitly. Instead, it substitutes the optimal policy form into the Bradley--Terry preference likelihood, and the partition function $Z(x)$ cancels in the reward difference. The result is a single-stage preference MLE \eqref{eq:dpo_loss} that implicitly solves both the E-step and M-step. DPO can thus be viewed as a degenerate GEMPI instantiation in which the E-step is analytically eliminated.

\paragraph{DAR$\to$V-MPO as corollary.}
The reduction derived in the DAR-to-V-MPO mapping above is now a direct consequence of the GEMPI parameterisation: collapsing two anchors to one ($\pi_{\mathrm{ref}}=\pi_t$) merges the geometric mixture powers $\pi_{\mathrm{ref}}^{\alpha/\Lambda}\pi_t^{\beta/\Lambda} = \pi_t^{1} = \pi_t$, recovering the single-anchor form with $\eta = \alpha+\beta$. Alternatively, sending $\alpha\to 0$ zeros out the $\pi_{\mathrm{ref}}$ contribution, yielding the same result.


% ============================================================
\subsection{Dropout Compatibility}
\label{sec:dropout_case}

Dropout \parencite{srivastava2014dropout} randomly zeros each hidden activation with probability $p$ during training, scaling survivors by $1/(1{-}p)$. Most large-scale pre-training runs disable it, but during fine-tuning the dataset is orders of magnitude smaller and overfitting is a real concern, making dropout relevant again. Yet all mainstream PPO implementations forgo dropout entirely. 


\paragraph{Why GEMPI methods are compatible.}
The GEMPI template guarantees dropout compatibility whenever the M-step uses detached (stop-gradient) weights from the E-step. The E-step weights depend on advantages and anchor log-probabilities, not on a ratio of two forward passes under different modes. The M-step is weighted teacher-forced cross-entropy---structurally identical to SFT with per-sequence weights---so dropout is compatible for the same reason it is compatible with any supervised cross-entropy objective. The M-step gradient with dropout active is
\[
  \nabla_\theta \mathcal{L}_\pi
  =
  -\sum_{i\in S} w_i \sum_t \nabla_\theta\log\pi_\theta^{(\mathrm{mask})}(y_t^{(i)}\mid\cdot\,),
\]
which is an unbiased estimator of the weighted-MLE gradient under the dropout distribution, exactly as in supervised learning. No eval-mode forward pass needs to be compared against this quantity.

% ============================================================
\subsection{Novel Instantiations}
\label{sec:gempi_novel}

The GEMPI framework, by making the axes of variation explicit, suggests several unexplored combinations.

\paragraph{Adaptive-Temperature DAR (AT-DAR).}
DAR with $m=2$ but treating both $\alpha$ and $\beta$ as dual variables with KL budgets $\varepsilon_\alpha$ and $\varepsilon_\beta$. The dual objective becomes
\[
  L_{\mathrm{dual}}(\alpha,\beta)
  =
  \alpha\,\varepsilon_\alpha + \beta\,\varepsilon_\beta
  +
  (\alpha+\beta)\log Z(x;\alpha,\beta),
\]
giving DAR the same adaptive stability as V-MPO while retaining the dual-anchor structure.

\paragraph{DAR with M-step Trust Region (DAR-TR).}
Adding a KL trust-region constraint to DAR's currently unconstrained M-step creates a method combining DAR's dual-anchor E-step with V-MPO's conservative parametric projection: the full GEMPI tuple with $m=2$, $D_1=D_2=\mathrm{KL}$, $D_M=\mathrm{KL}$ with budget $\varepsilon_M$.

\paragraph{Top-$k$ DAR.}
Applying V-MPO's top-$k$ filtering to the DAR E-step. Currently DAR uses all samples; restricting to the top-$k$ by advantage before computing the geometric mixture weights could improve sample efficiency by focusing the M-step on demonstrably high-quality sequences.

% ============================================================
\paragraph{Worked Instantiation: LLM V-MPO}
\label{sec:vmpo_llm_gempi}
For prompts $x \sim \mathcal{D}_x$, the policy $\pi_\theta(y\mid x)$ generates full responses $y=(y_1,\dots,y_T)$ and receives sequence-level rewards from a preference/reward model. We use sequence-level advantages
\[
  A^{(i)} = r^{(i)} - V_\phi(x^{(i)}),
\]
and perform EM-style policy improvement with V-MPO structure.

\paragraph{GEMPI tuple.}
The LLM variant corresponds to
\[
  \mathcal{G}_{\mathrm{LLM\text{-}V\!-\!MPO}}
  =
  \big(
    \{\mathrm{KL}\},
    \{\pi_t\},
    \{\eta\},
    \text{dual-adaptive},
    \text{top-}k,
    \mathrm{KL},
    \varepsilon_\alpha
  \big),
\]
that is, a single KL anchor $\pi_t$ in the E-step, dual adaptation of $\eta$, top-$k$ support filtering, and an optional KL trust region in the M-step.

\paragraph{Sequence-level E-step for transformer rollouts.}
Given a rollout batch
\[
  \mathcal{B}=\{(x^{(i)},y^{(i)},A^{(i)})\}_{i=1}^{N},
\]
and selected subset $S\subset\{1,\dots,N\}$, the non-parametric E-step target is
\begin{equation}
  \label{eq:seq_estep_weights}
  \psi(i) \propto \exp\!\Big(\frac{A^{(i)}}{\eta}\Big).
\end{equation}
After normalisation, these become per-sequence weights $w_i$ used by the M-step.

The temperature is adapted through the dual objective
\[
  \mathcal{L}_\eta(\eta)
  =
  \eta\,\varepsilon_\eta
  +
  \eta\log\!\Big(\frac{1}{k}\sum_{i\in S}\exp\!\Big(\frac{A^{(i)}}{\eta}\Big)\Big),
\]
with $k=|S|$. A numerically stable log-sum-exp form is
\begin{equation}
  \label{eq:seq_logsumexp}
  g(\eta)
  =
  m + \log\!\Big(\frac{1}{k}\sum_{i\in S}\exp(u_i-m)\Big),
  \qquad
  u_i=\frac{A^{(i)}}{\eta},\quad m=\max_{i\in S}u_i.
\end{equation}

In practice, $S$ is chosen as top-$k$ by detached scaled advantage:
\[
  u_i^{\mathrm{det}} = \frac{A^{(i)}}{\eta_{\mathrm{det}}}, \qquad
  k = \max\!\big(1,\lfloor \rho N\rfloor\big), \qquad
  S = \operatorname{Top-}k\!\left(u_i^{\mathrm{det}}\right).
\]

\paragraph{M-step as weighted teacher-forced transformer training.}
The parametric projection is weighted teacher-forced cross-entropy:
\begin{equation}
  \label{eq:seq_weighted_mstep}
  \mathcal{L}_\pi(\theta)
  =
  -\sum_{i\in S} w_i \sum_{t=1}^{T_i}
  \log \pi_\theta\!\big(y_t^{(i)} \mid y_{<t}^{(i)}, x^{(i)}\big).
\end{equation}

This keeps the update in standard transformer training form (teacher forcing over tokens) while importing RL information through detached sequence weights $w_i$. When desired, a KL trust-region term to $\pi_t$ is added exactly as in the V-MPO M-step.


\section{References}

\printbibliography[title={~}]

\appendix

\section{Method Comparison}
\label{sec:appendix_comparison}

\medskip
\noindent
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.4}
\begin{center}
  \scriptsize
  \begin{tabular}{p{2.8cm} p{2.2cm} p{2.2cm} p{2.2cm} p{2.2cm} p{2.2cm} p{2.3cm}}
    \toprule
    \textbf{Criterion}
    & \textbf{PPO}
    & \textbf{MPO / V-MPO}
    & \textbf{AWR}
    & \textbf{DAR}
    & \textbf{DPO}
    & \textbf{GEMPI} \\
    \midrule

    \textbf{Optimisation paradigm}
    & Policy gradient (PMD, neg.\ entropy mirror map)
    & EM / regularised policy iteration
    & EM / weighted regression
    & EM / closed-form dual-KL target
    & Preference MLE (collapsed EM)
    & Generalised EM (subsumes all) \\

    \textbf{Core objective}
    & Clipped IS surrogate $L^{\text{CLIP}}(\theta)$
    & MAP improvement $\log p_\theta(I{=}1){+}\log p(\theta)$
    & Advantage-weighted MLE (KL-constrained)
    & Dual-KL advantage maximisation $J_{\text{DAR}}$
    & Binary cross-entropy on pref.\ pairs $\mathcal{L}_{\text{DPO}}$
    & $\max_q\,\mathbb{E}_q[A]-\textstyle\sum_j\lambda_j D_j(q\|\pi_j)$, then project \\

    \textbf{Update phases}
    & Single-phase gradient step
    & E-step (non-parametric target) $+$ M-step (parametric projection)
    & E-step $+$ M-step
    & E-step (closed form) $+$ M-step
    & Single-phase; no explicit E-step
    & E-step $+$ M-step (fully configurable) \\

    \textbf{E-step target $q^\star$}
    & N/A
    & $\pi_t\exp(A/\eta)$
    & $\pi_t\exp(A/\beta)$
    & $\pi_{\text{ref}}^{\alpha/\Lambda}\pi_t^{\beta/\Lambda}\exp(A/\Lambda)$
    & $\pi_{\text{ref}}\exp(r/\beta)$ (implicit; never materialised)
    & $\prod_j\pi_j^{\lambda_j/\Lambda}\exp(A/\Lambda)$ \\

    \textbf{E-step anchors ($m$)}
    & N/A
    & 1 \;($\pi_t$)
    & 1 \;($\pi_t$)
    & 2 \;($\pi_{\text{ref}},\pi_t$)
    & 1 \;($\pi_{\text{ref}}$, implicit)
    & $m\!\geq\!0$ (free) \\

    \textbf{KL anchor(s)}
    & Implicit via clipping; optional $\hat{\text{KL}}$ penalty to $\pi_{\text{old}}$
    & $\pi_t$ (E-step); $\pi_t$ (M-step trust region)
    & $\pi_t$ (E-step only)
    & $\pi_{\text{ref}}$ and $\pi_t$ (dual anchor)
    & $\pi_{\text{ref}}$ (single, fixed)
    & Any set $\{\pi_j\}$ \\

    \textbf{Temperature mechanism}
    & N/A; clip threshold $\epsilon$ fixed
    & Adaptive dual: $\eta$ optimised via convex $\mathcal{L}_\eta$
    & Fixed $\beta$
    & Fixed $(\alpha,\beta)$; AT-DAR variant makes both adaptive
    & Fixed $\beta$
    & Fixed or dual-adaptive ($\lambda_j$ per anchor) \\

    \textbf{Gradient signal}
    & $\nabla_\theta r_t(\theta)\,A_t$ \newline (ratio-coupled to $\theta$)
    & $w_i\nabla_\theta\log\pi_\theta$ \newline (detached weights)
    & $w_i\nabla_\theta\log\pi_\theta$ \newline (detached)
    & $w_i\nabla_\theta\log\pi_\theta$ \newline (detached)
    & $\sigma(\cdot)\bigl(\nabla_\theta\log\pi_\theta(y_w)-\nabla_\theta\log\pi_\theta(y_l)\bigr)$
    & $w_i\nabla_\theta\log\pi_\theta$ \newline (detached) \\

    \textbf{IS ratio $\pi_\theta/\pi_{\text{old}}$ required?}
    & \textbf{Yes}  clipped to $[1{\pm}\epsilon]$
    & No
    & No
    & No (optional IS correction if off-policy)
    & No
    & No (IS weights optional for off-policy) \\

    \textbf{M-step loss}
    & N/A (single-phase)
    & $-\!\sum_i w_i\log\pi_\theta(a_i|s_i)$ $+$ KL trust region
    & $-\!\sum_i w_i\log\pi_\theta(a_i|s_i)$ (unconstrained)
    & $-\!\sum_i w_i\log\pi_\theta(y_i|x_i)$ (unconstrained)
    & N/A (collapsed)
    & $-\mathbb{E}_{q^\star}[\log\pi_\theta]+\gamma D_M(\pi_{\theta_k}\|\pi_\theta)$ \\

    \textbf{M-step trust region}
    & N/A
    & \textbf{Yes}  KL with Lagrange dual $\alpha$ \eqref{eq:vmpo_mstep_lag}
    & No
    & No; DAR-TR variant adds KL constraint
    & N/A
    & Optional ($D_M$, budget $\varepsilon_M$) \\

    \textbf{Top-$k$ / sample filtering}
    & No (full-batch IS)
    & \textbf{Yes}  top-$k$ fraction $\rho$ by advantage
    & No (all samples)
    & No; Top-$k$ DAR variant available
    & No
    & Configurable ($\mathcal{F}$: identity or top-$k$) \\

    \textbf{Value function / baseline?}
    & Yes  GAE with $V_\phi$
    & Yes  $n$-step bootstrapped $V^\pi_\phi$
    & Yes  $V^\mu(s)$ by TD/$\lambda$
    & Yes  $V^{\pi_t}(x)$ for advantage
    & No
    & Yes (when $A$ uses a learned baseline) \\

    \textbf{Explicit reward model?}
    & Yes
    & Yes
    & Yes
    & Yes
    & No  implicit $\hat r_\theta{=}\beta\log(\pi_\theta/\pi_{\text{ref}})$
    & Yes (typical; not mandated by framework) \\

    \textbf{Online / offline?}
    & Online (on-policy rollouts; mini-epochs with IS)
    & Online (on-policy rollouts)
    & Offline or near-off-policy
    & Online (rollouts from $\pi_t$)
    & Offline (fixed preference dataset)
    & Configurable \\

    \textbf{Training data format}
    & State--action trajectories $+$ scalar rewards
    & State--action trajectories $+$ scalar rewards
    & State--action trajectories $+$ returns
    & Prompt--response pairs $+$ scalar rewards
    & Pairwise preference triples $(x,y_w,y_l)$
    & General (reward or preference signals) \\

    \textbf{Dropout compatible?}
    & \textbf{No}  stochastic numerator corrupts IS ratio; inconsistent denominator
    & \textbf{Yes}  detached M-step weights; no ratio in objective
    & \textbf{Yes}  detached weights; standard cross-entropy
    & \textbf{Yes}  detached weights
    & \textbf{Yes}  single forward passes only; no ratio
    & \textbf{Yes}  guaranteed when M-step weights detached \\

    \textbf{EM / MAP derivation?}
    & No  direct PG theorem
    & \textbf{Yes}  full MAP EM (Sections~3--4)
    & \textbf{Yes}  constrained policy search $\to$ Boltzmann $\pi^\star$
    & \textbf{Yes}  dual-KL optimisation $\to$ closed-form $\pi^\star$
    & Partial  optimal policy derived, then EM collapsed via BT cancellation
    & \textbf{Yes}  defines the general EM template \\

    \textbf{Adaptive temperature?}
    & No
    & \textbf{Yes} ($\eta$ via dual $\mathcal{L}_\eta$)
    & No ($\beta$ fixed)
    & No (fixed $\alpha,\beta$); AT-DAR variant adds it
    & No ($\beta$ fixed)
    & Yes (dual-adaptive mode) \\

    \textbf{GEMPI tuple}
    & N/A (policy-gradient paradigm)
    & $m{=}1$, $D_1{=}\mathrm{KL}$, $\pi_1{=}\pi_t$, dual $\eta$, top-$k$, $D_M{=}\mathrm{KL}$
    & $m{=}1$, $\pi_1{=}\pi_t$, $\lambda_1{=}\beta$ fixed, no $D_M$, no filter
    & $m{=}2$, $\pi_1{=}\pi_{\text{ref}}$, $\pi_2{=}\pi_t$, $(\alpha,\beta)$ fixed, no $D_M$
    & $m{=}1$, $\pi_1{=}\pi_{\text{ref}}$, $\Lambda{=}\beta$, E-step collapsed (no M-step)
    & Full tuple $\mathcal{G}$  all axes free \\

    \textbf{Novel variants / extensions}
    & Seq-level PPO (product IS ratio; KL early stop)
    & LLM V-MPO (seq-level E/M-step, Section~\ref{sec:vmpo_llm_gempi})
    & ---
    & AT-DAR; DAR-TR (M-step KL); Top-$k$ DAR
    & IPO, $\beta$-DPO, rDPO
    & Any combo of $m$, $D_j$, $\mathcal{T}$, $\mathcal{F}$, $D_M$ \\

    \textbf{Primary LLM use-case}
    & Online RLHF (InstructGPT, LLaMA-RLHF)
    & Online RL fine-tuning with EM stability
    & Near-offline advantage-weighted SFT
    & Online alignment with dual regularisation
    & Offline preference alignment (no RM needed)
    & Unifying design framework \\

    \bottomrule
  \end{tabular}
\end{center}


% ============================================================
\section{GTrXL: Gated Transformer-XL for RL}
\label{sec:gtrxl}

The V-MPO authors \parencite{song2019vmpoonpolicymaximumposteriori} reported strong empirical results pairing V-MPO with a Transformer-XL (TrXL) backbone \parencite{dai2019transformerxlattentivelanguagemodels} on Atari, and later with the Gated Transformer-XL (GTrXL) \parencite{parisotto2019stabilizingtransformersreinforcementlearning} on DMLab-30. This appendix summarises the GTrXL modifications and why they complement V-MPO's EM structure. Reference implementations are available at \href{https://github.com/kimiyoung/transformer-xl}{github.com/kimiyoung/transformer-xl} and \href{https://github.com/nenuadrian/DI-engine/tree/main/benchmarks}{github.com/nenuadrian/DI-engine}.

Standard TrXL fails in RL, performing at random-policy level on benchmarks such as DMLab-30. GTrXL addresses this with two targeted modifications.

\paragraph{Identity Map Reordering (TrXL-I).}
Layer normalisation is moved to the \emph{input} of each sub-layer rather than the output, creating a direct identity path from the first layer's input to the last layer's output. At initialisation this biases the network towards a Markovian (reactive) policy and provides a clean gradient path, making the training landscape more amenable to policy optimisation.

\paragraph{GRU Gating Layers.}
Residual connections are replaced with learnable gating layers. The best-performing variant uses GRU-type gates:
\begin{align*}
  r &= \sigma(W_r y + U_r x), \\
  z &= \sigma(W_z y + U_z x - b_g), \\
  \hat{h} &= \tanh(W_g y + U_g (r \odot x)), \\
  g(x,y) &= (1-z) \odot x + z \odot \hat{h},
\end{align*}
where $x$ is the residual input and $y$ is the sub-layer output. Initialising $b_g = 2$ places each gate near the identity at the start of training, preserving the reactive-policy initialisation from the layer-norm reordering.

\paragraph{Why GTrXL suits V-MPO.}
V-MPO collects fresh on-policy trajectories each update, making long-horizon memory critical. GTrXL provides: (i) \textbf{long-range memory} via relative position encodings spanning thousands of past time-steps; (ii) \textbf{stable optimisation}  GRU gating achieves a 0\% divergence rate across hyperparameter sweeps vs.\ 16\% for plain TrXL-I, compatible with V-MPO's fixed Adam learning rate; (iii) \textbf{robust performance}  on DMLab-30, GTrXL (GRU) reaches $117.6 \pm 0.3$ human-normalised score vs.\ $99.3 \pm 1.0$ for LSTM.


\end{document}
