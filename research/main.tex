\documentclass[10pt]{article}

\usepackage[margin=1in]{geometry}

\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{url}
% For \coloneqq and related symbols
\usepackage{mathtools}
\usepackage{breqn}
\usepackage[style=authoryear, backend=biber]{biblatex}

\addbibresource{references.bib}

\begin{document}

\setcounter{secnumdepth}{1}

\title{Expectation--Maximization Frameworks for LLM RL Fine-Tuning}

\maketitle
\tableofcontents
\clearpage

\section{Introduction}
Transformer language models are commonly adapted to downstream tasks via supervised fine-tuning (SFT), and further improved via RL fine-tuning against a learned or human preference reward. While these procedures are usually presented as distinct (cross-entropy training versus policy optimization), both can be interpreted as alternating between constructing a training target distribution and then fitting the model to that target.

\section{Research Questions}

\begin{enumerate}
  \item Can supervised fine-tuning and KL-regularized RL fine-tuning be expressed under a common EM/MAP formulation with a shared E-step/M-step interpretation?
  \item In what precise sense does V-MPO correspond to regularized policy iteration, and how does that differ from direct policy-gradient optimization (e.g.\ PPO)?
  \item Does adaptive temperature optimization in the E-step provide a practical stability advantage over fixed-temperature weighting at LLM scale?
  \item How does the DAR closed-form target relate to the V-MPO target, and under which limits are they equivalent?
  \item Do EM-style weighted-MLE updates provide practical benefits for transformer fine-tuning, including compatibility with dropout-style regularization?
\end{enumerate}

\section{Preliminaries}

\subsection{Maximum Likelihood Estimation and Maximum a Posteriori}

MLE and MAP are methods for estimating some variable in the setting of probability distributions. They compute a single estimate, instead of a full distribution.

Given observed data $x$ and parameters $\theta$, maximum likelihood (ML) estimation solves
\[
  \theta_{\mathrm{MLE}}
  =
  \arg\max_\theta \log p_\theta(X)
  =
  \arg\max_\theta
  \sum_{i=1}^{N}
  \log p_\theta(x_i).
\]

Maximum a posteriori (MAP) estimation adds a prior, coming from the Bayesian perspective:
\[
  p(\theta \mid X)
  =
  \frac{p(x \mid \theta)\, p(\theta)}{p(x)}
\]

% Proportional form
\[
  p(\theta \mid X)
  \propto
  p(X \mid \theta)\, p(\theta)
\]

% With i.i.d. likelihood
\[
  \theta_{\mathrm{MAP}}
  =
  \arg\max_\theta
  \left[
    \sum_{i=1}^{N}
    \log p(x_i \mid \theta)
    +
    \log p(\theta)
  \right] =
  \arg\max_\theta
  \bigl(
    \log p_\theta(X)
    +
    \log p(\theta)
  \bigr).
\]

\subsection{EM Lower Bound -- ELBO}

Assume a latent variable model with latent $z$. For any auxiliary distribution $q(z)$:
\[
  \log p_\theta(x)
  =
  \underbrace{
    \mathbb{E}_{q(z)}
    \big[
      \log p_\theta(x,z) - \log q(z)
    \big]
  }_{\mathcal{L}(q,\theta)}
  +
  \mathrm{KL}
  \big(
    q(z)\,\|\,p_\theta(z\mid x)
  \big).
\]

Since KL is nonnegative, $\mathcal{L}(q,\theta)$ is a lower bound on $\log p_\theta(x)$.

For MAP, the objective
\[
  \mathcal{L}_{\mathrm{MAP}}(\theta)
  =
  \log p_\theta(x)
  +
  \log p(\theta)
\]
admits the decomposition
\[
  \mathcal{L}_{\mathrm{MAP}}(\theta)
  =
  \mathcal{L}_{\mathrm{MAP}}(q,\theta)
  +
  \mathrm{KL}
  \big(
    q(z)\,\|\,p_\theta(z\mid x)
  \big),
\]
with
\[
  \mathcal{L}_{\mathrm{MAP}}(q,\theta)
  =
  \mathbb{E}_{q(z)}
  \big[
    \log p_\theta(x,z) - \log q(z)
  \big]
  +
  \log p(\theta).
\]

\subsection{EM Iterations}

At iteration $k$, EM alternates:
\[
  \text{E-step:}\quad
  q_{k+1}(z)=p_{\theta_k}(z\mid x),
\]
\[
  \text{M-step:}\quad
  \theta_{k+1}
  =
  \arg\max_\theta \mathcal{L}_{\mathrm{MAP}}(q_{k+1},\theta).
\]

Equivalently, the MAP M-step maximises
\[
  Q_{\mathrm{MAP}}(\theta,\theta_k)
  =
  \mathbb{E}_{z\sim p_{\theta_k}(z\mid x)}
  \big[
    \log p_\theta(x,z)
  \big]
  +
  \log p(\theta).
\]

This gives the standard monotonic-improvement template: construct a target distribution in the E-step, then fit a parametric model to that target in the M-step.

\subsection{EM Policy Iteration vs Policy Gradients}

Policy-gradient update (direct parameter-space step).

\[
  J(\theta)=\mathbb{E}_{\tau\sim\pi_\theta}[R(\tau)].
\]
A policy-gradient method takes a local ascent step
\[
  \theta_{k+1}
  =
  \theta_k
  +
  \lambda\,
  \widehat{\nabla_\theta J(\theta_k)},
\]
optionally with clipping or explicit KL penalties.

EM-style methods introduce an auxiliary improved policy $q(a\mid s)$ and alternate:
\[
  \text{E-step:}\quad
  q_{k+1}
  =
  \arg\max_q
  \Big(
    \mathbb{E}_{s\sim d_{\pi_k},\,a\sim q}
    [A^{\pi_k}(s,a)]
    -
    \eta\,
    \mathrm{KL}(q(\cdot\mid s)\,\|\,\pi_k(\cdot\mid s))
  \Big),
\]
whose solution has Boltzmann form
\[
  q_{k+1}(a\mid s)
  \propto
  \pi_k(a\mid s)\,
  \exp\!\Big(\frac{A^{\pi_k}(s,a)}{\eta}\Big).
\]
Then the M-step projects back to the parametric family:
\[
  \theta_{k+1}
  =
  \arg\max_\theta\;
  \mathbb{E}_{s\sim d_{\pi_k},\,a\sim q_{k+1}}
  [\log \pi_\theta(a\mid s)],
\]
often with an additional trust-region term on $\mathrm{KL}(\pi_k\|\pi_\theta)$.

Policy gradients optimize parameters directly via noisy first-order steps. EM-style methods perform policy improvement in distribution space first, then do weighted maximum-likelihood fitting. This is why V-MPO is naturally viewed as regularized policy iteration in EM form rather than as a pure policy-gradient method.


\section{V-MPO: Maximum a Posteriori Policy Optimization}

V-MPO \parencite{song2019vmpoonpolicymaximumposteriori} decomposes optimisation into EM phases.

The total objective is
\begin{equation*}
  \mathcal{L}(\phi,\theta,\eta,\alpha)
  =
  \mathcal{L}_V(\phi)
  +
  \mathcal{L}_{\mathrm{V\text{-}MPO}}(\theta,\eta,\alpha),
\end{equation*}
where
\begin{equation*}
  \mathcal{L}_{\mathrm{V\text{-}MPO}}(\theta,\eta,\alpha)
  =
  \mathcal{L}_\pi(\theta)
  +
  \mathcal{L}_\eta(\eta)
  +
  \mathcal{L}_\alpha(\theta,\alpha).
\end{equation*}
% ============================================================
\paragraph{Policy Evaluation (Critic Update).}

The value function is fitted via n-step bootstrapped regression:
\begin{equation*}
  \mathcal{L}_V(\phi)
  =
  \frac{1}{2|\mathcal{D}|}
  \sum_{s_t \sim \mathcal{D}}
  \left(
    V^\pi_\phi(s_t)
    -
    G_t^{(n)}
  \right)^2,
\end{equation*}
with
\[
  G_t^{(n)}
  =
  \sum_{k=t}^{t+n-1}
  \gamma^{k-t} r_k
  +
  \gamma^n V^\pi_\phi(s_{t+n}).
\]

Advantages are defined as
\[
  A^\pi(s_t,a_t)
  =
  G_t^{(n)} - V^\pi_\phi(s_t).
\]

% ============================================================
\paragraph{Policy Improvement via EM}

We formulate policy improvement as MAP estimation:
\begin{equation*}
  \theta^\star
  =
  \arg\max_\theta
  \log p_\theta(I=1)
  +
  \log p(\theta),
\end{equation*}
where $I$ denotes the improvement event.

Introduce a variational distribution $\psi(s,a)$:
\begin{equation*}
  \log p_\theta(I=1)
  =
  \sum_{s,a}
  \psi(s,a)
  \log
  \frac{p_\theta(I=1,s,a)}{\psi(s,a)}
  +
  \mathrm{KL}
  \big(
    \psi(s,a)
    \;\|\;
    p_\theta(s,a \mid I=1)
  \big).
\end{equation*}

We now alternate between E-step and M-step.

% ============================================================
\paragraph{E-Step: Non-Parametric Policy Construction}

The E-step solves
\begin{equation*}
  \begin{aligned}
    \psi^\star
    =
    \arg\max_{\psi}
    &\;
    \sum_{s,a}
    \psi(s,a)
    A^{\pi_{\theta_{\mathrm{old}}}}(s,a) \\
    \text{s.t.}
    &\;
    \sum_{s,a}
    \psi(s,a)
    \log
    \frac{\psi(s,a)}{p_{\theta_{\mathrm{old}}}(s,a)}
    <
    \epsilon_\eta,
    \\
    &\;
    \sum_{s,a} \psi(s,a) = 1.
  \end{aligned}
\end{equation*}

The Lagrangian is
\begin{equation*}
  \begin{aligned}
    J(\psi,\eta,\lambda)
    =
    &\sum_{s,a}
    \psi(s,a)
    A^{\pi_{\theta_{\mathrm{old}}}}(s,a)
    \\
    &+
    \eta
    \Big(
      \epsilon_\eta
      -
      \sum_{s,a}
      \psi(s,a)
      \log
      \frac{\psi(s,a)}{p_{\theta_{\mathrm{old}}}(s,a)}
    \Big)
    \\
    &+
    \lambda
    \Big(
      1 - \sum_{s,a} \psi(s,a)
    \Big).
  \end{aligned}
\end{equation*}

Here, stationarity refers to the KKT first-order optimality condition with respect to the variational distribution $\psi$:
\begin{equation*}
  \frac{\partial J}{\partial \psi(s,a)}
  =
  A^{\pi_{\theta_{\mathrm{old}}}}(s,a)
  -
  \eta
  \left(
    \log
    \frac{\psi(s,a)}{p_{\theta_{\mathrm{old}}}(s,a)}
    + 1
  \right)
  -
  \lambda
  =
  0,
  \quad \forall (s,a).
\end{equation*}

Solving this stationarity condition for $\psi$ gives
\begin{equation*}
  \psi(s,a)
  =
  \frac{
    p_{\theta_{\mathrm{old}}}(s,a)
    \exp\!\big(
      A^{\pi_{\theta_{\mathrm{old}}}}(s,a)/\eta
    \big)
  }{
    \sum_{s',a'}
    p_{\theta_{\mathrm{old}}}(s',a')
    \exp\!\big(
      A^{\pi_{\theta_{\mathrm{old}}}}(s',a')/\eta
    \big)
  }.
\end{equation*}

The temperature dual is
\begin{equation*}
  \mathcal{L}_\eta(\eta)
  =
  \eta \epsilon_\eta
  +
  \eta
  \log
  \Big(
    \sum_{s,a}
    p_{\theta_{\mathrm{old}}}(s,a)
    \exp
    \big(
      A^{\pi_{\theta_{\mathrm{old}}}}(s,a)/\eta
    \big)
  \Big).
\end{equation*}

% ============================================================
\paragraph{M-Step: Parametric Projection with KL Constraint}

The M-step minimises the negative lower bound:

\begin{equation*}
  \mathcal{L}_\pi(\theta)
  =
  -
  \sum_{s,a}
  \psi(s,a)
  \log
  \pi_\theta(a|s).
\end{equation*}

Subject to a KL trust-region constraint:
\begin{equation*}
  \mathbb{E}_{s \sim p(s)}
  \big[
    \mathrm{KL}
    (
      \pi_{\theta_{\mathrm{old}}}(\cdot|s)
      \;\|\;
      \pi_\theta(\cdot|s)
    )
  \big]
  <
  \epsilon_\alpha.
\end{equation*}

The Lagrangian form is
\begin{equation}
  J(\theta,\alpha)
  =
  \mathcal{L}_\pi(\theta)
  +
  \alpha
  \Big(
    \epsilon_\alpha
    -
    \mathbb{E}_{s}
    \mathrm{KL}
    (
      \pi_{\theta_{\mathrm{old}}}
      \|\,
      \pi_\theta
    )
  \Big).
  \tag{12}
\end{equation}

In implementation, the loss becomes
\begin{equation}
  \begin{aligned}
    \mathcal{L}_\alpha(\theta,\alpha)
    =
    &
    \alpha
    \Big(
      \epsilon_\alpha
      -
      \mathrm{sg}
      [
        \mathrm{KL}
        (
          \pi_{\theta_{\mathrm{old}}}
          \|\,
          \pi_\theta
        )
      ]
    \Big)
    \\
    &+
    \mathrm{sg}[\alpha]
    \,
    \mathrm{KL}
    (
      \pi_{\theta_{\mathrm{old}}}
      \|\,
      \pi_\theta
    ).
  \end{aligned}
\end{equation}

\section{PPO: Proximal Policy Optimization}

PPO is a PG method as it directly estimates and follows the gradient of expected return in parameter space with an on-policy actor-critic algorithm that constrains each policy update to stay close to the behaviour policy via a clipped surrogate objective, avoiding the instability of unconstrained policy gradient steps.

A trajectory $\tau = (s_0,a_0,\dots,s_{T-1},a_{T-1})$ is collected under the current policy $\pi_{\theta_{\mathrm{old}}}$. Per-step log-probabilities and their sum are
\[
  \ell_{\theta,t} = \log \pi_\theta(a_t \mid s_t),
  \qquad
  \ell_\theta(\tau) = \sum_{t=0}^{T-1} \ell_{\theta,t}.
\]

The discounted reward-to-go from step $t$ is
\[
  R_t = \sum_{k=t}^{T-1} \gamma^{k-t} r_k.
\]

The policy gradient theorem gives the direction of steepest ascent for the expected return:
\[
  \nabla_\theta J(\theta)
  =
  \mathbb{E}_{\tau \sim \pi_\theta}
  \left[
    \sum_{t=0}^{T-1}
    \nabla_\theta \log \pi_\theta(a_t \mid s_t)\, A_t
  \right].
\]

To reuse data collected under $\pi_{\theta_{\mathrm{old}}}$, importance sampling introduces the per-step probability ratio
\[
  r_t(\theta)
  =
  \frac{\pi_\theta(a_t \mid s_t)}
  {\pi_{\theta_{\mathrm{old}}}(a_t \mid s_t)}
  =
  \exp\!\left(\ell_{\theta,t}-\ell_{\mathrm{old},t}\right).
\]

The unclipped surrogate objective is then $L^{\mathrm{PG}}(\theta) = \mathbb{E}_t\!\left[r_t(\theta)\, A_t\right]$, but without further constraint this can lead to destructively large updates.

PPO resolves this by clipping the ratio to $[1-\epsilon,\, 1+\epsilon]$ and taking the pessimistic (minimum) of the clipped and unclipped terms:
\[
  L^{\mathrm{CLIP}}(\theta)
  =
  \mathbb{E}_t
  \left[
    \min\!\left(
      r_t(\theta)\, A_t,\;
      \operatorname{clip}\!\left(
        r_t(\theta),\, 1-\epsilon,\, 1+\epsilon
      \right) A_t
    \right)
  \right].
\]
When the advantage is positive the update is capped at a ratio of $1+\epsilon$; when negative it is capped at $1-\epsilon$. 


The critic is fitted by minimising a squared regression loss to the empirical returns:
\[
  L^{\mathrm{VF}}(\phi)
  =
  \mathbb{E}_t
  \left[
    \left(V_\phi(s_t) - R_t\right)^2
  \right].
\]

An entropy bonus encourages exploration by penalising premature policy collapse:
\[
  S(\pi_\theta(\cdot \mid s_t))
  =
  -\sum_a \pi_\theta(a \mid s_t) \log \pi_\theta(a \mid s_t).
\]


Rather than using raw Monte Carlo returns to estimate $A_t$, PPO typically uses GAE, which trades off bias and variance via a decay parameter $\lambda \in [0,1]$.

The TD residual at each step is
\[
  \delta_t = r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t).
\]

GAE accumulates these residuals with exponentially decaying weights:
\[
  A_t^{\mathrm{GAE}(\gamma,\lambda)}
  =
  \sum_{l=0}^{\infty} (\gamma\lambda)^l\, \delta_{t+l},
\]
which satisfies the efficient recurrence
\[
  A_t = \delta_t + \gamma\lambda\, A_{t+1}.
\]

Advantages are then batch-normalised before use:
\[
  \hat{A}_t = \frac{A_t - \mu_A}{\sigma_A + \varepsilon}.
\]


\paragraph{Full Objective}

The three terms are combined into a single objective (to minimise):
\[
  \mathcal{L}(\theta,\phi)
  =
  -L^{\mathrm{CLIP}}(\theta)
  +
  c_1\, L^{\mathrm{VF}}(\phi)
  -
  c_2\, \mathbb{E}_t\!\left[S(\pi_\theta(\cdot \mid s_t))\right],
\]
where $c_1$ and $c_2$ are scalar coefficients balancing the three losses.


\paragraph{Sequence-Level PPO (LLM Case)}

When the ``action'' is an entire generated sequence (as in LLM fine-tuning), the per-step ratios multiply into a sequence-level ratio:
\[
  r_{\mathrm{seq}}(\theta)
  =
  \exp\!\left(
    \sum_{t=0}^{T-1}
    \bigl(\ell_{\theta,t} - \ell_{\mathrm{old},t}\bigr)
  \right).
\]

A KL penalty between the updated and old policy can be estimated cheaply as
\[
  \widehat{\mathrm{KL}}
  =
  \mathbb{E}_t\!\left[\ell_{\mathrm{old},t} - \ell_{\theta,t}\right],
\]
and is often added to the objective or used as an early-stopping criterion to keep the sequence-level ratio well-behaved.


\section{The Case for Dropout}
\label{sec:dropout_case}

Dropout \parencite{srivastava2014dropout} randomly zeros each hidden activation with probability $p$ during training and scales the remaining activations by $1/(1-p)$:
\[
  \tilde{h}_j = \frac{m_j}{1-p}\, h_j, \qquad m_j \sim \mathrm{Bernoulli}(1-p).
\]
In a Transformer this is typically applied in two places: after the attention weights (attention dropout) and after each feed-forward sub-layer (residual dropout). If $\mathbf{h}^{(l)}$ is the hidden state at layer $l$, the residual-dropout forward pass through one sub-layer $f^{(l)}$ is
\[
  \mathbf{h}^{(l)} = \mathbf{h}^{(l-1)} + \mathrm{Dropout}\!\big(f^{(l)}(\mathbf{h}^{(l-1)})\big).
\]

During SFT, dropout serves its classical purpose: it regularises the model and reduces overfitting to the demonstration distribution.  Most large-scale LLM pre-training runs (GPT-3, LLaMA, etc.) disable dropout entirely, relying instead on the sheer volume of data for regularisation. However, during fine-tuning the dataset is typically orders of magnitude smaller, and overfitting is a real concern, making dropout relevant again.

\subsection{Why PPO Disables Dropout}

Standard on-policy PPO collects a batch of trajectories under the current policy $\pi_{\theta_{\mathrm{old}}}$ (in eval mode, no dropout), then performs several epochs of gradient updates on that batch. The clipped objective relies on the importance-sampling ratio
\[
  r_t(\theta) = \frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{\mathrm{old}}}(a_t \mid s_t)}.
\]

\paragraph{Problem 1: stochastic numerator.}
If dropout is active during the optimisation epochs, $\pi_\theta(a_t \mid s_t)$ becomes a random variable that changes on every forward pass even for fixed $\theta$. The ratio $r_t(\theta)$ therefore fluctuates stochastically, injecting noise directly into the surrogate objective and its gradients. Because PPO's clip window $[1-\epsilon,\,1+\epsilon]$ is deliberately narrow (typically $\epsilon=0.2$), even moderate stochastic perturbation can push the ratio outside the clip region or mask genuine policy changes, degrading the signal.

\paragraph{Problem 2: inconsistent denominator.}
The denominator $\pi_{\theta_{\mathrm{old}}}(a_t \mid s_t)$ is computed once at rollout time in eval mode (no dropout). If the training forward pass uses a different dropout mask, the numerator and denominator are computed under different effective networks.  The ratio no longer measures how far the policy has moved; it conflates policy change with mask change:
\[
  r_t(\theta) = \frac{\pi_\theta^{(\text{mask}_1)}(a_t \mid s_t)}{\pi_{\theta_{\mathrm{old}}}^{(\text{no mask})}(a_t \mid s_t)}.
\]
This breaks the theoretical guarantee that clipping $r_t$ constrains the KL divergence between consecutive policies.

\paragraph{Problem 3: KL penalty corruption.}
Many PPO implementations add an auxiliary KL penalty $\widehat{\mathrm{KL}} = \mathbb{E}_t[\ell_{\mathrm{old},t} - \ell_{\theta,t}]$ or use it for early stopping. With dropout active, the estimated KL is biased upward (the mask-induced variance appears as divergence), causing premature termination of updates or an overly conservative step.

For these reasons, all mainstream PPO implementations (including those used for RLHF in InstructGPT, LLaMA, etc.) run the policy in eval mode during both rollout and optimisation, forgoing any regularisation benefit that dropout could provide.

\subsection{Why V-MPO / EM-Style Methods Are Dropout-Compatible}

The V-MPO update decomposes into an E-step that computes per-sample weights, followed by an M-step that is pure weighted supervised learning.  Neither step requires a probability ratio between two forward passes under different modes.

\paragraph{E-step: weights from advantages, not ratios.}
The non-parametric target uses the same sequence-level exponential weighting as
Eq.~\eqref{eq:seq_estep_weights}.
Here $A^{(i)} = G^{(i)} - V_\phi(s^{(i)})$ depends on the value function and the observed return, not on a ratio of policy probabilities. The value function $V_\phi$ is a regression target; dropout can be disabled for the single forward pass that computes advantages at rollout time (just as PPO evaluates its value head in eval mode), or it can remain active since the advantage computation is a one-shot evaluation, not an iterative ratio.

\paragraph{M-step: standard cross-entropy.}
The M-step objective is the weighted teacher-forced cross-entropy in
Eq.~\eqref{eq:seq_weighted_mstep},
which is structurally identical to SFT with per-sequence weights. Dropout is fully compatible here for the same reason it is compatible with any supervised cross-entropy objective: the loss is evaluated under a single stochastic forward pass, and the gradient is an unbiased estimator of the expected loss under the dropout distribution.

\paragraph{No ratio, no conflict.}
The key structural difference is summarised in the following table:

\medskip
\begin{center}
  \begin{tabular}{lcc}
    \toprule
    & \textbf{PPO} & \textbf{V-MPO / EM} \\
    \midrule
    Gradient signal & $\nabla_\theta\, r_t(\theta)\, A_t$ & $\nabla_\theta\, w_i \log \pi_\theta$ \\
    Requires ratio $\pi_\theta / \pi_{\theta_{\mathrm{old}}}$? & Yes & No \\
    Weights depend on current $\theta$? & Yes (through $r_t$) & No (fixed from E-step) \\
    Dropout in training pass & Corrupts ratio & Standard regularisation \\
    \bottomrule
  \end{tabular}
\end{center}
\medskip

Because the E-step weights $w_i$ are detached from the current parameters, the M-step gradient with dropout active is
\[
  \nabla_\theta \mathcal{L}_\pi = -\sum_{i \in S} w_i \sum_t \nabla_\theta \log \pi_\theta^{(\text{mask})}(y_t^{(i)} \mid \cdot\,),
\]
which is an unbiased estimate of the true weighted-MLE gradient under the dropout distribution, exactly as in supervised learning. No eval-mode forward pass needs to be compared against this quantity.

\subsection{Practical Implications}

In the LLM fine-tuning regime where data is limited and overfitting is a practical concern, V-MPO's EM structure therefore offers a regularisation advantage over PPO: dropout (and related stochastic regularisers such as DropPath or stochastic depth) can be enabled during the M-step without any algorithmic modification, providing the same generalisation benefits observed in supervised fine-tuning. PPO, by contrast, must rely on non-architectural regularisation (weight decay, gradient clipping, early stopping) because its core mechanism is incompatible with stochastic forward passes.

\section{DPO: Direct Preference Optimization}
\label{sec:dpo}

Direct Preference Optimization (DPO) \parencite{rafailov2023dpo} is a preference MLE method: it bypasses reward modelling and policy-gradient estimation entirely, reducing alignment to a single binary cross-entropy objective over preference pairs. This section derives DPO from the same KL-regularised RLHF objective used throughout this document, characterises it as maximum-likelihood estimation, and relates it to the GEMPI framework.

Unlike policy-gradient methods, DPO needs no reward model (the reward is implicit), no value function, no advantage estimation, no importance-sampling ratios, and no online rollouts. The entire training signal comes from offline preference pairs.


The standard RLHF objective is
\begin{equation}\label{eq:dpo_rlhf}
  \max_{\pi}
  \mathbb{E}_{x\sim\mathcal{D},\, y\sim\pi(\cdot|x)}
  \big[r(x,y)\big]
  -
  \beta\,
  D_{\mathrm{KL}}\!\big(\pi(\cdot|x)\,\|\,\pi_{\mathrm{ref}}(\cdot|x)\big),
\end{equation}
where $\beta > 0$ controls regularisation toward the reference policy $\pi_{\mathrm{ref}}$.

\paragraph{Closed-form optimal policy.}
The optimisation over $\pi$ for each $x$ is a KL-regularised linear problem with the same structure as the GEMPI E-step. The solution is
\begin{equation}\label{eq:dpo_optpolicy}
  \pi^\star(y\mid x)
  =
  \frac{1}{Z(x)}\,
  \pi_{\mathrm{ref}}(y\mid x)\,
  \exp\!\Big(\frac{r(x,y)}{\beta}\Big),
\end{equation}
where $Z(x) = \sum_y \pi_{\mathrm{ref}}(y\mid x)\exp(r(x,y)/\beta)$ is the partition function. This is a GEMPI E-step with $m=1$, $\pi_1=\pi_{\mathrm{ref}}$, $\lambda_1=\beta$, $\Lambda=\beta$.

\paragraph{Reward reparameterisation.}
Rearranging \eqref{eq:dpo_optpolicy} expresses the reward as a function of the optimal policy:
\begin{equation}\label{eq:dpo_reward_reparam}
  r(x,y)
  =
  \beta\log\frac{\pi^\star(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}
  +
  \beta\log Z(x).
\end{equation}

\paragraph{Bradley--Terry preference model.}
The probability of preferring response $y_w$ over $y_l$ under the Bradley--Terry model is $p(y_w \succ y_l \mid x) = \sigma\big(r(x,y_w) - r(x,y_l)\big)$, where $\sigma$ is the logistic function. Substituting \eqref{eq:dpo_reward_reparam}:
\begin{equation}\label{eq:dpo_bt_sub}
  p(y_w \succ y_l \mid x)
  =
  \sigma\!\bigg(
    \beta\log\frac{\pi^\star(y_w\mid x)}{\pi_{\mathrm{ref}}(y_w\mid x)}
    -
    \beta\log\frac{\pi^\star(y_l\mid x)}{\pi_{\mathrm{ref}}(y_l\mid x)}
  \bigg).
\end{equation}
The partition function $Z(x)$ cancels in the reward difference---this is the key algebraic step that makes DPO tractable.

\paragraph{The DPO loss.}
Replacing the unknown $\pi^\star$ with a parametric policy $\pi_\theta$ and maximising the log-likelihood of observed preferences yields the DPO objective:
\begin{equation}\label{eq:dpo_loss}
  \mathcal{L}_{\mathrm{DPO}}(\theta)
  =
  -\mathbb{E}_{(x,y_w,y_l)\sim\mathcal{D}}
  \left[
    \log\sigma\!\bigg(
      \beta\log\frac{\pi_\theta(y_w\mid x)}{\pi_{\mathrm{ref}}(y_w\mid x)}
      -
      \beta\log\frac{\pi_\theta(y_l\mid x)}{\pi_{\mathrm{ref}}(y_l\mid x)}
    \bigg)
  \right].
\end{equation}

\paragraph{DPO as Preference MLE}

The DPO loss \eqref{eq:dpo_loss} is binary cross-entropy over preference pairs---structurally, it is maximum likelihood estimation on a classification task. The gradient has the form
\[
  \nabla_\theta \mathcal{L}_{\mathrm{DPO}}
  =
  -\beta\,\mathbb{E}\!\Big[
    \sigma\!\big(\hat{r}_\theta(y_l) - \hat{r}_\theta(y_w)\big)
    \big(
      \nabla_\theta\log\pi_\theta(y_w\mid x)
      -
      \nabla_\theta\log\pi_\theta(y_l\mid x)
    \big)
  \Big],
\]
where $\hat{r}_\theta(y) \coloneqq \beta\log(\pi_\theta(y\mid x)/\pi_{\mathrm{ref}}(y\mid x))$ is the implicit reward. The sigmoid weight measures how wrong the current model is on this pair: it is large when the model incorrectly prefers $y_l$, and vanishes as the model learns the correct ranking. This is a weighted MLE update that increases the log-probability of the preferred response and decreases that of the dispreferred one.


\paragraph{DPO and the EM Framework}

The closed-form optimal policy \eqref{eq:dpo_optpolicy} is identical to GEMPI with $m=1$, $D_1=\mathrm{KL}$, $\pi_1=\pi_{\mathrm{ref}}$, $\Lambda=\beta$:
\[
  q^\star(y\mid x) \propto \pi_{\mathrm{ref}}(y\mid x)\,\exp\!\Big(\frac{r(x,y)}{\beta}\Big).
\]
In a standard EM method (V-MPO, DAR, AWR), one would now construct this target distribution and then project to $\pi_\theta$ via weighted MLE in the M-step.

\paragraph{The ``collapsed EM'' interpretation.}
DPO eliminates the explicit E-step entirely. Instead of constructing $q^\star$ and then fitting $\pi_\theta$ to it, DPO substitutes the functional form of the optimal policy directly into the Bradley--Terry preference likelihood. The intractable partition function $Z(x)$ cancels in the reward difference $r(x,y_w) - r(x,y_l)$, yielding a closed-form objective that implicitly solves both the E-step and M-step in a single stage.

\paragraph{What is lost.}
By collapsing the two stages, DPO gives up several degrees of freedom available to explicit EM methods:
\begin{itemize}
  \item \textbf{Reward flexibility.} DPO's reward is defined as $\beta\log(\pi_\theta/\pi_{\mathrm{ref}})$; it cannot incorporate richer reward signals (e.g.\ multi-aspect scores, process rewards) without redefining the objective.
  \item \textbf{Temperature adaptation.} The coefficient $\beta$ is fixed; there is no dual mechanism to adapt it online as in V-MPO.
  \item \textbf{Multi-anchor regularisation.} DPO anchors to $\pi_{\mathrm{ref}}$ only. DAR's dual-anchor structure ($\pi_{\mathrm{ref}}$ and $\pi_t$) is not available within the DPO framework.
  \item \textbf{Online data.} DPO operates on a fixed offline preference dataset; the policy does not generate new rollouts during training.
\end{itemize}

\paragraph{Dropout compatibility.}
DPO's gradient involves only single forward passes through $\pi_\theta$ (computing $\log\pi_\theta(y_w\mid x)$ and $\log\pi_\theta(y_l\mid x)$), with no ratio between two forward passes under different modes. Dropout is therefore fully compatible with DPO, for the same structural reason it is compatible with SFT and EM-style M-steps.


\section{AWR: Advantage-Weighted Regression}

Consider an MDP with states \(s\in\mathcal{S}\), actions \(a\in\mathcal{A}\), reward \(r(s,a)\) and discount
\( \gamma\in[0,1)\). For a policy \(\pi\) define the return of a state--action pair under a sampling policy
\(\mu\) as
\[
R^\mu_{s,a} \;=\; \sum_{t=0}^{\infty} \gamma^{t} r_t \qquad\text{(trajectory return when action \(a\) is taken in \(s\) then follow \(\mu\)).}
\]
The value and advantage under \(\mu\) are
\[
V^\mu(s) \;=\; \mathbb{E}_{a\sim\mu(\cdot|s)}\big[ R^\mu_{s,a} \big],
\qquad
A^\mu(s,a) \;=\; R^\mu_{s,a} - V^\mu(s).
\]

\paragraph{Expected improvement objective.}
Define the expected improvement of a candidate policy \(\pi\) over \(\mu\) as
\[
\eta(\pi) \;=\; J(\pi) - J(\mu)
\;=\;
\mathbb{E}_{s\sim d_\pi}\mathbb{E}_{a\sim\pi(\cdot|s)}\big[ A^\mu(s,a)\big],
\]
where \(d_\pi(s)=\sum_{t=0}^\infty \gamma^{t} p(s_t=s\mid\pi)\) is the unnormalised discounted state distribution.
To avoid sampling from \(\pi\) during the optimisation we use the common first-order surrogate that uses the
sampling state distribution \(d_\mu\):
\[
\widehat{\eta}(\pi)
\;=\;
\mathbb{E}_{s\sim d_\mu}\mathbb{E}_{a\sim\pi(\cdot|s)}\big[ A^\mu(s,a)\big].
\]

\paragraph{Constrained policy search (primal).}
We formulate a constrained optimisation that maximises the surrogate expected improvement while
keeping \(\pi\) close to \(\mu\) in KL averaged under \(d_\mu\):
\[
\begin{aligned}
&\max_{\pi} \quad 
\mathbb{E}_{s\sim d_\mu}\mathbb{E}_{a\sim\pi(\cdot|s)}\big[ R^\mu_{s,a} - V^\mu(s) \big],\\[4pt]
&\text{s.t.}\quad 
\mathbb{E}_{s\sim d_\mu}\big[ D_{\mathrm{KL}}\!\big(\pi(\cdot|s)\,\|\,\mu(\cdot|s)\big)\big] \le \varepsilon,
\qquad \forall s:\ \int_a \pi(a|s)\,da = 1.
\end{aligned}
\]

\paragraph{Lagrangian and stationarity.}
Introduce Lagrange multiplier \(\beta>0\) for the KL constraint and pointwise normaliser \(\alpha_s\).
The (softened) Lagrangian is
\[
\mathcal{L}(\pi,\beta,\alpha)
=
\mathbb{E}_{s\sim d_\mu}\!\Big[ 
\mathbb{E}_{a\sim\pi(\cdot|s)}\big[ R^\mu_{s,a}-V^\mu(s) \big]
\Big]
+ \beta\Big(\varepsilon - \mathbb{E}_{s\sim d_\mu}\!\big[ D_{\mathrm{KL}}(\pi(\cdot|s)\|\mu(\cdot|s))\big]\Big)
+ \mathbb{E}_{s\sim d_\mu}\!\big[ \alpha_s(1-\!\!\int_a\pi(a|s)\,da)\big].
\]
Differentiate \(\mathcal{L}\) w.r.t. \(\pi(a|s)\), set derivative to zero and solve for \(\pi\). Rearranging yields the
Boltzmann-tilted closed form for the optimal (per-state) conditional distribution:
\[
\pi^\star(a|s)
\;=\;
\frac{1}{Z(s)}\;
\mu(a|s)\;
\exp\!\Big(\frac{1}{\beta}\big(R^\mu_{s,a}-V^\mu(s)\big)\Big),
\qquad
Z(s)\;=\;\int \mu(a'|s)\exp\!\Big(\tfrac{1}{\beta}\big(R^\mu_{s,a'}-V^\mu(s)\big)\Big)\,da'.
\tag{1}
\]

This formula is obtained by solving the stationarity condition
\(\partial\mathcal{L}/\partial\pi(a|s)=0\) and enforcing normalisation; it is the soft, advantage-weighted
reweighting of the behaviour distribution \(\mu(a|s)\).

\paragraph{Projection to parameterised policy (regression step).}
If \(\pi\) is parameterised (e.g. neural network) we cannot set it equal to \(\pi^\star\) pointwise. Instead
project \(\pi^\star\) onto the parameterised family by minimising expected KL:
\[
\pi_{k+1} \;=\; \arg\min_{\pi\in\Pi}\;
\mathbb{E}_{s\sim d_\mu}\big[ D_{\mathrm{KL}}\big(\pi^\star(\cdot|s)\,\|\,\pi(\cdot|s)\big)\big].
\]
Expanding the KL and substituting \(\pi^\star\) (Equation (1)) yields an equivalent supervised regression objective:
\[
\pi_{k+1}
\;=\;
\arg\max_{\pi\in\Pi}\;
\mathbb{E}_{s\sim d_\mu}\mathbb{E}_{a\sim\mu(\cdot|s)}
\Big[
\log\pi(a|s)\;
\exp\!\Big(\frac{1}{\beta}\big(R^\mu_{s,a}-V^\mu(s)\big)\Big)
\Big].
\tag{2}
\]
This is the central AWR policy update: fit \(\pi\) by maximum likelihood on behaviour data weighted by exponentiated advantages.

\paragraph{Value update.}
AWR alternates the policy update with a value regression that fits \(V\) to returns in \(\mathcal{D}\),
typically by minimising a squared TD($\lambda$) or Monte-Carlo return loss:
\[
V^{\mathcal{D}} \;=\; \arg\min_{V}\;
\mathbb{E}_{(s,a)\sim\mathcal{D}}\big[\,\big(R^{\mathcal{D}}_{s,a} - V(s)\big)^2\big].
\tag{4}
\]

\section{DAR: Direct Advantage Regression}

DAR \parencite{he2025directadvantageregressionaligning} studies online alignment with AI-generated reward signals. The method preserves the regularised online RLHF objective while replacing iterative policy-gradient updates with a closed-form advantage-weighted target followed by supervised projection. This section summarises that objective and its relation to V-MPO.

\paragraph{RL Fine-tuning}

Given a language model $\pi_\theta$ to be aligned, a prompt dataset $\mathcal{D}(x)$ and a reward model $r$, online RL fine-tuning aims to optimise:

\begin{equation*}
  J_{\mathrm{RLHF}}(\pi_\theta; \pi_{\mathrm{ref}})
  =
  \max_{\pi_\theta}
  \mathbb{E}_{x \sim \mathcal{D}(x), y \sim \pi_\theta(y|x)}
  \left[
    r(x,y)
  \right]
  -
  \alpha
  D_{\mathrm{KL}}
  \left(
    \pi_\theta(y|x)
    \;\|\;
    \pi_{\mathrm{ref}}(y|x)
  \right).
\end{equation*}

Here $\alpha > 0$ controls KL regularization toward a fixed reference policy $\pi_{\mathrm{ref}}$.

%-------------------------------------------------------------

\paragraph{Advantage Weighted Regression}

Advantage Weighted Regression (AWR) maximizes:

\begin{equation*}
  J_{\mathrm{AWR}}(\pi_\theta)
  =
  \max_{\pi_\theta}
  \mathbb{E}_{x \sim d_{\pi_\theta}(x), y \sim \pi_\theta(y|x)}
  \left[
    A(x,y)
  \right],
\end{equation*}

where
\[
  A(x,y) = r(x,y) - V^{\pi_t}(x).
\]

To remove dependence on $d_{\pi_\theta}$, we approximate using $d_{\pi_t}$ and impose KL trust-region regularization:

\begin{equation*}
  J_{\mathrm{AWR}}(\pi_\theta; \pi_t)
  =
  \max_{\pi_\theta}
  \mathbb{E}_{x \sim d_{\pi_t}(x), y \sim \pi_\theta(y|x)}
  \left[
    A(x,y)
  \right]
  -
  \beta
  D_{\mathrm{KL}}
  \left(
    \pi_\theta(y|x)
    \;\|\;
    \pi_t(y|x)
  \right).
\end{equation*}

%-------------------------------------------------------------

\paragraph{Dual-Constrained Objective}

DAR incorporates reference regularization:

\begin{equation*}
  J_{\mathrm{DAR}}(\pi_\theta; \pi_{\mathrm{ref}}, \pi_t)
  =
  \max_{\pi_\theta}
  \mathbb{E}_{x \sim d_{\pi_t}(x), y \sim \pi_\theta(y|x)}
  \left[
    A(x,y)
  \right]
  -
  \alpha D_{\mathrm{KL}}(\pi_\theta \| \pi_{\mathrm{ref}})
  -
  \beta D_{\mathrm{KL}}(\pi_\theta \| \pi_t).
\end{equation*}

\paragraph{Theorem.}
Under mild assumptions, for the dual-constrained advantage (or reward)
maximization objective above with strictly positive KL coefficients, the
optimal policy is:

\begin{equation*}
  \pi^\star(y\mid x)
  =
  \frac{1}{Z(x)}\,
  \pi_{\mathrm{ref}}(y\mid x)^{\frac{\alpha}{\alpha+\beta}}
  \;
  \pi_t(y\mid x)^{\frac{\beta}{\alpha+\beta}}
  \;
  \exp\!\Big(\frac{A(x,y)}{\alpha+\beta}\Big),
\end{equation*}
where
\begin{equation*}
  Z(x)
  =
  \sum_{y}
  \pi_{\mathrm{ref}}(y\mid x)^{\frac{\alpha}{\alpha+\beta}}
  \;
  \pi_t(y\mid x)^{\frac{\beta}{\alpha+\beta}}
  \;
  \exp\!\Big(\frac{A(x,y)}{\alpha+\beta}\Big),
\end{equation*}
is the partition function.

\paragraph{Proof.}

\begingroup
\allowdisplaybreaks
\setlength{\jot}{2pt}
\begin{dmath*}
  \max_{\pi}\mathbb{E}_{x,y\sim\pi}\big[ A(x,y)\big]
  \;-\;\alpha D_{\mathrm{KL}}\big[\pi(y|x)\,\|\,\pi_{\mathrm{ref}}(y|x)\big]
  \;-\;\beta D_{\mathrm{KL}}\big[\pi(y|x)\,\|\,\pi_t(y|x)\big] \\
  = \min_{\pi}\mathbb{E}_{x,y\sim\pi}\Big[\alpha\log\frac{\pi(y|x)}{\pi_{\mathrm{ref}}(y|x)}
  +\beta\log\frac{\pi(y|x)}{\pi_t(y|x)}-A(x,y)\Big] \\
  = \min_{\pi}\mathbb{E}_{x,y\sim\pi}\Big[(\alpha+\beta)\log\pi(y|x)
  -\alpha\log\pi_{\mathrm{ref}}(y|x)-\beta\log\pi_t(y|x)-A(x,y)\Big] \\
  = \min_{\pi}\mathbb{E}_{x,y\sim\pi}\Big[\log\pi(y|x)
    -\log\pi_{\mathrm{ref}}(y|x)^{\frac{\alpha}{\alpha+\beta}}
    -\log\pi_t(y|x)^{\frac{\beta}{\alpha+\beta}}
  -\frac{1}{\alpha+\beta}A(x,y)\Big] \\
  = \min_{\pi}\mathbb{E}_{x,y\sim\pi}\Big[\log\frac{\pi(y|x)}
    {\pi_{\mathrm{ref}}(y|x)^{\frac{\alpha}{\alpha+\beta}}\,
    \pi_t(y|x)^{\frac{\beta}{\alpha+\beta}}}
  -\frac{1}{\alpha+\beta}A(x,y)\Big] \\
  = \min_{\pi}\mathbb{E}_{x,y\sim\pi}\Big[\log\frac{\pi(y|x)}
    {\frac{1}{Z(x)}\pi_{\mathrm{ref}}(y|x)^{\frac{\alpha}{\alpha+\beta}}
      \pi_t(y|x)^{\frac{\beta}{\alpha+\beta}}
  \exp\!\big(\frac{1}{\alpha+\beta}A(x,y)\big)}-\log Z(x)\Big].
\end{dmath*}

Because the partition function does not depend on $\pi$, $\log Z(x)$ is constant with respect to the optimisation variable and can be dropped:
\begin{dmath*}
  \min_{\pi}\mathbb{E}_{x,y\sim\pi}\Big[\log\frac{\pi(y|x)}
    {\frac{1}{Z(x)}\pi_{\mathrm{ref}}(y|x)^{\frac{\alpha}{\alpha+\beta}}
      \pi_t(y|x)^{\frac{\beta}{\alpha+\beta}}
  \exp\!\big(\frac{1}{\alpha+\beta}A(x,y)\big)}\Big] \\
  = \min_{\pi}\mathbb{E}_x\;D_{\mathrm{KL}}\!\Bigg[
    \pi(y|x)\;\Bigg\|\;
    \frac{1}{Z(x)}\pi_{\mathrm{ref}}(y|x)^{\frac{\alpha}{\alpha+\beta}}
    \pi_t(y|x)^{\frac{\beta}{\alpha+\beta}}
  \exp\!\big(\tfrac{1}{\alpha+\beta}A(x,y)\big)\Bigg].
\end{dmath*}
\endgroup

By Gibbs' inequality, the KL term is minimised when the two distributions are identical:
\begin{equation}
  \pi^\star(y|x)
  \;=\;
  \frac{1}{Z(x)}\,
  \pi_{\mathrm{ref}}(y|x)^{\frac{\alpha}{\alpha+\beta}}
  \pi_t(y|x)^{\frac{\beta}{\alpha+\beta}}
  \exp\!\Big(\frac{1}{\alpha+\beta}A(x,y)\Big).
\end{equation}

The improved parametric policy is then obtained by minimising the KL divergence to the target distribution above:

\[
  \min_{\pi_\theta}\mathbb{E}_{x\sim d_{\pi_t}(x)} D_{\mathrm{KL}}\!\big[\pi^\star(\cdot\mid x)\;\|\;\pi_\theta(\cdot\mid x)\big],
\]
Substituting $\pi^\star$ gives:
\[
  \min_{\pi_\theta}\mathbb{E}_{x\sim d_{\pi_t}(x)} D_{\mathrm{KL}}\!\Bigg[
    \frac{1}{Z(x)}\pi_{\mathrm{ref}}(y\mid x)^{\frac{\alpha}{\alpha+\beta}}
    \pi_t(y\mid x)^{\frac{\beta}{\alpha+\beta}}
    \exp\!\Big(\frac{1}{\alpha+\beta}A(x,y)\Big)
    \;\Big\|\;
  \pi_\theta(\cdot\mid x)\Bigg],
\]
Expanding the KL divergence and dropping terms independent of $\pi_\theta$ yields:
\[
  \min_{\pi_\theta}\mathbb{E}_{x\sim d_{\pi_t}(x)}\Bigg[-\sum_y
    \frac{1}{Z(x)}\,
    \pi_{\mathrm{ref}}(y\mid x)^{\frac{\alpha}{\alpha+\beta}}
    \pi_t(y\mid x)^{\frac{\beta}{\alpha+\beta}}
    \exp\!\Big(\frac{1}{\alpha+\beta}A(x,y)\Big)\,
  \log\pi_\theta(y\mid x)\Bigg],
\]
Since $Z(x)$ is a positive constant with respect to $\pi_\theta$, it can be removed without changing the optimum:
\[
  \min_{\pi_\theta}\mathbb{E}_{x\sim d_{\pi_t}(x)}\Big[-\sum_y
    \pi_{\mathrm{ref}}(y\mid x)^{\frac{\alpha}{\alpha+\beta}}
    \pi_t(y\mid x)^{\frac{\beta}{\alpha+\beta}}
    \exp\!\Big(\frac{1}{\alpha+\beta}A(x,y)\Big)\,
  \log\pi_\theta(y\mid x)\Big],
\]
Using $\pi_t$ as the sampling policy gives the final practical objective:
\[
  \max_{\pi_\theta}\mathbb{E}_{x\sim d_{\pi_t}(x)}\mathbb{E}_{y\sim\pi_t(y\mid x)}
  \left[\left(\frac{\pi_{\mathrm{ref}}(y\mid x)}{\pi_t(y\mid x)}\right)^{\frac{\alpha}{\alpha+\beta}}
    \exp\!\Big(\frac{1}{\alpha+\beta}A(x,y)\Big)\,
  \log\pi_\theta(y\mid x)\right].
\]

\paragraph{Mapping \textsc{DAR} to V-MPO}

The closed-form optimal policy derived in
Direct Advantage Regression (DAR) reduces to the V-MPO target under simple limits and
special-case identifications. The derivation clarifies when DAR may be viewed as a
sequence-level variant of the V-MPO/AWR family.

\medskip

\paragraph{DAR closed-form target.}
DAR yields a closed-form optimal
policy of the form
\begin{equation}\label{eq:dar_opt}
  \pi^\star(y\!\mid\!x)
  \;=\;
  \frac{1}{Z(x)}
  \;\pi_{\mathrm{ref}}(y\!\mid\!x)^{\frac{\alpha}{\alpha+\beta}}
  \;
  \pi_{t}(y\!\mid\!x)^{\frac{\beta}{\alpha+\beta}}
  \;
  \exp\!\Big(\frac{A(x,y)}{\alpha+\beta}\Big),
\end{equation}
where $(\alpha,\beta)>0$ are the two KL coefficients (reference and sampling policy),
$\pi_{\mathrm{ref}}$ a chosen reference policy, $\pi_t$ the sampling policy used to
collect $\mathcal{D}_{\pi_t}$, and $A(x,y)$ the (sequence-level) advantage or score.
Define the shorthand
\[
  t \coloneqq \alpha+\beta > 0.
\]
Taking logarithms of \eqref{eq:dar_opt} produces the additive form used below:
\begin{equation}\label{eq:dar_log}
  \log \pi^\star(y\!\mid\!x) = -\log Z(x)
  + \frac{\alpha}{t}\log\pi_{\mathrm{ref}}(y\!\mid\!x)
  + \frac{\beta}{t}\log\pi_t(y\!\mid\!x)
  + \frac{1}{t}A(x,y).
\end{equation}

\paragraph{V-MPO canonical target.}
V-MPO (and related advantage-weighted regression methods) uses a target proportional to
the sampling policy multiplied by an exponential advantage factor. In sequence notation:
\begin{equation}\label{eq:vmpo_form}
  \psi_{\mathrm{V\!-\!MPO}}(y\!\mid\!x)
  \propto
  \pi_{t}(y\!\mid\!x)\;\exp\!\Big(\frac{A(x,y)}{\eta}\Big),
\end{equation}
where $\eta>0$ is the temperature (V-MPO treats $\eta$ as a dual variable and may
optimise it by solving a convex dual).

\paragraph{Special case 1: identical reference and sampling policies.}
Set $\pi_{\mathrm{ref}}=\pi_t$ in \eqref{eq:dar_opt}. Then the multiplicative powers
collapse:
\[
  \pi_{\mathrm{ref}}^{\alpha/t}\pi_t^{\beta/t} = \pi_t^{(\alpha+\beta)/t} = \pi_t.
\]
Substituting into \eqref{eq:dar_opt} gives
\[
  \pi^\star(y\!\mid\!x) \propto \pi_t(y\!\mid\!x)\,\exp\!\Big(\frac{A(x,y)}{t}\Big).
\]
Comparing with \eqref{eq:vmpo_form} we identify the V-MPO temperature as
\[
  \eta = t = \alpha+\beta.
\]
Thus DAR reduces to the V-MPO structural form when the reference policy equals the
sampling policy; the DAR temperature is the sum of the two KL coefficients.

\paragraph{Special case 2: \(\alpha\to 0\) (reference KL vanishes).}
Examine the limit $\alpha\to 0$ while holding $t=\alpha+\beta$ finite (equivalently,
let $\beta\to t$). From \eqref{eq:dar_log},
\[
  \lim_{\alpha\to 0}\log\pi^\star(y\!\mid\!x)
  =
  -\log Z(x) + \underbrace{\tfrac{\alpha}{t}}_{\to 0}\log\pi_{\mathrm{ref}}
  + \underbrace{\tfrac{\beta}{t}}_{\to 1}\log\pi_t
  + \tfrac{1}{t}A(x,y),
\]
so
\[
  \pi^\star(y\!\mid\!x) \propto \pi_t(y\!\mid\!x)\,\exp\!\Big(\frac{A(x,y)}{t}\Big),
\]
again matching \eqref{eq:vmpo_form} with $\eta=t$. The algebraic limit is well-defined
because the coefficients $\alpha/t$ and $\beta/t$ converge to $(0,1)$ respectively.

\paragraph{Algebraic equivalence (log-space explanation).}
Write the DAR unnormalised log-weight for a sample $(x,y)$ as
\[
  \ell_{\mathrm{DAR}}(x,y)
  \;=\;
  \frac{\alpha}{t}\log\pi_{\mathrm{ref}}(y\!\mid\!x)
  +
  \frac{\beta}{t}\log\pi_t(y\!\mid\!x)
  +
  \frac{1}{t}A(x,y).
\]
If either $\pi_{\mathrm{ref}}=\pi_t$ or $\alpha/t\to 0$, the first two terms reduce to
$\log\pi_t(y\!\mid\!x)$. Hence
\[
  \ell_{\mathrm{DAR}}(x,y) \to \log\pi_t(y\!\mid\!x) + \frac{1}{t}A(x,y),
\]
and exponentiation recovers the V-MPO unnormalised weight
\(
  \exp(\log\pi_t + A/t) = \pi_t \exp(A/t).
\)

\paragraph{Practical consequences and distinctions.}
Although DAR and V-MPO share the same advantage-exponential device, they differ in:
\begin{enumerate}
  \item \textbf{Dual handling.} V-MPO treats $\eta$ (and an average-KL dual $\alpha$)
    as optimisation variables updated by convex-dual objectives; DAR typically treats
    $\alpha,\beta$ as explicit regularisation coefficients (or tunes them as hyperparameters),
    and derives a closed-form mixture. Consequently, V-MPO emphasises adaptive temperature
    control, while DAR emphasises calibrated mixture regularisation for LLM-scale training.
  \item \textbf{Factorisation and tractability.} V-MPO is usually applied to low-dimensional
    action spaces (or per-step discrete actions); LLM-scale training uses sequence-level
    weights on teacher-forced token log-probabilities (the DAR practical objective).
  \item \textbf{Engineered stabilisers.} DAR introduces practical stabilisers for
    large-vocabulary sequence training (weight clipping, advantage normalisation,
    importance-weight corrections). V-MPO implementations use top-$k$ selection and
    explicit dual updates which require careful numerical handling when lifted to LLMs.
\end{enumerate}

\paragraph{Bridge to the EM.}
The sequence-level EM procedure in
Section~\ref{sec:vmpo_llm_gempi}, Eq.~\eqref{eq:seq_estep_weights} through Eq.~\eqref{eq:seq_weighted_mstep},
and the DAR formulation are two views of the same update pattern: an E-step that
constructs exponential advantage weights followed by a weighted teacher-forced M-step.
In the LLM-scale V-MPO view, $\eta$ is adapted online via a dual objective and the
E-step may be restricted to top-$k$ samples for stability; in the DAR view, the
closed-form target introduces an additional reference/sampling mixture term and uses
fixed coefficients $(\alpha,\beta)$, which recover the V-MPO-style form in the limits
derived above.

\paragraph{Sequence-level weighted loss (common form).}
Both approaches use the same weighted teacher-forced M-step objective as in
Eq.~\eqref{eq:seq_weighted_mstep}.
The per-sequence weights differ only by the base factor:
\[
  w_i^{\mathrm{DAR}} \propto \exp\!\Big(\tfrac{\alpha}{t}\log\pi_{\mathrm{ref}}^{(i)}
  + \tfrac{\beta}{t}\log\pi_t^{(i)} + \tfrac{1}{t}A^{(i)}\Big),
  \qquad
  w_i^{\mathrm{V\!-\!MPO}} \propto \exp\!\Big(\log\pi_t^{(i)} + \tfrac{1}{\eta}A^{(i)}\Big).
\]

\section{(G)TrXL - Transformers for RL}

The V-MPO authors \parencite{song2019vmpoonpolicymaximumposteriori} reported strong empirical results on Transformers in Atari using TrXL \parencite{dai2019transformerxlattentivelanguagemodels}, an architecture later refined for RL through GTrXL \parencite{parisotto2019stabilizingtransformersreinforcementlearning}.

\subsection{Implementation}
\begin{itemize}
  \item \href{https://github.com/kimiyoung/transformer-xl}{github.com/kimiyoung/transformer-xl}
  \item  \href{https://github.com/nenuadrian/DI-engine/tree/main/benchmarks}{github.com/nenuadrian/DI-engine/tree/main/benchmarks}
\end{itemize}

\subsection{GTrXL with V-MPO}

The V-MPO paper \parencite{song2019vmpoonpolicymaximumposteriori} replaces the LSTM core with a Transformer-XL (TrXL) for single-task Atari, motivated by the argument that in a fully observable environment recurrent architectures enable the agent to utilise more useful representations than are available in the immediate observation. However, standard TrXL fails in RL, performing at random-policy level on benchmarks such as DMLab-30. The Gated Transformer-XL (GTrXL) \parencite{parisotto2019stabilizingtransformersreinforcementlearning} addresses this with two targeted modifications.

\subsubsection{Identity Map Reordering (TrXL-I)}

Layer normalisation is moved to the input of each sub-layer rather than the output, creating a direct identity path from the first layer's input to the last layer's output. At initialisation this biases the network towards a Markovian (reactive) policy and provides a clear gradient path, making the training landscape far more amenable to policy-gradient methods.

\subsubsection{Gating Layers}

Residual connections are replaced with learnable gating layers. The best-performing variant uses GRU-type gates:
\begin{align*}
  r &= \sigma(W_r y + U_r x), \\
  z &= \sigma(W_z y + U_z x - b_g), \\
  \hat{h} &= \tanh(W_g y + U_g (r \odot x)), \\
  g(x,y) &= (1-z) \odot x + z \odot \hat{h},
\end{align*}
where $x$ is the residual input and $y$ is the sub-layer output. Initialising the bias $b_g = 2$ places each gate near the identity at the start of training, preserving the reactive-policy initialisation provided by the layer-norm reordering.

\subsubsection{Why GTrXL Suits V-MPO}

V-MPO is an on-policy algorithm that performs policy improvement via an EM procedure (E-step top-$k$ advantage weighting; M-step weighted maximum-likelihood with a KL trust-region). Because it collects fresh trajectories each update, the memory architecture is critical: the agent must integrate information over long horizons to form useful state representations. GTrXL provides:

\begin{itemize}
  \item \textbf{Long-range memory.} Relative position encodings inherited from TrXL let the network attend over a memory tensor spanning up to thousands of past time-steps, far beyond what an LSTM can retain.
  \item \textbf{Stable optimisation.} The GRU gating achieves a 0\% divergence rate across hyperparameter sweeps, compared with 16\% for the plain TrXL-I variant, making it compatible with V-MPO's fixed Adam learning rate and without requiring population-based hyperparameter search.
  \item \textbf{Robust performance on memory tasks.} On DMLab-30, GTrXL (GRU) reaches a human-normalised score of $117.6 \pm 0.3$ versus $99.3 \pm 1.0$ for LSTM, with the largest gains on memory-dependent levels and no regression on reactive ones.
\end{itemize}

\subsubsection{Architecture and Training Configuration}

For single-task Atari with TrXL, the shared policy-value network consists of a convolutional ResNet backbone feeding into the transformer core, with previous reward and action as additional inputs. Representative TrXL hyperparameters used in V-MPO experiments: embedding size 256, 8 layers, 4 attention heads, key/value size 32, MLP size 512, unroll length 63, batch size 128. The value and policy heads branch from the transformer output; parameters are updated jointly under the combined V-MPO objective $\mathcal{L}(\phi,\theta,\eta,\alpha)$ using Adam at a fixed learning rate of $10^{-4}$.


\section{Generalized EM Policy Improvement (GEMPI)}

\paragraph{The GEMPI tuple.}
A Generalized EM Policy Improvement method is specified by
\[
  \mathcal{G}
  =
  \big(
    \{D_j\}_{j=1}^m,\;
    \{\pi_j\}_{j=1}^m,\;
    \{\lambda_j\}_{j=1}^m,\;
    \mathcal{T},\;
    \mathcal{F},\;
    D_M,\;
    \varepsilon_M
  \big),
\]
where $\{D_j\}$ are divergence functionals used in the E-step, $\{\pi_j\}$ are anchor policies, $\{\lambda_j > 0\}$ are regularization coefficients, $\mathcal{T}$ is the temperature mechanism (fixed, dual-adaptive, or closed-form), $\mathcal{F}$ is a filtering mechanism (identity or top-$k$), and $D_M$ with budget $\varepsilon_M$ is an optional M-step trust-region divergence.

% ============================================================
\subsection{General Regularized E-Step}

The E-step constructs a non-parametric target distribution $q^\star(\cdot\mid x)$ by solving a regularized advantage maximization over $m$ anchor policies:
\begin{equation}\label{eq:gempi_estep}
  q^\star
  =
  \arg\max_{q}
  \bigg\{
    \mathbb{E}_{y\sim q}\!\big[A(x,y)\big]
    -
    \sum_{j=1}^{m} \lambda_j\, D_j\!\big(q(\cdot\mid x)\,\big\|\,\pi_j(\cdot\mid x)\big)
  \bigg\},
\end{equation}
subject to $q$ being a valid distribution. The advantage $A(x,y)$ may be sequence-level (reward minus baseline) or token-level, and the divergences $D_j$ penalize deviation from each anchor $\pi_j$.

\paragraph{Theorem (Multi-KL Closed Form).}
When all divergences are KL, i.e.\ $D_j = \mathrm{KL}$ for $j=1,\dots,m$, the solution to \eqref{eq:gempi_estep} is
\begin{equation}\label{eq:gempi_closedform}
  q^\star(y\mid x)
  =
  \frac{1}{Z(x)}
  \prod_{j=1}^{m}
  \pi_j(y\mid x)^{\lambda_j/\Lambda}
  \;\exp\!\Big(\frac{A(x,y)}{\Lambda}\Big),
\end{equation}
where $\Lambda \coloneqq \sum_{j=1}^{m}\lambda_j$ is the effective temperature and $Z(x)$ is the partition function ensuring normalization.

\paragraph{Proof.}
Write the Lagrangian with multiplier $\mu$ for the normalization constraint:
\[
  \mathcal{J}(q,\mu)
  =
  \sum_y q(y\mid x)\, A(x,y)
  -
  \sum_{j=1}^{m} \lambda_j
  \sum_y q(y\mid x)\log\frac{q(y\mid x)}{\pi_j(y\mid x)}
  +
  \mu\Big(1 - \sum_y q(y\mid x)\Big).
\]
Expanding the KL terms and grouping:
\[
  \mathcal{J}
  =
  \sum_y q(y\mid x)
  \bigg[
    A(x,y)
    -
    \Lambda\log q(y\mid x)
    +
    \sum_{j=1}^{m}\lambda_j \log\pi_j(y\mid x)
  \bigg]
  +
  \mu\Big(1-\sum_y q(y\mid x)\Big).
\]
Taking the functional derivative with respect to $q(y\mid x)$ and setting to zero:
\[
  A(x,y)
  -
  \Lambda\big(\log q(y\mid x) + 1\big)
  +
  \sum_{j=1}^{m}\lambda_j\log\pi_j(y\mid x)
  -
  \mu
  =
  0.
\]
Solving for $\log q(y\mid x)$:
\[
  \log q(y\mid x)
  =
  \frac{1}{\Lambda}A(x,y)
  +
  \sum_{j=1}^{m}\frac{\lambda_j}{\Lambda}\log\pi_j(y\mid x)
  +
  \mathrm{const}.
\]
Exponentiating and normalizing yields \eqref{eq:gempi_closedform}. \hfill$\square$

\paragraph{Log-space form.}
The unnormalized log-weight for sample $(x,y)$ is
\begin{equation}\label{eq:gempi_logweight}
  \ell(x,y)
  =
  \sum_{j=1}^{m}
  \frac{\lambda_j}{\Lambda}\log\pi_j(y\mid x)
  +
  \frac{1}{\Lambda}A(x,y).
\end{equation}
The coefficients $\lambda_j/\Lambda$ sum to one, so the first term is a convex combination of the anchor log-policies---a geometric mixture---shifted by the scaled advantage. This reveals the E-step target as a geometry-aware interpolation between the anchors, tilted toward high-advantage sequences.

\paragraph{Multi-temperature dual.}
When the E-step is posed as a constrained problem---maximize expected advantage subject to $\mathrm{KL}(q\|\pi_j) < \varepsilon_j$ for each anchor---the Lagrangian yields the same functional form with each $\lambda_j$ replaced by an optimal dual variable $\lambda_j^\star$. The dual objective generalizes V-MPO's scalar temperature dual:
\begin{equation}\label{eq:gempi_dual}
  L_{\mathrm{dual}}(\lambda_1,\dots,\lambda_m)
  =
  \sum_{j=1}^{m}\lambda_j\,\varepsilon_j
  +
  \Lambda\,\log Z(x;\lambda_1,\dots,\lambda_m).
\end{equation}
This is jointly convex in $(\lambda_1,\dots,\lambda_m)$ and can be minimized by gradient descent, adapting all temperatures simultaneously.

% ============================================================
\subsection{General M-Step}

The M-step projects the non-parametric target $q^\star$ back to the parametric policy family:
\begin{equation}\label{eq:gempi_mstep}
  \theta_{k+1}
  =
  \arg\min_{\theta}
  \bigg\{
    -\mathbb{E}_{q^\star}\!\big[\log\pi_\theta(y\mid x)\big]
    +
    \gamma\,
    D_M\!\big(\pi_{\theta_k}(\cdot\mid x)\,\big\|\,\pi_\theta(\cdot\mid x)\big)
  \bigg\},
\end{equation}
where $\gamma \ge 0$ controls the M-step trust region. The three main variants are:

\begin{itemize}
  \item \textbf{Unconstrained} ($\gamma=0$): pure weighted maximum likelihood. Used by DAR and AWR.
  \item \textbf{Penalized} ($\gamma > 0$, $D_M = \mathrm{KL}$): soft KL penalty preventing the parametric policy from overshooting. The V-MPO M-step uses this with $\gamma$ treated as a Lagrangian dual variable $\alpha$ optimized against a budget $\varepsilon_\alpha$.
  \item \textbf{Hard-constrained}: replace the penalty with a constraint $D_M(\pi_{\theta_k}\|\pi_\theta) < \varepsilon_M$ and solve via the Lagrangian, yielding the stop-gradient decomposition used in V-MPO's implementation.
\end{itemize}

\paragraph{Importance-weighted practical form.}
When samples are drawn from the sampling policy $\pi_t$ but $q^\star \neq \pi_t$, the M-step loss uses importance weights:
\[
  \mathcal{L}_\pi(\theta)
  =
  -\sum_{i\in S} w_i
  \sum_{t=1}^{T_i}
  \log\pi_\theta\!\big(y_t^{(i)}\mid y_{<t}^{(i)}, x^{(i)}\big),
\]
where $w_i \propto q^\star(y^{(i)}\mid x^{(i)})/\pi_t(y^{(i)}\mid x^{(i)})$. When $\pi_t$ is one of the anchors, the corresponding factor in the geometric mixture cancels partially, simplifying the weights.

% \subsection{Finite-Sample Approximation in the M-Step}

The closed-form target \eqref{eq:gempi_closedform} is a population-level object: the
partition function
\[
  Z(x) = \sum_y \prod_{j=1}^m \pi_j(y\mid x)^{\lambda_j/\Lambda}
  \exp\!\Big(\frac{A(x,y)}{\Lambda}\Big)
\]
sums over the entire output space, which is intractable for large vocabulary sequence
models. In practice, $Z(x)$ is estimated from a finite batch
$\mathcal{B} = \{y^{(i)}\}_{i=1}^N$ drawn from the sampling policy $\pi_t$, giving
\[
  \hat Z(x) = \frac{1}{N}\sum_{i=1}^N
  \frac{\prod_j \pi_j(y^{(i)}\mid x)^{\lambda_j/\Lambda}
  \exp\!\big(\frac{1}{\Lambda}A(x,y^{(i)})\big)}
  {\pi_t(y^{(i)}\mid x)},
\]
a self-normalised importance-weighted estimate. Three distinct approximation errors arise.

\paragraph{1. Partition function bias.}

$\hat Z(x)$ is a ratio estimator and is therefore biased. By the delta method,
\[
  \mathbb{E}[\hat Z(x)] = Z(x)
  \Big(1 + O\!\big(N^{-1}\big)\big),
\]
so the bias is $O(N^{-1})$ and vanishes as the batch grows. The self-normalised
importance weights
\[
  \hat w_i
  = \frac{
    \prod_j \pi_j(y^{(i)}\mid x)^{\lambda_j/\Lambda}
    \exp\!\big(\frac{1}{\Lambda}A(x,y^{(i)})\big)
    /\,\pi_t(y^{(i)}\mid x)
  }{
    \sum_{i'} \prod_j \pi_j(y^{(i')}\mid x)^{\lambda_j/\Lambda}
    \exp\!\big(\frac{1}{\Lambda}A(x,y^{(i')})\big)
    /\,\pi_t(y^{(i')}\mid x)
  }
\]
converge to $q^\star(y^{(i)}\mid x)/\pi_t(y^{(i)}\mid x)$ almost surely as $N\to\infty$
by the strong law, provided $q^\star \ll \pi_t$ (absolute continuity).

\paragraph{2. Effective sample size and weight degeneracy.}

The quality of the finite-sample approximation is governed by the effective sample size
\[
  \mathrm{ESS} = \frac{\big(\sum_i \hat w_i\big)^2}{\sum_i \hat w_i^2},
\]
which equals $N$ when all weights are equal and degrades toward $1$ when a single
sample dominates. The ESS is controlled by the mismatch between $q^\star$ and $\pi_t$,
quantified by their $\chi^2$-divergence:
\[
  \mathrm{ESS} \approx \frac{N}{1 + \chi^2(q^\star \| \pi_t)}.
\]
Within the GEMPI parameterisation, increasing $\Lambda$ (i.e.\ raising regularisation
coefficients or lowering the effective temperature) shrinks the advantage tilt, bringing
$q^\star$ closer to the geometric anchor mixture and hence to $\pi_t$. This directly
improves ESS, providing a formal justification for the practical observation that lower
temperature produces more stable weighted updates.

\paragraph{3. M-step gradient bias under finite ESS.}

The M-step objective \eqref{eq:gempi_mstep} evaluated with self-normalised weights is
\[
  \hat{\mathcal{L}}_\pi(\theta)
  =
  -\sum_{i\in S} \hat w_i
  \sum_{t=1}^{T_i}
  \log\pi_\theta\!\big(y_t^{(i)}\mid y_{<t}^{(i)},x^{(i)}\big).
\]
The gradient $\nabla_\theta \hat{\mathcal{L}}_\pi$ is a biased estimator of
$\nabla_\theta \mathbb{E}_{q^\star}[-\log\pi_\theta]$ because the self-normalised
weights themselves depend on the sample. The bias is of order $O(N^{-1})$ and has been
analysed in the self-normalised IS literature \parencite{Hesterberg1995,
mcbook}; it does not affect consistency but does affect the finite-sample
variance of the gradient.

\paragraph{Connection to GEMPI stabilisers.}

The three structural stabilisers discussed above can be understood as
direct mitigations of the above errors.

\begin{itemize}
  \item \textbf{Adaptive temperature} (dual optimisation of $\lambda_j$) minimises
    $\chi^2(q^\star \| \pi_t)$, maximising ESS and reducing partition function bias.
  \item \textbf{Top-$k$ filtering} replaces soft self-normalised weighting over all $N$
    samples with hard selection of the $k$ samples where $q^\star / \pi_t$ is largest.
    This acts as a variance-reduction strategy: by concentrating mass on the samples
    most likely under $q^\star$, it reduces the second moment of the importance weights
    at the cost of introducing a support truncation bias of order
    $O\big(\mathbb{E}_{q^\star}[q^\star/\pi_t \cdot \mathbf{1}_{i \notin S}]\big)$,
    which is small when the top-$k$ fraction $\rho$ is chosen so that the omitted tail
    of $q^\star$ is negligible.
  \item \textbf{Log-sum-exp normalisation} addresses floating-point
    representation of $\hat Z$ rather than its statistical properties, but is a
    necessary precondition for the estimates above to be numerically meaningful at
    large $|\!A|/\Lambda$.
\end{itemize}

\paragraph{Remark on the population vs.\ sample limit.}
In the limit $N \to \infty$ with $\pi_t$ having full support over the output space,
$\hat w_i \to q^\star(y^{(i)}\mid x)/\pi_t(y^{(i)}\mid x)$ and the M-step recovers the
population KL minimization $\min_\theta D_{\mathrm{KL}}(q^\star \| \pi_\theta)$ exactly.
At finite $N$, the M-step minimizes the same KL subject to the support constraint imposed
by the batch, which is a strictly easier problem and can lead to underfitting of the
tails of $q^\star$. This finite-support effect is distinct from, and additive with,
the self-normalisation bias above.

\subsection{Recovering Existing Methods}

The GEMPI framework subsumes the EM-style methods derived in the preceding sections. Each method corresponds to a specific instantiation of the tuple $\mathcal{G}$.

\medskip
\begin{center}
  \small
  \begin{tabular}{lccccccc}
    \toprule
    \textbf{Method}
    & $m$
    & \textbf{Anchors}
    & \textbf{Coefficients}
    & $\Lambda$
    & \textbf{Temperature}
    & \textbf{M-step $D_M$}
    & \textbf{Filtering} \\
    \midrule
    SFT
    & 0
    & ---
    & ---
    & ---
    & ---
    & None
    & --- \\
    AWR
    & 1
    & $\pi_t$
    & $\beta$
    & $\beta$
    & Fixed
    & None
    & None \\
    V-MPO
    & 1
    & $\pi_t$
    & $\eta$
    & $\eta$
    & Adaptive dual
    & KL ($\alpha$ dual)
    & Top-$k$ \\
    DAR
    & 2
    & $\pi_{\mathrm{ref}},\pi_t$
    & $\alpha,\beta$
    & $\alpha{+}\beta$
    & Fixed
    & None
    & None \\
    RL-EM
    & 1
    & $\pi_{\mathrm{ref}}$
    & $\beta$
    & $\beta$
    & Fixed
    & Optional KL
    & None \\
    \midrule
    DPO (collapsed)
    & 1
    & $\pi_{\mathrm{ref}}$
    & $\beta$
    & $\beta$
    & Fixed
    & None
    & --- \\
    \bottomrule
  \end{tabular}
\end{center}
\medskip

\paragraph{V-MPO.}
Set $m=1$, $D_1=\mathrm{KL}$, $\pi_1=\pi_t$. The closed form \eqref{eq:gempi_closedform} becomes
\[
  q^\star(y\mid x) \propto \pi_t(y\mid x)\,\exp\!\Big(\frac{A(x,y)}{\eta}\Big),
\]
with $\Lambda = \eta$. The temperature is treated as a dual variable optimised via the scalar dual \eqref{eq:gempi_dual} (which reduces to $\mathcal{L}_\eta = \eta\varepsilon_\eta + \eta\log(Z/k)$). Top-$k$ filtering restricts the support. The M-step adds a KL trust region with dual $\alpha$. This recovers the V-MPO formulation derived earlier in this document.

\paragraph{DAR.}
Set $m=2$, $D_1=D_2=\mathrm{KL}$, $\pi_1=\pi_{\mathrm{ref}}$ with coefficient $\alpha$, $\pi_2=\pi_t$ with coefficient $\beta$. The closed form \eqref{eq:gempi_closedform} becomes
\[
  q^\star(y\mid x)
  \propto
  \pi_{\mathrm{ref}}(y\mid x)^{\alpha/(\alpha+\beta)}\;
  \pi_t(y\mid x)^{\beta/(\alpha+\beta)}\;
  \exp\!\Big(\frac{A(x,y)}{\alpha+\beta}\Big),
\]
with $\Lambda = \alpha+\beta$. This is precisely the DAR optimal policy in Eq.~\eqref{eq:dar_opt}.

\paragraph{AWR.}
Set $m=1$, $D_1=\mathrm{KL}$, $\pi_1=\pi_t$, $\lambda_1=\beta$ fixed. No dual optimization, no filtering, unconstrained M-step. The target is $q^\star \propto \pi_t\exp(A/\beta)$, recovering standard Advantage Weighted Regression \parencite{peng2019advantageweightedregression}.

\paragraph{SFT.}
The degenerate case with no E-step optimization: $q(\tau\mid x) = \delta(\tau=y)$ places all mass on demonstrated responses, and the M-step is pure maximum likelihood. This corresponds to $m=0$ in the GEMPI tuple.

\paragraph{DPO (collapsed).}
The KL-regularised RLHF objective \eqref{eq:dpo_rlhf} yields the same E-step closed form as GEMPI with $m=1$, $\pi_1=\pi_{\mathrm{ref}}$, $\Lambda=\beta$: namely $q^\star \propto \pi_{\mathrm{ref}}\exp(r/\beta)$. DPO does not construct this target explicitly. Instead, it substitutes the optimal policy form into the Bradley--Terry preference likelihood, and the partition function $Z(x)$ cancels in the reward difference. The result is a single-stage preference MLE \eqref{eq:dpo_loss} that implicitly solves both the E-step and M-step. DPO can thus be viewed as a degenerate GEMPI instantiation in which the E-step is analytically eliminated.

\paragraph{DAR$\to$V-MPO as corollary.}
The reduction derived in the DAR-to-V-MPO mapping above is now a direct consequence of the GEMPI parameterisation: collapsing two anchors to one ($\pi_{\mathrm{ref}}=\pi_t$) merges the geometric mixture powers $\pi_{\mathrm{ref}}^{\alpha/\Lambda}\pi_t^{\beta/\Lambda} = \pi_t^{1} = \pi_t$, recovering the single-anchor form with $\eta = \alpha+\beta$. Alternatively, sending $\alpha\to 0$ zeros out the $\pi_{\mathrm{ref}}$ contribution, yielding the same result.

% ============================================================
\subsection{The Role of Divergence Choice}

The preceding derivations use KL divergence exclusively in the E-step. This is analogous to the most common PMD instantiation (negative entropy as mirror map). However, the GEMPI E-step \eqref{eq:gempi_estep} is defined for arbitrary divergences $D_j$. Replacing KL with other members of the $f$-divergence family changes the form of the advantage tilting: for KL the tilting is exponential in the advantage; for the $\chi^2$-divergence ($f(u)=(u-1)^2$) the optimality condition yields linear tilting $q^\star \propto \pi_j(1 + A/(2\lambda_j))$; for R\'enyi divergences the tilting follows a power law. The choice of E-step divergence thus plays the same role in GEMPI that the choice of Bregman generating function plays in Bregman divergence theory, or the choice of mirror map in PMD.

\medskip
\begin{center}
  \begin{tabular}{lll}
    \toprule
    \textbf{PMD (policy gradients)} & \textbf{GEMPI (EM alignment)} \\
    \midrule
    Mirror map $h$ &E-step divergence $D_j$ \\
    $h = -H$ (neg.\ entropy) $\to$ NPG/TRPO & $D_j = \mathrm{KL}$ $\to$ V-MPO/DAR/AWR \\
    $h = \tfrac{1}{2}\|\cdot\|^2$ $\to$ proj.\ gradient & $D_j = \chi^2$ $\to$ linear adv.\ weighting \\
    Step size / temperature $\eta$& Effective temperature $\Lambda$ \\
    Single proximal step & Two-phase: E-step + M-step projection \\
    \bottomrule
  \end{tabular}
\end{center}

% ============================================================
\subsection{Stability Properties as Structural Consequences}

The GEMPI decomposition reveals that the practical stabilization mechanisms used across the various methods are not ad-hoc engineering choices but follow from specific structural decisions within the framework.

\paragraph{Adaptive temperature.}
Treating the coefficients $\lambda_j$ as dual variables (Eq.~\eqref{eq:gempi_dual}) rather than fixed hyperparameters gives automatic temperature adaptation. V-MPO exercises this with a single dual variable ($m=1$); DAR currently uses fixed $(\alpha,\beta)$ but could, within GEMPI, optimise both as a two-temperature dual, gaining adaptive stability while retaining the dual-anchor structure.

\paragraph{Trust regions in the M-step.}
The M-step constraint $D_M(\pi_{\theta_k}\|\pi_\theta) < \varepsilon_M$ is an independent axis of variation: any E-step target can be paired with any M-step trust-region policy. V-MPO uses a KL trust region; DAR and AWR do not. The framework makes explicit that adding or removing M-step constraints is orthogonal to the E-step design.

\paragraph{Top-$k$ filtering.}
Restricting the support of $q^\star$ to the top-$k$ samples by advantage before computing the geometric mixture weights is a form of hard thresholding on the filter set $\mathcal{F}$. This can be applied to any GEMPI instantiation, not only V-MPO.

\paragraph{Dropout compatibility.}
The GEMPI template guarantees dropout compatibility whenever the M-step uses detached (stop-gradient) weights from the E-step. In this case the M-step gradient
\[
  \nabla_\theta \mathcal{L}_\pi
  =
  -\sum_{i\in S} w_i \sum_t \nabla_\theta\log\pi_\theta^{(\mathrm{mask})}(y_t^{(i)}\mid\cdot\,)
\]
is an unbiased estimator of the weighted-MLE gradient under the dropout distribution, exactly as in supervised learning. This structural property generalises the later dropout discussion to a consequence of the GEMPI decomposition: any method instantiated within GEMPI inherits dropout compatibility in its M-step.

% ============================================================
\subsection{Novel Instantiations}

The GEMPI framework, by making the axes of variation explicit, suggests several unexplored combinations.

\paragraph{Adaptive-Temperature DAR (AT-DAR).}
DAR with $m=2$ but treating both $\alpha$ and $\beta$ as dual variables with KL budgets $\varepsilon_\alpha$ and $\varepsilon_\beta$. The dual objective becomes
\[
  L_{\mathrm{dual}}(\alpha,\beta)
  =
  \alpha\,\varepsilon_\alpha + \beta\,\varepsilon_\beta
  +
  (\alpha+\beta)\log Z(x;\alpha,\beta),
\]
giving DAR the same adaptive stability as V-MPO while retaining the dual-anchor structure.

\paragraph{DAR with M-step Trust Region (DAR-TR).}
Adding a KL trust-region constraint to DAR's currently unconstrained M-step creates a method combining DAR's dual-anchor E-step with V-MPO's conservative parametric projection: the full GEMPI tuple with $m=2$, $D_1=D_2=\mathrm{KL}$, $D_M=\mathrm{KL}$ with budget $\varepsilon_M$.

\paragraph{Top-$k$ DAR.}
Applying V-MPO's top-$k$ filtering to the DAR E-step. Currently DAR uses all samples; restricting to the top-$k$ by advantage before computing the geometric mixture weights could improve sample efficiency by focusing the M-step on demonstrably high-quality sequences.

% ============================================================
\subsection{Worked Instantiation: LLM V-MPO}
\label{sec:vmpo_llm_gempi}

This subsection instantiates GEMPI for online RL alignment of autoregressive transformers. For prompts $x \sim \mathcal{D}_x$, the policy $\pi_\theta(y\mid x)$ generates full responses $y=(y_1,\dots,y_T)$ and receives sequence-level rewards from a preference/reward model. We use sequence-level advantages
\[
  A^{(i)} = r^{(i)} - V_\phi(x^{(i)}),
\]
and perform EM-style policy improvement with V-MPO structure.

\paragraph{GEMPI tuple.}
The LLM variant corresponds to
\[
  \mathcal{G}_{\mathrm{LLM\text{-}V\!-\!MPO}}
  =
  \big(
    \{\mathrm{KL}\},
    \{\pi_t\},
    \{\eta\},
    \text{dual-adaptive},
    \text{top-}k,
    \mathrm{KL},
    \varepsilon_\alpha
  \big),
\]
that is, a single KL anchor $\pi_t$ in the E-step, dual adaptation of $\eta$, top-$k$ support filtering, and an optional KL trust region in the M-step.

\paragraph{Sequence-level E-step for transformer rollouts.}
Given a rollout batch
\[
  \mathcal{B}=\{(x^{(i)},y^{(i)},A^{(i)})\}_{i=1}^{N},
\]
and selected subset $S\subset\{1,\dots,N\}$, the non-parametric E-step target is
\begin{equation}
  \label{eq:seq_estep_weights}
  \psi(i) \propto \exp\!\Big(\frac{A^{(i)}}{\eta}\Big).
\end{equation}
After normalisation, these become per-sequence weights $w_i$ used by the M-step.

The temperature is adapted through the dual objective
\[
  \mathcal{L}_\eta(\eta)
  =
  \eta\,\varepsilon_\eta
  +
  \eta\log\!\Big(\frac{1}{k}\sum_{i\in S}\exp\!\Big(\frac{A^{(i)}}{\eta}\Big)\Big),
\]
with $k=|S|$. A numerically stable log-sum-exp form is
\begin{equation}
  \label{eq:seq_logsumexp}
  g(\eta)
  =
  m + \log\!\Big(\frac{1}{k}\sum_{i\in S}\exp(u_i-m)\Big),
  \qquad
  u_i=\frac{A^{(i)}}{\eta},\quad m=\max_{i\in S}u_i.
\end{equation}

In practice, $S$ is chosen as top-$k$ by detached scaled advantage:
\[
  u_i^{\mathrm{det}} = \frac{A^{(i)}}{\eta_{\mathrm{det}}}, \qquad
  k = \max\!\big(1,\lfloor \rho N\rfloor\big), \qquad
  S = \operatorname{Top-}k\!\left(u_i^{\mathrm{det}}\right).
\]

\paragraph{M-step as weighted teacher-forced transformer training.}
The parametric projection is weighted teacher-forced cross-entropy:
\begin{equation}
  \label{eq:seq_weighted_mstep}
  \mathcal{L}_\pi(\theta)
  =
  -\sum_{i\in S} w_i \sum_{t=1}^{T_i}
  \log \pi_\theta\!\big(y_t^{(i)} \mid y_{<t}^{(i)}, x^{(i)}\big).
\end{equation}

This keeps the update in standard transformer training form (teacher forcing over tokens) while importing RL information through detached sequence weights $w_i$. When desired, a KL trust-region term to $\pi_t$ is added exactly as in the V-MPO M-step.

\paragraph{Transformer-specific practicalities.}
\textit{Sequence-level credit assignment.} Rewards are naturally sequence-level in LLM alignment; the EM weighting avoids unstable token-wise importance ratios.
\textit{Long-context batching.} Variable-length prompts and completions require masking and packed batches; the weighted M-step remains unchanged under standard attention masks.
\textit{Large-vocabulary stability.} Top-$k$ filtering and log-sum-exp normalisation reduce weight degeneracy and floating-point overflow in mixed-precision training.
\textit{Distributed training compatibility.} The objective is a weighted cross-entropy, so it integrates with gradient accumulation, FSDP/ZeRO, and standard transformer training pipelines.

\paragraph{Dropout in the LLM V-MPO M-step.}
Because E-step weights are detached from current parameters, the M-step with dropout mask sampling remains
\[
  \nabla_\theta \mathcal{L}_\pi
  =
  -\sum_{i\in S} w_i \sum_t
  \nabla_\theta \log \pi_\theta^{(\mathrm{mask})}\!\big(y_t^{(i)}\mid y_{<t}^{(i)},x^{(i)}\big),
\]
which is the same unbiased estimator structure as weighted supervised learning. In contrast to PPO, no ratio $\pi_\theta/\pi_{\theta_{\mathrm{old}}}$ is required inside the objective, so stochastic forward passes do not corrupt the core optimisation signal. A detailed PPO-vs-EM comparison is given in Section~\ref{sec:dropout_case}.

\section{Three Paradigms for LLM Alignment}
\label{sec:three_paradigms}


\paragraph{Paradigm 1: Policy Gradient (PPO)}

The gradient signal is $\nabla_\theta r_t(\theta)\,A_t$---direct parameter-space ascent on the expected return. The importance-sampling ratio $r_t(\theta) = \pi_\theta(a_t\mid s_t)/\pi_{\theta_{\mathrm{old}}}(a_t\mid s_t)$ couples the gradient to the current parameters. Trust regions are enforced via clipping (a heuristic approximation to a KL constraint). In the Policy Mirror Descent (PMD) framework \parencite{lan2023policymirrordescentrl, tomar2022mdpo}, PPO corresponds to the negative-entropy mirror map. The method is online and on-policy: fresh rollouts are required for each update.

\paragraph{Paradigm 2: Preference MLE (DPO)}

The gradient signal is a weighted cross-entropy on preference pairs:
\[
  \nabla_\theta \mathcal{L}_{\mathrm{DPO}}
  \propto
  \sigma(\hat{r}_\theta(y_l) - \hat{r}_\theta(y_w))
  \big(\nabla_\theta\log\pi_\theta(y_w) - \nabla_\theta\log\pi_\theta(y_l)\big).
\]
There is no explicit reward model, no E-step, and no importance ratios. The method operates offline on a fixed preference dataset. The implicit reward $\hat{r}_\theta = \beta\log(\pi_\theta/\pi_{\mathrm{ref}})$ substitutes for explicit reward modelling. As derived in Section~\ref{sec:dpo}, DPO can be viewed as ``collapsed EM'': the Boltzmann optimal policy is substituted analytically into Bradley--Terry, bypassing the E-step entirely.

\paragraph{Paradigm 3: EM-Based (V-MPO, DAR, AWR---unified by GEMPI)}

The update is two-phase: an E-step constructs a non-parametric target $q^\star$, and an M-step projects to $\pi_\theta$ via weighted MLE. The gradient signal is $w_i\nabla_\theta\log\pi_\theta(y)$, where the weights $w_i$ are detached from the current parameters $\theta$. Explicit reward or advantage signals are required, and rollouts are typically generated online. Trust regions operate in the E-step (via adaptive temperature or KL constraints on $q^\star$) and/or the M-step (via KL penalties on $\pi_\theta$). The GEMPI tuple parameterises the full space of choices: divergences, anchors, temperature mechanism, filtering, and M-step constraint.


\medskip
\begin{center}
  \small
  \begin{tabular}{lccc}
    \toprule
    & \textbf{Policy Gradient (PPO)}
    & \textbf{Preference MLE (DPO)}
    & \textbf{EM-Based (GEMPI)} \\
    \midrule
    Gradient form
    & $\nabla_\theta\, r_t(\theta)\, A_t$
    & $\sigma(\cdot)\,\nabla_\theta\log\pi_\theta$
    & $w_i\,\nabla_\theta\log\pi_\theta$ \\
    Reward model?
    & Yes (explicit)
    & No (implicit)
    & Yes (explicit) \\
    Value function?
    & Yes
    & No
    & Yes \\
    Online rollouts?
    & Yes
    & No
    & Yes \\
    Importance ratio?
    & Yes
    & No
    & No \\
    Dropout compatible?
    & No
    & Yes
    & Yes \\
    Trust region mechanism
    & Clipping (param.\ space)
    & $\beta$ (implicit KL)
    & E-step + M-step (distr.\ space) \\
    GEMPI instantiation
    & N/A
    & Collapsed ($m\!=\!1$)
    & Direct \\
    \bottomrule
  \end{tabular}
\end{center}
\medskip


\paragraph{Parameter space vs.\ distribution space.}
PPO optimises in parameter space via noisy gradient steps; GEMPI optimises in distribution space first (E-step), then projects to the parametric family (M-step). This two-phase structure is what yields detached weights and, consequently, dropout compatibility.

\paragraph{DPO as collapsed EM.}
DPO avoids the E-step by exploiting the Bradley--Terry cancellation of $Z(x)$. This elegantly removes the need for reward modelling and online rollouts, but ties the method to pairwise preference data and prevents it from incorporating richer reward signals (e.g.\ multi-aspect scores, process supervision) or adapting temperature online.

\paragraph{DAR as a bridge.}
Like DPO, it derives a closed-form optimal policy from a KL-regularised objective; like V-MPO, it uses that target in an explicit M-step with advantage weighting. DAR's dual-anchor structure ($\pi_{\mathrm{ref}}$ and $\pi_t$) generalises both the DPO anchor ($\pi_{\mathrm{ref}}$ only) and the V-MPO anchor ($\pi_t$ only). Within GEMPI, DAR is the natural meeting point of the preference-MLE and EM paradigms.

\paragraph{EM-based methods.}
Offer the most flexible design space: any reward signal can be used, temperature can be adapted online via dual optimisation, multiple anchors can regularise the target distribution, and the M-step inherits dropout compatibility from its weighted-MLE structure. The GEMPI framework makes these axes of variation explicit and enables principled construction of new methods by selecting from the tuple $\mathcal{G}$. Policy-gradient and preference-MLE methods are important baselines and limiting cases, but neither offers the same combinatorial flexibility.

\section{References}

\printbibliography[title={~}]

\end{document}
