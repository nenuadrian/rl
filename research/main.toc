\contentsline {section}{\numberline {1}Research Questions}{2}{section.1}%
\contentsline {section}{\numberline {2}Preliminaries}{2}{section.2}%
\contentsline {subsection}{Maximum Likelihood Estimation and Maximum a Posteriori}{2}{section*.2}%
\contentsline {subsection}{EM Lower Bound -- ELBO}{2}{section*.3}%
\contentsline {subsection}{EM Iterations}{3}{section*.4}%
\contentsline {subsection}{EM Policy Iteration vs Policy Gradients}{3}{section*.5}%
\contentsline {section}{\numberline {3}V-MPO: Maximum a Posteriori Policy Optimization}{3}{section.3}%
\contentsline {paragraph}{Policy Evaluation (Critic Update).}{4}{section*.6}%
\contentsline {paragraph}{Policy Improvement via EM}{4}{section*.7}%
\contentsline {paragraph}{E-Step: Non-Parametric Policy Construction}{4}{section*.8}%
\contentsline {paragraph}{M-Step: Parametric Projection with KL Constraint}{5}{section*.9}%
\contentsline {section}{\numberline {4}PPO: Proximal Policy Optimization}{5}{section.4}%
\contentsline {paragraph}{Full Objective}{6}{section*.10}%
\contentsline {paragraph}{Sequence-Level PPO (LLM Case)}{6}{section*.11}%
\contentsline {section}{\numberline {5}The Case for Dropout}{6}{section.5}%
\contentsline {subsection}{Why PPO Disables Dropout}{7}{section*.12}%
\contentsline {paragraph}{Problem 1: stochastic numerator.}{7}{section*.13}%
\contentsline {paragraph}{Problem 2: inconsistent denominator.}{7}{section*.14}%
\contentsline {paragraph}{Problem 3: KL penalty corruption.}{7}{section*.15}%
\contentsline {subsection}{Why V-MPO / EM-Style Methods Are Dropout-Compatible}{7}{section*.16}%
\contentsline {paragraph}{E-step: weights from advantages, not ratios.}{7}{section*.17}%
\contentsline {paragraph}{M-step: standard cross-entropy.}{7}{section*.18}%
\contentsline {paragraph}{No ratio, no conflict.}{8}{section*.19}%
\contentsline {subsection}{Practical Implications}{8}{section*.20}%
\contentsline {section}{\numberline {6}DPO: Direct Preference Optimization}{8}{section.6}%
\contentsline {paragraph}{Closed-form optimal policy.}{8}{section*.21}%
\contentsline {paragraph}{Reward reparameterisation.}{8}{section*.22}%
\contentsline {paragraph}{Bradley--Terry preference model.}{9}{section*.23}%
\contentsline {paragraph}{The DPO loss.}{9}{section*.24}%
\contentsline {paragraph}{DPO as Preference MLE}{9}{section*.25}%
\contentsline {paragraph}{DPO and the EM Framework}{9}{section*.26}%
\contentsline {paragraph}{The ``collapsed EM'' interpretation.}{9}{section*.27}%
\contentsline {paragraph}{What is lost.}{9}{section*.28}%
\contentsline {paragraph}{Dropout compatibility.}{10}{section*.29}%
\contentsline {section}{\numberline {7}AWR: Advantage-Weighted Regression}{10}{section.7}%
\contentsline {paragraph}{Expected improvement objective.}{10}{section*.30}%
\contentsline {paragraph}{Constrained policy search (primal).}{10}{section*.31}%
\contentsline {paragraph}{Lagrangian and stationarity.}{10}{section*.32}%
\contentsline {paragraph}{Projection to parameterised policy (regression step).}{10}{section*.33}%
\contentsline {paragraph}{Value update.}{11}{section*.34}%
\contentsline {section}{\numberline {8}DAR: Direct Advantage Regression}{11}{section.8}%
\contentsline {paragraph}{RL Fine-tuning}{11}{section*.35}%
\contentsline {paragraph}{Advantage Weighted Regression}{11}{section*.36}%
\contentsline {paragraph}{Dual-Constrained Objective}{11}{section*.37}%
\contentsline {paragraph}{Theorem.}{11}{section*.38}%
\contentsline {paragraph}{Proof.}{12}{section*.39}%
\contentsline {paragraph}{Mapping \textsc {DAR} to V-MPO}{12}{section*.40}%
\contentsline {paragraph}{DAR closed-form target.}{13}{section*.41}%
\contentsline {paragraph}{V-MPO canonical target.}{13}{section*.42}%
\contentsline {paragraph}{Special case 1: identical reference and sampling policies.}{13}{section*.43}%
\contentsline {paragraph}{Special case 2: \(\alpha \rightarrow 0\) (reference KL vanishes).}{13}{section*.44}%
\contentsline {paragraph}{Algebraic equivalence (log-space explanation).}{13}{section*.45}%
\contentsline {paragraph}{Practical consequences and distinctions.}{14}{section*.46}%
\contentsline {paragraph}{Bridge to the EM.}{14}{section*.47}%
\contentsline {paragraph}{Sequence-level weighted loss (common form).}{14}{section*.48}%
\contentsline {section}{\numberline {9}MaxMin-RLHF}{14}{section.9}%
\contentsline {paragraph}{Method.}{14}{section*.49}%
\contentsline {paragraph}{Diversity and an impossibility bound. }{15}{section*.50}%
\contentsline {paragraph}{Generative model for pairwise labels.}{15}{section*.51}%
\contentsline {paragraph}{E-step (responsibilities).}{15}{section*.52}%
\contentsline {paragraph}{M-step (reward update).}{15}{section*.53}%
\contentsline {paragraph}{Objective and policy iteration. }{16}{section*.54}%
\contentsline {paragraph}{Algorithm 1: MaxMin-RLHF (policy-level)}{16}{section*.55}%
\contentsline {paragraph}{Algorithm 2: Learning rewards with EM (clustered reward models)}{16}{section*.56}%
\contentsline {section}{\numberline {10}Generalized EM Policy Improvement (GEMPI)}{16}{section.10}%
\contentsline {paragraph}{The GEMPI tuple.}{17}{section*.57}%
\contentsline {subsection}{General Regularized E-Step}{17}{section*.58}%
\contentsline {paragraph}{Theorem (Multi-KL Closed Form).}{17}{section*.59}%
\contentsline {paragraph}{Proof.}{17}{section*.60}%
\contentsline {paragraph}{Log-space form.}{17}{section*.61}%
\contentsline {paragraph}{Multi-temperature dual.}{18}{section*.62}%
\contentsline {subsection}{General M-Step}{18}{section*.63}%
\contentsline {paragraph}{Importance-weighted practical form.}{18}{section*.64}%
\contentsline {paragraph}{1. Partition function bias.}{18}{section*.65}%
\contentsline {paragraph}{2. Effective sample size and weight degeneracy.}{19}{section*.66}%
\contentsline {paragraph}{3. M-step gradient bias under finite ESS.}{19}{section*.67}%
\contentsline {paragraph}{Connection to GEMPI stabilisers.}{19}{section*.68}%
\contentsline {paragraph}{Remark on the population vs.\ sample limit.}{19}{section*.69}%
\contentsline {subsection}{Recovering Existing Methods}{19}{section*.70}%
\contentsline {paragraph}{V-MPO.}{20}{section*.71}%
\contentsline {paragraph}{DAR.}{20}{section*.72}%
\contentsline {paragraph}{AWR.}{20}{section*.73}%
\contentsline {paragraph}{SFT.}{20}{section*.74}%
\contentsline {paragraph}{DPO (collapsed).}{20}{section*.75}%
\contentsline {paragraph}{DAR$\rightarrow $V-MPO as corollary.}{20}{section*.76}%
\contentsline {subsection}{The Role of Divergence Choice}{20}{section*.77}%
\contentsline {subsection}{Stability Properties as Structural Consequences}{21}{section*.78}%
\contentsline {paragraph}{Adaptive temperature.}{21}{section*.79}%
\contentsline {paragraph}{Trust regions in the M-step.}{21}{section*.80}%
\contentsline {paragraph}{Top-$k$ filtering.}{21}{section*.81}%
\contentsline {paragraph}{Dropout compatibility.}{21}{section*.82}%
\contentsline {subsection}{Novel Instantiations}{21}{section*.83}%
\contentsline {paragraph}{Adaptive-Temperature DAR (AT-DAR).}{21}{section*.84}%
\contentsline {paragraph}{DAR with M-step Trust Region (DAR-TR).}{21}{section*.85}%
\contentsline {paragraph}{Top-$k$ DAR.}{22}{section*.86}%
\contentsline {subsection}{Worked Instantiation: LLM V-MPO}{22}{section*.87}%
\contentsline {paragraph}{GEMPI tuple.}{22}{section*.88}%
\contentsline {paragraph}{Sequence-level E-step for transformer rollouts.}{22}{section*.89}%
\contentsline {paragraph}{M-step as weighted teacher-forced transformer training.}{22}{section*.90}%
\contentsline {paragraph}{Transformer-specific practicalities.}{23}{section*.91}%
\contentsline {paragraph}{Dropout in the LLM V-MPO M-step.}{23}{section*.92}%
\contentsline {section}{\numberline {A}Comprehensive Method Comparison}{23}{appendix.A}%
\contentsline {section}{\numberline {B}GTrXL: Gated Transformer-XL for RL}{25}{appendix.B}%
\contentsline {paragraph}{Identity Map Reordering (TrXL-I).}{25}{section*.95}%
\contentsline {paragraph}{GRU Gating Layers.}{25}{section*.96}%
\contentsline {paragraph}{Why GTrXL suits V-MPO.}{25}{section*.97}%
\contentsline {paragraph}{Architecture and training configuration.}{25}{section*.98}%
